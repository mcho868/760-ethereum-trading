{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "013c0584",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning for Ethereum Trading â€” 15D Methodology\n",
    "\n",
    "This notebook implements the methodology from your paper summary for ETH trading using DRL, adapted to 15-day episodes and a simplified, anti-overfitting signal design.\n",
    "\n",
    "## Structure\n",
    "1. Imports\n",
    "2. Configuration (defaults + JSON override support)\n",
    "3. Utilities (signals, metrics, helpers)\n",
    "4. Data Loading & Feature Preparation\n",
    "5. Trading Environment (Enhanced incremental actions + PnL-focused rewards)\n",
    "6. Training Helpers (A2C)\n",
    "7. Bulk Config Testing (reads external JSON of runs)\n",
    "8. Evaluation & Visualization\n",
    "\n",
    "Notes:\n",
    "- Signals use a moving-average pseudo-spread and rolling z-score to define trading zones.\n",
    "- The environment uses incremental position changes and PnL-focused rewards to reduce static positions.\n",
    "- Bulk testing reads a separate JSON file to sweep hyperparameters and environment knobs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60534a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports ready\n"
     ]
    }
   ],
   "source": [
    "# ===================== IMPORTS =====================\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, Optional, Tuple, List\n",
    "\n",
    "# RL / Env\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement\n",
    "\n",
    "# Viz\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (15, 8)\n",
    "plt.rcParams['figure.dpi'] = 110\n",
    "\n",
    "print(\"âœ… Imports ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6ffb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Defaults loaded. 15D episode length: 21600 minutes\n"
     ]
    }
   ],
   "source": [
    "# ===================== CONFIGURATION (defaults + JSON override) =====================\n",
    "\n",
    "# Data\n",
    "DATA_PATH = \"../ETHUSDT_1m_with_indicators.parquet\"\n",
    "TIMESTAMP_COL = \"ts\"\n",
    "PRICE_COL = \"close\"\n",
    "OUTPUT_DIR = \"./processed_data_15d\"\n",
    "\n",
    "# Signal generation (pseudo-spread + z-score)\n",
    "MA_PERIOD = 60\n",
    "WINDOW_SIZE = 120\n",
    "OPEN_THRESHOLD = 2.0\n",
    "CLOSE_THRESHOLD = 0.5\n",
    "\n",
    "# Trading\n",
    "SEED_MONEY = 10000.0\n",
    "FEE_RATE = 0.00005\n",
    "SLIPPAGE = 0.00005\n",
    "\n",
    "# Episode sizing for 15 days\n",
    "MINUTES_PER_DAY = 24 * 60\n",
    "EPISODE_LENGTH = 15 * MINUTES_PER_DAY\n",
    "EPISODE_LENGTH_EVAL = 15 * MINUTES_PER_DAY\n",
    "\n",
    "# Temporal split\n",
    "TRAIN_DURATION_MINUTES = 365 * MINUTES_PER_DAY\n",
    "TEST_DURATION_MINUTES = 28 * MINUTES_PER_DAY\n",
    "MIN_DATA_BUFFER = 200\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Enhanced rewards / actions\n",
    "INCREMENTAL_STEP_SIZE = 0.3\n",
    "PNL_REWARD_SCALE = 80.0\n",
    "MOMENTUM_REWARD_SCALE = 0.5\n",
    "ACTIVITY_REWARD_SCALE = 0.1\n",
    "STATIC_DELTA_THRESH = 0.015\n",
    "STATIC_PENALTY_BASE = 0.05\n",
    "STATIC_PENALTY_ESCALATION = 0.2\n",
    "\n",
    "# A2C defaults\n",
    "TOTAL_TIMESTEPS = 100_000\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 1024\n",
    "GAMMA = 0.99\n",
    "GAE_LAMBDA = 0.95\n",
    "ENT_COEF = 0.02\n",
    "VF_COEF = 0.5\n",
    "MAX_GRAD_NORM = 0.5\n",
    "\n",
    "# Bulk config file path (for sweeps)\n",
    "BULK_CONFIG_PATH = \"../Test_drl_training/training_config_15d.json\"  # 15D-specific sweep file\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"ðŸ”§ Defaults loaded. 15D episode length: {EPISODE_LENGTH} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086f9d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Utilities ready\n"
     ]
    }
   ],
   "source": [
    "# ===================== UTILITIES =====================\n",
    "from enum import Enum\n",
    "\n",
    "class TradingZone(Enum):\n",
    "    LONG = 0\n",
    "    NEUTRAL_LONG = 1\n",
    "    CLOSE = 2\n",
    "    NEUTRAL_SHORT = 3\n",
    "    SHORT = 4\n",
    "\n",
    "def calculate_pseudo_spread(prices: pd.Series, ma_period: int, window_size: int) -> pd.DataFrame:\n",
    "    ma = prices.rolling(window=ma_period, min_periods=1).mean()\n",
    "    spread = prices - ma\n",
    "    spread_mean = spread.rolling(window=window_size, min_periods=1).mean()\n",
    "    spread_std = spread.rolling(window=window_size, min_periods=1).std()\n",
    "    z_score = (spread - spread_mean) / (spread_std + 1e-8)\n",
    "    z_score = z_score.fillna(0)\n",
    "    out = pd.DataFrame({\n",
    "        'price': prices,\n",
    "        'ma': ma,\n",
    "        'spread': spread,\n",
    "        'z_score': z_score\n",
    "    })\n",
    "    return out\n",
    "\n",
    "def z_to_zone(z: float, open_thresh: float, close_thresh: float) -> int:\n",
    "    if z > open_thresh:\n",
    "        return TradingZone.SHORT.value\n",
    "    elif z > close_thresh:\n",
    "        return TradingZone.NEUTRAL_SHORT.value\n",
    "    elif z >= -close_thresh:\n",
    "        return TradingZone.CLOSE.value\n",
    "    elif z >= -open_thresh:\n",
    "        return TradingZone.NEUTRAL_LONG.value\n",
    "    else:\n",
    "        return TradingZone.LONG.value\n",
    "\n",
    "def create_temporal_data_split(df: pd.DataFrame, train_minutes: int, test_minutes: int,\n",
    "                               min_buffer: int, seed: Optional[int]) -> Tuple[pd.DataFrame, pd.DataFrame, Dict[str, Any]]:\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    total = len(df)\n",
    "    required = train_minutes + test_minutes + min_buffer\n",
    "    if total < required:\n",
    "        raise ValueError(f\"Insufficient data: need {required}, have {total}\")\n",
    "    max_train_start = total - train_minutes - test_minutes\n",
    "    min_train_start = min_buffer\n",
    "    start = np.random.randint(min_train_start, max_train_start)\n",
    "    train_df = df.iloc[start:start+train_minutes].reset_index(drop=True)\n",
    "    test_df = df.iloc[start+train_minutes:start+train_minutes+test_minutes].reset_index(drop=True)\n",
    "    info = {\n",
    "        'train_start_idx': int(start),\n",
    "        'train_end_idx': int(start+train_minutes),\n",
    "        'test_start_idx': int(start+train_minutes),\n",
    "        'test_end_idx': int(start+train_minutes+test_minutes)\n",
    "    }\n",
    "    return train_df, test_df, info\n",
    "\n",
    "def calculate_metrics(nav_series: np.ndarray, initial: float) -> Dict[str, float]:\n",
    "    if len(nav_series) == 0:\n",
    "        return {}\n",
    "    final_value = float(nav_series[-1])\n",
    "    returns = np.diff(nav_series) / nav_series[:-1] if len(nav_series) > 1 else np.array([0.0])\n",
    "    sharpe = (np.mean(returns) / (np.std(returns) + 1e-12)) * np.sqrt(525600) if len(returns) > 1 else 0.0\n",
    "    peak = np.maximum.accumulate(nav_series)\n",
    "    drawdown = (nav_series - peak) / (peak + 1e-12)\n",
    "    return {\n",
    "        'final_value': final_value,\n",
    "        'final_nav': final_value / initial,\n",
    "        'total_return': (final_value - initial) / initial,\n",
    "        'sharpe': float(sharpe),\n",
    "        'max_drawdown': float(abs(np.min(drawdown)))\n",
    "    }\n",
    "\n",
    "print(\"âœ… Utilities ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d3ca95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Train: 525,600 rows | Test: 40,320 rows | Features: ['z_score', 'zone']\n"
     ]
    }
   ],
   "source": [
    "# ===================== DATA LOADING & FEATURES =====================\n",
    "\n",
    "def load_and_prepare(data_path: str) -> Tuple[pd.DataFrame, pd.DataFrame, List[str], Dict[str, Any]]:\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"Missing data file: {data_path}\")\n",
    "    df = pd.read_parquet(data_path)\n",
    "    if df.index.name and 'ts' in str(df.index.name):\n",
    "        df = df.reset_index()\n",
    "    if TIMESTAMP_COL not in df.columns:\n",
    "        for cand in ['timestamp', 'time', 'date', 'datetime', 'ts']:\n",
    "            if cand in df.columns:\n",
    "                df = df.rename(columns={cand: TIMESTAMP_COL})\n",
    "                break\n",
    "        else:\n",
    "            raise ValueError(f\"Timestamp column not found. Available: {list(df.columns)}\")\n",
    "    df = df.sort_values(TIMESTAMP_COL).reset_index(drop=True)\n",
    "    df[PRICE_COL] = pd.to_numeric(df[PRICE_COL], errors='coerce')\n",
    "    df = df.dropna(subset=[PRICE_COL])\n",
    "    df = df[df[PRICE_COL] > 0]\n",
    "\n",
    "    sig = calculate_pseudo_spread(df[PRICE_COL], MA_PERIOD, WINDOW_SIZE)\n",
    "    df = pd.concat([df.reset_index(drop=True), sig.reset_index(drop=True)], axis=1)\n",
    "    df['zone'] = df['z_score'].apply(lambda z: z_to_zone(z, OPEN_THRESHOLD, CLOSE_THRESHOLD))\n",
    "\n",
    "    features = ['z_score', 'zone']\n",
    "    train_df, test_df, info = create_temporal_data_split(\n",
    "        df, TRAIN_DURATION_MINUTES, TEST_DURATION_MINUTES, MIN_DATA_BUFFER, RANDOM_SEED\n",
    "    )\n",
    "\n",
    "    return train_df, test_df, features, info\n",
    "\n",
    "train_df, test_df, feature_cols, split_info = load_and_prepare(DATA_PATH)\n",
    "print(f\"ðŸ“Š Train: {len(train_df):,} rows | Test: {len(test_df):,} rows | Features: {feature_cols}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e819b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 15D Environment ready\n"
     ]
    }
   ],
   "source": [
    "# ===================== ENVIRONMENT (15D Enhanced) =====================\n",
    "\n",
    "class TradingEnv15D(gym.Env):\n",
    "    metadata = {\"render_modes\": []}\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, feature_cols: List[str], episode_length: int,\n",
    "                 seed_money: float,\n",
    "                 incremental_step: float,\n",
    "                 pnl_scale: float,\n",
    "                 static_delta_thresh: float):\n",
    "        super().__init__()\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.feature_cols = feature_cols\n",
    "        self.episode_length = int(episode_length)\n",
    "        self.seed_money = float(seed_money)\n",
    "        self.incremental_step = float(incremental_step)\n",
    "        self.pnl_scale = float(pnl_scale)\n",
    "        self.static_delta_thresh = float(static_delta_thresh)\n",
    "\n",
    "        self.prices = self.df[PRICE_COL].values\n",
    "        self.z_scores = self.df['z_score'].values\n",
    "        self.zones = self.df['zone'].values\n",
    "\n",
    "        self.min_start = max(MA_PERIOD, WINDOW_SIZE)\n",
    "        self.max_start = len(self.df) - self.episode_length - 2\n",
    "\n",
    "        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([-1.0, -5.0, 0.0], dtype=np.float32),\n",
    "            high=np.array([1.0, 5.0, 4.0], dtype=np.float32),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.position = 0.0\n",
    "        self.portfolio_value = self.seed_money\n",
    "        self.cash = self.seed_money\n",
    "        self.shares = 0.0\n",
    "        self.current_step = 0\n",
    "        self.episode_start = 0\n",
    "        self.static_steps = 0\n",
    "\n",
    "    def _obs(self) -> np.ndarray:\n",
    "        i = min(self.current_step, len(self.z_scores)-1)\n",
    "        return np.array([float(self.position), float(self.z_scores[i]), float(self.zones[i])], dtype=np.float32)\n",
    "\n",
    "    def reset(self, seed: Optional[int] = None, options: Optional[Dict[str, Any]] = None):\n",
    "        super().reset(seed=seed)\n",
    "        self.episode_start = self.np_random.integers(self.min_start, self.max_start)\n",
    "        self.current_step = self.episode_start\n",
    "        self.position = 0.0\n",
    "        self.portfolio_value = self.seed_money\n",
    "        self.cash = self.seed_money\n",
    "        self.shares = 0.0\n",
    "        self.static_steps = 0\n",
    "        return self._obs(), {\"portfolio_value\": self.portfolio_value, \"nav\": self.portfolio_value / self.seed_money}\n",
    "\n",
    "    def step(self, action: np.ndarray):\n",
    "        if np.isscalar(action):\n",
    "            a = float(np.clip(action, -1.0, 1.0))\n",
    "        else:\n",
    "            a = float(np.clip(action[0], -1.0, 1.0))\n",
    "\n",
    "        # Incremental position change\n",
    "        new_position = float(np.clip(self.position + a * self.incremental_step, -1.0, 1.0))\n",
    "        delta = abs(new_position - self.position)\n",
    "        self.position = new_position\n",
    "\n",
    "        # Trade cost\n",
    "        i = min(self.current_step, len(self.prices)-2)\n",
    "        price_now = self.prices[i]\n",
    "        trade_value = delta * self.portfolio_value\n",
    "        cost = trade_value * (FEE_RATE + SLIPPAGE)\n",
    "\n",
    "        # Update holdings\n",
    "        target_equity = self.position * self.portfolio_value\n",
    "        self.shares = target_equity / price_now\n",
    "        self.cash = self.portfolio_value - target_equity - cost\n",
    "\n",
    "        # Portfolio update to next step\n",
    "        price_next = self.prices[i+1]\n",
    "        equity_value = self.shares * price_next\n",
    "        self.portfolio_value = self.cash + equity_value\n",
    "\n",
    "        # Rewards\n",
    "        price_ret = (price_next - price_now) / (price_now + 1e-12)\n",
    "        pnl_reward = ((equity_value + self.cash) - (self.cash + self.shares * price_now)) / max(self.portfolio_value, 1e-8)\n",
    "        pnl_reward *= self.pnl_scale\n",
    "\n",
    "        # Activity incentive / static penalty\n",
    "        if delta > self.static_delta_thresh:\n",
    "            activity_reward = ACTIVITY_REWARD_SCALE * delta\n",
    "            self.static_steps = 0\n",
    "        else:\n",
    "            self.static_steps += 1\n",
    "            activity_reward = -STATIC_PENALTY_BASE * (1 + STATIC_PENALTY_ESCALATION * self.static_steps)\n",
    "\n",
    "        reward = float(pnl_reward + activity_reward)\n",
    "\n",
    "        self.current_step += 1\n",
    "        terminated = self.current_step >= self.episode_start + self.episode_length\n",
    "\n",
    "        obs = self._obs()\n",
    "        info = {\"portfolio_value\": float(self.portfolio_value), \"nav\": float(self.portfolio_value / self.seed_money)}\n",
    "        return obs, reward, terminated, False, info\n",
    "\n",
    "print(\"âœ… 15D Environment ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72587b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== TRAINING HELPERS (A2C) =====================\n",
    "\n",
    "def make_train_env(incremental_step=INCREMENTAL_STEP_SIZE,\n",
    "                   pnl_scale=PNL_REWARD_SCALE,\n",
    "                   static_delta=STATIC_DELTA_THRESH):\n",
    "    def _fn():\n",
    "        return Monitor(TradingEnv15D(\n",
    "            df=train_df,\n",
    "            feature_cols=feature_cols,\n",
    "            episode_length=EPISODE_LENGTH,\n",
    "            seed_money=SEED_MONEY,\n",
    "            incremental_step=incremental_step,\n",
    "            pnl_scale=pnl_scale,\n",
    "            static_delta_thresh=static_delta\n",
    "        ))\n",
    "    return DummyVecEnv([_fn])\n",
    "\n",
    "def make_eval_env(incremental_step=INCREMENTAL_STEP_SIZE,\n",
    "                  pnl_scale=PNL_REWARD_SCALE,\n",
    "                  static_delta=STATIC_DELTA_THRESH):\n",
    "    def _fn():\n",
    "        return Monitor(TradingEnv15D(\n",
    "            df=test_df,\n",
    "            feature_cols=feature_cols,\n",
    "            episode_length=EPISODE_LENGTH_EVAL,\n",
    "            seed_money=SEED_MONEY,\n",
    "            incremental_step=incremental_step,\n",
    "            pnl_scale=pnl_scale,\n",
    "            static_delta_thresh=static_delta\n",
    "        ))\n",
    "    return DummyVecEnv([_fn])\n",
    "\n",
    "def train_a2c(config: Dict[str, Any], run_name: str) -> str:\n",
    "    lr = float(config.get('learning_rate', LEARNING_RATE))\n",
    "    n_steps = int(config.get('n_steps', BATCH_SIZE))\n",
    "    gamma = float(config.get('gamma', GAMMA))\n",
    "    gae_lambda = float(config.get('gae_lambda', GAE_LAMBDA))\n",
    "    ent_coef = float(config.get('ent_coef', ENT_COEF))\n",
    "    vf_coef = float(config.get('vf_coef', VF_COEF))\n",
    "    max_grad_norm = float(config.get('max_grad_norm', MAX_GRAD_NORM))\n",
    "    total_timesteps = int(config.get('total_timesteps', TOTAL_TIMESTEPS))\n",
    "\n",
    "    inc = float(config.get('increment_step_size', INCREMENTAL_STEP_SIZE))\n",
    "    pnl_scale = float(config.get('pnl_reward_scale', PNL_REWARD_SCALE))\n",
    "    static_delta = float(config.get('static_delta_thresh', STATIC_DELTA_THRESH))\n",
    "\n",
    "    env = make_train_env(inc, pnl_scale, static_delta)\n",
    "\n",
    "    model = A2C(\n",
    "        policy='MlpPolicy',\n",
    "        env=env,\n",
    "        learning_rate=lr,\n",
    "        n_steps=n_steps,\n",
    "        gamma=gamma,\n",
    "        gae_lambda=gae_lambda,\n",
    "        ent_coef=ent_coef,\n",
    "        vf_coef=vf_coef,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "        verbose=0,\n",
    "        seed=42,\n",
    "        device='auto'\n",
    "    )\n",
    "\n",
    "    eval_env = make_eval_env(inc, pnl_scale, static_delta)\n",
    "    early_stop = StopTrainingOnNoModelImprovement(max_no_improvement_evals=3, min_evals=3, verbose=0)\n",
    "    eval_cb = EvalCallback(\n",
    "        eval_env,\n",
    "        best_model_save_path=OUTPUT_DIR,\n",
    "        log_path=OUTPUT_DIR,\n",
    "        eval_freq=int(config.get('eval_freq', 25_000)),\n",
    "        n_eval_episodes=int(config.get('n_eval_episodes', 5)),\n",
    "        deterministic=True,\n",
    "        render=False,\n",
    "        callback_after_eval=early_stop\n",
    "    )\n",
    "\n",
    "    start = time.time()\n",
    "    model.learn(total_timesteps=total_timesteps, callback=eval_cb)\n",
    "    mins = (time.time() - start) / 60\n",
    "\n",
    "    model_path = os.path.join(OUTPUT_DIR, f\"a2c_{run_name}\")\n",
    "    model.save(model_path)\n",
    "    print(f\"ðŸ’¾ Saved {run_name} in {OUTPUT_DIR} ({mins:.1f}m)\")\n",
    "    return model_path + \".zip\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434b89c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== BULK CONFIG TESTING =====================\n",
    "\n",
    "import glob\n",
    "\n",
    "def load_bulk_configs(config_path: str) -> List[Dict[str, Any]]:\n",
    "    with open(config_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data.get('training_configurations', [])\n",
    "\n",
    "results_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "def run_bulk(config_path: str, limit: Optional[int] = None, name_prefix: str = \"15d\") -> pd.DataFrame:\n",
    "    configs = load_bulk_configs(config_path)\n",
    "    if limit is not None:\n",
    "        configs = configs[:limit]\n",
    "    print(f\"ðŸ§ª Running {len(configs)} configs from {config_path}\")\n",
    "\n",
    "    for idx, entry in enumerate(configs, 1):\n",
    "        cfg_name = entry.get('name', f'cfg_{idx:04d}')\n",
    "        params = entry.get('params', {})\n",
    "        run_name = f\"{name_prefix}_{cfg_name}\"\n",
    "        print(f\"[{idx}/{len(configs)}] Training {run_name}\")\n",
    "        model_zip = train_a2c(params, run_name)\n",
    "\n",
    "        # model-driven evaluation episode\n",
    "        env = make_eval_env(\n",
    "            params.get('increment_step_size', INCREMENTAL_STEP_SIZE),\n",
    "            params.get('pnl_reward_scale', PNL_REWARD_SCALE),\n",
    "            params.get('static_delta_thresh', STATIC_DELTA_THRESH)\n",
    "        )\n",
    "        try:\n",
    "            model = A2C.load(model_zip, env=env)\n",
    "        except Exception:\n",
    "            model = A2C.load(model_zip.replace('.zip',''), env=env)\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        navs = []\n",
    "        steps = 0\n",
    "        while not done and steps < EPISODE_LENGTH_EVAL:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            navs.append(info[0].get('portfolio_value', SEED_MONEY))\n",
    "            done = bool(done[0])\n",
    "            steps += 1\n",
    "        metrics = calculate_metrics(np.array(navs, dtype=float), SEED_MONEY)\n",
    "\n",
    "        row = {\n",
    "            'run_name': run_name,\n",
    "            **{k: v for k, v in params.items()},\n",
    "            **metrics,\n",
    "            'model_path': model_zip\n",
    "        }\n",
    "        results_rows.append(row)\n",
    "        if idx % 10 == 0:\n",
    "            pd.DataFrame(results_rows).to_csv(os.path.join(OUTPUT_DIR, 'bulk_results_partial.csv'), index=False)\n",
    "\n",
    "    df_res = pd.DataFrame(results_rows)\n",
    "    out_csv = os.path.join(OUTPUT_DIR, 'bulk_results.csv')\n",
    "    df_res.to_csv(out_csv, index=False)\n",
    "    print(f\"âœ… Bulk results saved: {out_csv}\")\n",
    "    return df_res\n",
    "\n",
    "# Kick off small dry run by default (you can increase limit)\n",
    "# bulk_results = run_bulk(BULK_CONFIG_PATH, limit=5, name_prefix=\"15d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea58238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Eval & viz ready\n"
     ]
    }
   ],
   "source": [
    "# ===================== EVALUATION & VISUALIZATION =====================\n",
    "\n",
    "def evaluate_model_simple(env, model, max_steps: int) -> Dict[str, Any]:\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    navs, positions = [], []\n",
    "    steps = 0\n",
    "    while not done and steps < max_steps:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        navs.append(info[0].get('portfolio_value', SEED_MONEY))\n",
    "        base = env.envs[0].env\n",
    "        positions.append(float(base.position))\n",
    "        done = bool(done[0])\n",
    "        steps += 1\n",
    "    metrics = calculate_metrics(np.array(navs, dtype=float), SEED_MONEY)\n",
    "    metrics['avg_abs_position'] = float(np.mean(np.abs(positions))) if positions else 0.0\n",
    "    return metrics, navs, positions\n",
    "\n",
    "\n",
    "def plot_nav(navs: List[float], title: str, save: Optional[str] = None):\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(navs, lw=1.5)\n",
    "    plt.axhline(y=SEED_MONEY, color='gray', ls='--', alpha=0.6)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Portfolio Value ($)')\n",
    "    plt.xlabel('Minutes')\n",
    "    plt.grid(alpha=0.3)\n",
    "    if save:\n",
    "        plt.savefig(save, dpi=200, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "print(\"âœ… Eval & viz ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3fd697",
   "metadata": {},
   "source": [
    "## How to run\n",
    "\n",
    "- Optional: create a 15D-specific JSON config (recommended) and set `BULK_CONFIG_PATH` to it.\n",
    "- Quick sanity check: uncomment the last line in the Bulk section to run a small `limit=5` sweep.\n",
    "- After bulk finishes, load `processed_data_15d/bulk_results.csv` and sort by `final_nav`.\n",
    "- Train a final model with the best config and then evaluate with `evaluate_model_simple` + `plot_nav`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee14737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== SELECT BEST AND VISUALIZE =====================\n",
    "\n",
    "# Load bulk results, pick top by NAV, re-evaluate and plot NAV\n",
    "\n",
    "def select_and_visualize(results_csv: str, top_k: int = 5):\n",
    "    if not os.path.exists(results_csv):\n",
    "        print(f\"No results found at {results_csv}\")\n",
    "        return\n",
    "    df = pd.read_csv(results_csv)\n",
    "    if 'final_nav' not in df.columns:\n",
    "        print(\"Results CSV missing 'final_nav'\")\n",
    "        print(df.head())\n",
    "        return\n",
    "    df_sorted = df.sort_values('final_nav', ascending=False).reset_index(drop=True)\n",
    "    print(df_sorted.head(top_k))\n",
    "\n",
    "    best = df_sorted.iloc[0].to_dict()\n",
    "    inc = float(best.get('increment_step_size', INCREMENTAL_STEP_SIZE))\n",
    "    pnl = float(best.get('pnl_reward_scale', PNL_REWARD_SCALE))\n",
    "    sdt = float(best.get('static_delta_thresh', STATIC_DELTA_THRESH))\n",
    "\n",
    "    env = make_eval_env(inc, pnl, sdt)\n",
    "    model_path = str(best['model_path'])\n",
    "    try:\n",
    "        model = A2C.load(model_path, env=env)\n",
    "    except Exception:\n",
    "        model = A2C.load(model_path.replace('.zip',''), env=env)\n",
    "\n",
    "    metrics, navs, positions = evaluate_model_simple(env, model, EPISODE_LENGTH_EVAL)\n",
    "    print(\"Best metrics:\", metrics)\n",
    "    plot_nav(navs, title=f\"Best run: {best.get('run_name','')} NAV={metrics.get('final_nav',0):.3f}\",\n",
    "             save=os.path.join(OUTPUT_DIR, f\"best_{best.get('run_name','')}_nav.png\"))\n",
    "\n",
    "# Example usage (uncomment after bulk):\n",
    "# select_and_visualize(os.path.join(OUTPUT_DIR, 'bulk_results.csv'), top_k=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b13078",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

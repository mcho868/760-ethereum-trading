{
  "config": {
    "config_id": "optimized_baseline_no_sentiment",
    "algorithm": "A2C",
    "description": "Baseline A2C configuration without sentiment signal",
    "reward_components": {
      "pnl_scale": 10.0,
      "pnl_normalization": "nav",
      "sharpe_weight": 0.2,
      "sharpe_window": 120,
      "transaction_penalty": 0.1,
      "fee_rate": 0.001,
      "slippage": 0.0005,
      "drawdown_threshold": 0.15,
      "drawdown_penalty": 5.0,
      "holding_penalty": 0.001,
      "holding_reward": 0.002,
      "max_hold_periods": 1440,
      "activity_reward": 1.0,
      "inactivity_penalty": 0.01
    },
    "model_params": {
      "learning_rate": 0.0003,
      "n_steps": 2048,
      "gamma": 0.99,
      "gae_lambda": 0.95,
      "ent_coef": 0.01,
      "vf_coef": 0.5,
      "max_grad_norm": 0.5,
      "normalize_advantage": true,
      "use_rms_prop": true,
      "use_sde": false
    },
    "training": {
      "total_timesteps": 30000,
      "eval_freq": 5000,
      "n_eval_episodes": 5
    }
  },
  "metrics": {
    "mean_reward": -102215.8203125,
    "training_time": 127.33521008491516,
    "total_timesteps": 30000,
    "config_id": "optimized_baseline_no_sentiment"
  },
  "test_reward": -96078.8359375,
  "model_path": "./models/optimized_baseline_no_sentiment_model",
  "comparison_with_baseline": {
    "validation_improvement": 1184979.875,
    "test_improvement": 1145426.75,
    "training_time_ratio": 0.6055344344015874
  },
  "notes": "Optimized baseline with improved reward scaling"
}
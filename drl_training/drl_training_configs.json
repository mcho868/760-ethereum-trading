{
  "metadata": {
    "total_configurations": 1000,
    "generation_date": "2025-09-16",
    "description": "DRL hyperparameter configurations for reward function optimization",
    "methodology_version": "15D_state_space_v1.0",
    "algorithms": [
      "A2C",
      "TD3"
    ]
  },
  "configurations": [
    {
      "config_id": "a2c_0001",
      "algorithm": "A2C",
      "description": "A2C configuration 1 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0002",
      "algorithm": "TD3",
      "description": "TD3 configuration 2 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0003",
      "algorithm": "TD3",
      "description": "TD3 configuration 3 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0004",
      "algorithm": "A2C",
      "description": "A2C configuration 4 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 512,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0005",
      "algorithm": "TD3",
      "description": "TD3 configuration 5 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 100000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0006",
      "algorithm": "A2C",
      "description": "A2C configuration 6 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0007",
      "algorithm": "A2C",
      "description": "A2C configuration 7 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0008",
      "algorithm": "TD3",
      "description": "TD3 configuration 8 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0009",
      "algorithm": "A2C",
      "description": "A2C configuration 9 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0010",
      "algorithm": "A2C",
      "description": "A2C configuration 10 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0011",
      "algorithm": "TD3",
      "description": "TD3 configuration 11 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 100000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0012",
      "algorithm": "TD3",
      "description": "TD3 configuration 12 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0013",
      "algorithm": "TD3",
      "description": "TD3 configuration 13 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 64,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0014",
      "algorithm": "A2C",
      "description": "A2C configuration 14 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0015",
      "algorithm": "TD3",
      "description": "TD3 configuration 15 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0016",
      "algorithm": "TD3",
      "description": "TD3 configuration 16 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0017",
      "algorithm": "TD3",
      "description": "TD3 configuration 17 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0018",
      "algorithm": "A2C",
      "description": "A2C configuration 18 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 512,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0019",
      "algorithm": "A2C",
      "description": "A2C configuration 19 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0020",
      "algorithm": "A2C",
      "description": "A2C configuration 20 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 2048,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0021",
      "algorithm": "TD3",
      "description": "TD3 configuration 21 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 1000000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0022",
      "algorithm": "TD3",
      "description": "TD3 configuration 22 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0023",
      "algorithm": "A2C",
      "description": "A2C configuration 23 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 2048,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0024",
      "algorithm": "A2C",
      "description": "A2C configuration 24 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0025",
      "algorithm": "A2C",
      "description": "A2C configuration 25 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.01,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0026",
      "algorithm": "TD3",
      "description": "TD3 configuration 26 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0027",
      "algorithm": "A2C",
      "description": "A2C configuration 27 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0028",
      "algorithm": "A2C",
      "description": "A2C configuration 28 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 512,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0029",
      "algorithm": "TD3",
      "description": "TD3 configuration 29 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0030",
      "algorithm": "TD3",
      "description": "TD3 configuration 30 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 1000000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0031",
      "algorithm": "A2C",
      "description": "A2C configuration 31 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0032",
      "algorithm": "TD3",
      "description": "TD3 configuration 32 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0033",
      "algorithm": "TD3",
      "description": "TD3 configuration 33 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0034",
      "algorithm": "TD3",
      "description": "TD3 configuration 34 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0035",
      "algorithm": "TD3",
      "description": "TD3 configuration 35 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 1000000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0036",
      "algorithm": "A2C",
      "description": "A2C configuration 36 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 2048,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0037",
      "algorithm": "A2C",
      "description": "A2C configuration 37 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0038",
      "algorithm": "A2C",
      "description": "A2C configuration 38 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0039",
      "algorithm": "A2C",
      "description": "A2C configuration 39 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0040",
      "algorithm": "A2C",
      "description": "A2C configuration 40 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 512,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0041",
      "algorithm": "TD3",
      "description": "TD3 configuration 41 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0042",
      "algorithm": "A2C",
      "description": "A2C configuration 42 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0043",
      "algorithm": "A2C",
      "description": "A2C configuration 43 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0044",
      "algorithm": "A2C",
      "description": "A2C configuration 44 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0045",
      "algorithm": "TD3",
      "description": "TD3 configuration 45 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0046",
      "algorithm": "A2C",
      "description": "A2C configuration 46 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0047",
      "algorithm": "A2C",
      "description": "A2C configuration 47 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0048",
      "algorithm": "TD3",
      "description": "TD3 configuration 48 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0049",
      "algorithm": "TD3",
      "description": "TD3 configuration 49 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 100000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0050",
      "algorithm": "TD3",
      "description": "TD3 configuration 50 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0051",
      "algorithm": "A2C",
      "description": "A2C configuration 51 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0052",
      "algorithm": "TD3",
      "description": "TD3 configuration 52 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0053",
      "algorithm": "A2C",
      "description": "A2C configuration 53 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0054",
      "algorithm": "TD3",
      "description": "TD3 configuration 54 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0055",
      "algorithm": "TD3",
      "description": "TD3 configuration 55 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0056",
      "algorithm": "A2C",
      "description": "A2C configuration 56 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 512,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0057",
      "algorithm": "TD3",
      "description": "TD3 configuration 57 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0058",
      "algorithm": "A2C",
      "description": "A2C configuration 58 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 512,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0059",
      "algorithm": "A2C",
      "description": "A2C configuration 59 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0060",
      "algorithm": "TD3",
      "description": "TD3 configuration 60 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0061",
      "algorithm": "TD3",
      "description": "TD3 configuration 61 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0062",
      "algorithm": "A2C",
      "description": "A2C configuration 62 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 2048,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0063",
      "algorithm": "A2C",
      "description": "A2C configuration 63 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0064",
      "algorithm": "TD3",
      "description": "TD3 configuration 64 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0065",
      "algorithm": "A2C",
      "description": "A2C configuration 65 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 512,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0066",
      "algorithm": "TD3",
      "description": "TD3 configuration 66 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0067",
      "algorithm": "TD3",
      "description": "TD3 configuration 67 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0068",
      "algorithm": "TD3",
      "description": "TD3 configuration 68 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0069",
      "algorithm": "TD3",
      "description": "TD3 configuration 69 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0070",
      "algorithm": "TD3",
      "description": "TD3 configuration 70 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0071",
      "algorithm": "A2C",
      "description": "A2C configuration 71 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.001,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0072",
      "algorithm": "A2C",
      "description": "A2C configuration 72 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 512,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0073",
      "algorithm": "TD3",
      "description": "TD3 configuration 73 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 1000000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0074",
      "algorithm": "TD3",
      "description": "TD3 configuration 74 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 1000000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0075",
      "algorithm": "TD3",
      "description": "TD3 configuration 75 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0076",
      "algorithm": "A2C",
      "description": "A2C configuration 76 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0077",
      "algorithm": "A2C",
      "description": "A2C configuration 77 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0078",
      "algorithm": "TD3",
      "description": "TD3 configuration 78 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0079",
      "algorithm": "TD3",
      "description": "TD3 configuration 79 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0080",
      "algorithm": "A2C",
      "description": "A2C configuration 80 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 2048,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0081",
      "algorithm": "TD3",
      "description": "TD3 configuration 81 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0082",
      "algorithm": "A2C",
      "description": "A2C configuration 82 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.001,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0083",
      "algorithm": "TD3",
      "description": "TD3 configuration 83 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0084",
      "algorithm": "TD3",
      "description": "TD3 configuration 84 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 500000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0085",
      "algorithm": "TD3",
      "description": "TD3 configuration 85 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0086",
      "algorithm": "TD3",
      "description": "TD3 configuration 86 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0087",
      "algorithm": "A2C",
      "description": "A2C configuration 87 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0088",
      "algorithm": "TD3",
      "description": "TD3 configuration 88 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0089",
      "algorithm": "A2C",
      "description": "A2C configuration 89 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0090",
      "algorithm": "A2C",
      "description": "A2C configuration 90 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0091",
      "algorithm": "A2C",
      "description": "A2C configuration 91 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0092",
      "algorithm": "A2C",
      "description": "A2C configuration 92 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0093",
      "algorithm": "TD3",
      "description": "TD3 configuration 93 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 500000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0094",
      "algorithm": "TD3",
      "description": "TD3 configuration 94 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0095",
      "algorithm": "A2C",
      "description": "A2C configuration 95 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0096",
      "algorithm": "TD3",
      "description": "TD3 configuration 96 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0097",
      "algorithm": "A2C",
      "description": "A2C configuration 97 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 512,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0098",
      "algorithm": "A2C",
      "description": "A2C configuration 98 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0099",
      "algorithm": "A2C",
      "description": "A2C configuration 99 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0100",
      "algorithm": "A2C",
      "description": "A2C configuration 100 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0101",
      "algorithm": "A2C",
      "description": "A2C configuration 101 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0102",
      "algorithm": "TD3",
      "description": "TD3 configuration 102 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0103",
      "algorithm": "TD3",
      "description": "TD3 configuration 103 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0104",
      "algorithm": "TD3",
      "description": "TD3 configuration 104 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 100000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0105",
      "algorithm": "A2C",
      "description": "A2C configuration 105 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0106",
      "algorithm": "A2C",
      "description": "A2C configuration 106 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0107",
      "algorithm": "A2C",
      "description": "A2C configuration 107 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 512,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0108",
      "algorithm": "TD3",
      "description": "TD3 configuration 108 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 1000000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0109",
      "algorithm": "A2C",
      "description": "A2C configuration 109 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0110",
      "algorithm": "A2C",
      "description": "A2C configuration 110 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0111",
      "algorithm": "TD3",
      "description": "TD3 configuration 111 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0112",
      "algorithm": "A2C",
      "description": "A2C configuration 112 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0113",
      "algorithm": "A2C",
      "description": "A2C configuration 113 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0114",
      "algorithm": "A2C",
      "description": "A2C configuration 114 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0115",
      "algorithm": "A2C",
      "description": "A2C configuration 115 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0116",
      "algorithm": "A2C",
      "description": "A2C configuration 116 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0117",
      "algorithm": "TD3",
      "description": "TD3 configuration 117 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0118",
      "algorithm": "A2C",
      "description": "A2C configuration 118 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0119",
      "algorithm": "A2C",
      "description": "A2C configuration 119 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0120",
      "algorithm": "TD3",
      "description": "TD3 configuration 120 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0121",
      "algorithm": "TD3",
      "description": "TD3 configuration 121 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 100000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0122",
      "algorithm": "A2C",
      "description": "A2C configuration 122 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0123",
      "algorithm": "A2C",
      "description": "A2C configuration 123 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 512,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0124",
      "algorithm": "A2C",
      "description": "A2C configuration 124 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0125",
      "algorithm": "A2C",
      "description": "A2C configuration 125 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0126",
      "algorithm": "A2C",
      "description": "A2C configuration 126 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0127",
      "algorithm": "TD3",
      "description": "TD3 configuration 127 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0128",
      "algorithm": "TD3",
      "description": "TD3 configuration 128 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0129",
      "algorithm": "A2C",
      "description": "A2C configuration 129 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0130",
      "algorithm": "A2C",
      "description": "A2C configuration 130 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0131",
      "algorithm": "TD3",
      "description": "TD3 configuration 131 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 100000,
        "batch_size": 512,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0132",
      "algorithm": "TD3",
      "description": "TD3 configuration 132 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0133",
      "algorithm": "TD3",
      "description": "TD3 configuration 133 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0134",
      "algorithm": "A2C",
      "description": "A2C configuration 134 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0135",
      "algorithm": "TD3",
      "description": "TD3 configuration 135 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 500000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0136",
      "algorithm": "TD3",
      "description": "TD3 configuration 136 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 1000000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0137",
      "algorithm": "TD3",
      "description": "TD3 configuration 137 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0138",
      "algorithm": "A2C",
      "description": "A2C configuration 138 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.01,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0139",
      "algorithm": "TD3",
      "description": "TD3 configuration 139 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0140",
      "algorithm": "A2C",
      "description": "A2C configuration 140 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0141",
      "algorithm": "TD3",
      "description": "TD3 configuration 141 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 100000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0142",
      "algorithm": "A2C",
      "description": "A2C configuration 142 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0143",
      "algorithm": "A2C",
      "description": "A2C configuration 143 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0144",
      "algorithm": "TD3",
      "description": "TD3 configuration 144 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0145",
      "algorithm": "TD3",
      "description": "TD3 configuration 145 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0146",
      "algorithm": "A2C",
      "description": "A2C configuration 146 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0147",
      "algorithm": "A2C",
      "description": "A2C configuration 147 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0148",
      "algorithm": "A2C",
      "description": "A2C configuration 148 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 2048,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0149",
      "algorithm": "A2C",
      "description": "A2C configuration 149 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0150",
      "algorithm": "A2C",
      "description": "A2C configuration 150 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0151",
      "algorithm": "TD3",
      "description": "TD3 configuration 151 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0152",
      "algorithm": "A2C",
      "description": "A2C configuration 152 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 512,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0153",
      "algorithm": "TD3",
      "description": "TD3 configuration 153 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0154",
      "algorithm": "A2C",
      "description": "A2C configuration 154 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0155",
      "algorithm": "A2C",
      "description": "A2C configuration 155 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 512,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.001,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0156",
      "algorithm": "TD3",
      "description": "TD3 configuration 156 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 100000,
        "batch_size": 512,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0157",
      "algorithm": "TD3",
      "description": "TD3 configuration 157 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0158",
      "algorithm": "A2C",
      "description": "A2C configuration 158 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0159",
      "algorithm": "TD3",
      "description": "TD3 configuration 159 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 100000,
        "batch_size": 64,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0160",
      "algorithm": "TD3",
      "description": "TD3 configuration 160 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0161",
      "algorithm": "TD3",
      "description": "TD3 configuration 161 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0162",
      "algorithm": "A2C",
      "description": "A2C configuration 162 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 2048,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0163",
      "algorithm": "A2C",
      "description": "A2C configuration 163 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0164",
      "algorithm": "A2C",
      "description": "A2C configuration 164 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0165",
      "algorithm": "A2C",
      "description": "A2C configuration 165 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0166",
      "algorithm": "TD3",
      "description": "TD3 configuration 166 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0167",
      "algorithm": "TD3",
      "description": "TD3 configuration 167 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0168",
      "algorithm": "TD3",
      "description": "TD3 configuration 168 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0169",
      "algorithm": "A2C",
      "description": "A2C configuration 169 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0170",
      "algorithm": "A2C",
      "description": "A2C configuration 170 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 2048,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0171",
      "algorithm": "TD3",
      "description": "TD3 configuration 171 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0172",
      "algorithm": "TD3",
      "description": "TD3 configuration 172 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0173",
      "algorithm": "TD3",
      "description": "TD3 configuration 173 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0174",
      "algorithm": "A2C",
      "description": "A2C configuration 174 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0175",
      "algorithm": "TD3",
      "description": "TD3 configuration 175 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0176",
      "algorithm": "TD3",
      "description": "TD3 configuration 176 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0177",
      "algorithm": "TD3",
      "description": "TD3 configuration 177 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0178",
      "algorithm": "TD3",
      "description": "TD3 configuration 178 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0179",
      "algorithm": "TD3",
      "description": "TD3 configuration 179 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0180",
      "algorithm": "A2C",
      "description": "A2C configuration 180 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0181",
      "algorithm": "TD3",
      "description": "TD3 configuration 181 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0182",
      "algorithm": "TD3",
      "description": "TD3 configuration 182 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0183",
      "algorithm": "A2C",
      "description": "A2C configuration 183 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0184",
      "algorithm": "TD3",
      "description": "TD3 configuration 184 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 1000000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0185",
      "algorithm": "A2C",
      "description": "A2C configuration 185 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0186",
      "algorithm": "A2C",
      "description": "A2C configuration 186 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 2048,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0187",
      "algorithm": "TD3",
      "description": "TD3 configuration 187 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0188",
      "algorithm": "A2C",
      "description": "A2C configuration 188 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0189",
      "algorithm": "TD3",
      "description": "TD3 configuration 189 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0190",
      "algorithm": "TD3",
      "description": "TD3 configuration 190 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0191",
      "algorithm": "TD3",
      "description": "TD3 configuration 191 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 1000000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0192",
      "algorithm": "A2C",
      "description": "A2C configuration 192 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0193",
      "algorithm": "A2C",
      "description": "A2C configuration 193 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0194",
      "algorithm": "A2C",
      "description": "A2C configuration 194 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 512,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0195",
      "algorithm": "TD3",
      "description": "TD3 configuration 195 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0196",
      "algorithm": "TD3",
      "description": "TD3 configuration 196 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 100000,
        "batch_size": 64,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0197",
      "algorithm": "TD3",
      "description": "TD3 configuration 197 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0198",
      "algorithm": "A2C",
      "description": "A2C configuration 198 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0199",
      "algorithm": "A2C",
      "description": "A2C configuration 199 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 512,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0200",
      "algorithm": "TD3",
      "description": "TD3 configuration 200 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 100000,
        "batch_size": 64,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0201",
      "algorithm": "TD3",
      "description": "TD3 configuration 201 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0202",
      "algorithm": "TD3",
      "description": "TD3 configuration 202 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 500000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0203",
      "algorithm": "TD3",
      "description": "TD3 configuration 203 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0204",
      "algorithm": "TD3",
      "description": "TD3 configuration 204 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0205",
      "algorithm": "A2C",
      "description": "A2C configuration 205 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0206",
      "algorithm": "A2C",
      "description": "A2C configuration 206 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.01,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0207",
      "algorithm": "A2C",
      "description": "A2C configuration 207 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 512,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0208",
      "algorithm": "A2C",
      "description": "A2C configuration 208 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0209",
      "algorithm": "TD3",
      "description": "TD3 configuration 209 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0210",
      "algorithm": "A2C",
      "description": "A2C configuration 210 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0211",
      "algorithm": "TD3",
      "description": "TD3 configuration 211 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 1000000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0212",
      "algorithm": "TD3",
      "description": "TD3 configuration 212 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0213",
      "algorithm": "A2C",
      "description": "A2C configuration 213 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0214",
      "algorithm": "A2C",
      "description": "A2C configuration 214 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0215",
      "algorithm": "A2C",
      "description": "A2C configuration 215 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0216",
      "algorithm": "A2C",
      "description": "A2C configuration 216 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0217",
      "algorithm": "A2C",
      "description": "A2C configuration 217 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.001,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0218",
      "algorithm": "TD3",
      "description": "TD3 configuration 218 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0219",
      "algorithm": "A2C",
      "description": "A2C configuration 219 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0220",
      "algorithm": "A2C",
      "description": "A2C configuration 220 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0221",
      "algorithm": "TD3",
      "description": "TD3 configuration 221 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0222",
      "algorithm": "A2C",
      "description": "A2C configuration 222 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0223",
      "algorithm": "TD3",
      "description": "TD3 configuration 223 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0224",
      "algorithm": "A2C",
      "description": "A2C configuration 224 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0225",
      "algorithm": "TD3",
      "description": "TD3 configuration 225 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0226",
      "algorithm": "A2C",
      "description": "A2C configuration 226 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 512,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0227",
      "algorithm": "A2C",
      "description": "A2C configuration 227 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0228",
      "algorithm": "A2C",
      "description": "A2C configuration 228 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 512,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0229",
      "algorithm": "A2C",
      "description": "A2C configuration 229 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0230",
      "algorithm": "TD3",
      "description": "TD3 configuration 230 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0231",
      "algorithm": "A2C",
      "description": "A2C configuration 231 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0232",
      "algorithm": "TD3",
      "description": "TD3 configuration 232 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0233",
      "algorithm": "A2C",
      "description": "A2C configuration 233 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0234",
      "algorithm": "A2C",
      "description": "A2C configuration 234 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0235",
      "algorithm": "A2C",
      "description": "A2C configuration 235 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0236",
      "algorithm": "TD3",
      "description": "TD3 configuration 236 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0237",
      "algorithm": "A2C",
      "description": "A2C configuration 237 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0238",
      "algorithm": "A2C",
      "description": "A2C configuration 238 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 2048,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0239",
      "algorithm": "TD3",
      "description": "TD3 configuration 239 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0240",
      "algorithm": "TD3",
      "description": "TD3 configuration 240 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0241",
      "algorithm": "TD3",
      "description": "TD3 configuration 241 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0242",
      "algorithm": "TD3",
      "description": "TD3 configuration 242 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0243",
      "algorithm": "A2C",
      "description": "A2C configuration 243 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0244",
      "algorithm": "A2C",
      "description": "A2C configuration 244 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0245",
      "algorithm": "A2C",
      "description": "A2C configuration 245 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0246",
      "algorithm": "TD3",
      "description": "TD3 configuration 246 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0247",
      "algorithm": "TD3",
      "description": "TD3 configuration 247 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0248",
      "algorithm": "TD3",
      "description": "TD3 configuration 248 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 100000,
        "batch_size": 64,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0249",
      "algorithm": "TD3",
      "description": "TD3 configuration 249 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0250",
      "algorithm": "A2C",
      "description": "A2C configuration 250 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0251",
      "algorithm": "TD3",
      "description": "TD3 configuration 251 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0252",
      "algorithm": "TD3",
      "description": "TD3 configuration 252 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 100000,
        "batch_size": 64,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0253",
      "algorithm": "TD3",
      "description": "TD3 configuration 253 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0254",
      "algorithm": "TD3",
      "description": "TD3 configuration 254 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0255",
      "algorithm": "A2C",
      "description": "A2C configuration 255 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0256",
      "algorithm": "TD3",
      "description": "TD3 configuration 256 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0257",
      "algorithm": "A2C",
      "description": "A2C configuration 257 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0258",
      "algorithm": "TD3",
      "description": "TD3 configuration 258 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 64,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0259",
      "algorithm": "TD3",
      "description": "TD3 configuration 259 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0260",
      "algorithm": "TD3",
      "description": "TD3 configuration 260 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 100000,
        "batch_size": 64,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0261",
      "algorithm": "TD3",
      "description": "TD3 configuration 261 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0262",
      "algorithm": "A2C",
      "description": "A2C configuration 262 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0263",
      "algorithm": "TD3",
      "description": "TD3 configuration 263 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0264",
      "algorithm": "A2C",
      "description": "A2C configuration 264 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 2048,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0265",
      "algorithm": "TD3",
      "description": "TD3 configuration 265 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0266",
      "algorithm": "A2C",
      "description": "A2C configuration 266 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0267",
      "algorithm": "A2C",
      "description": "A2C configuration 267 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0268",
      "algorithm": "TD3",
      "description": "TD3 configuration 268 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0269",
      "algorithm": "TD3",
      "description": "TD3 configuration 269 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 500000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0270",
      "algorithm": "A2C",
      "description": "A2C configuration 270 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0271",
      "algorithm": "TD3",
      "description": "TD3 configuration 271 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0272",
      "algorithm": "A2C",
      "description": "A2C configuration 272 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 512,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0273",
      "algorithm": "TD3",
      "description": "TD3 configuration 273 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0274",
      "algorithm": "TD3",
      "description": "TD3 configuration 274 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0275",
      "algorithm": "A2C",
      "description": "A2C configuration 275 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0276",
      "algorithm": "A2C",
      "description": "A2C configuration 276 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0277",
      "algorithm": "TD3",
      "description": "TD3 configuration 277 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0278",
      "algorithm": "TD3",
      "description": "TD3 configuration 278 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0279",
      "algorithm": "A2C",
      "description": "A2C configuration 279 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0280",
      "algorithm": "A2C",
      "description": "A2C configuration 280 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0281",
      "algorithm": "A2C",
      "description": "A2C configuration 281 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0282",
      "algorithm": "TD3",
      "description": "TD3 configuration 282 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0283",
      "algorithm": "A2C",
      "description": "A2C configuration 283 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 512,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0284",
      "algorithm": "TD3",
      "description": "TD3 configuration 284 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 100000,
        "batch_size": 64,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0285",
      "algorithm": "TD3",
      "description": "TD3 configuration 285 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 500000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0286",
      "algorithm": "TD3",
      "description": "TD3 configuration 286 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0287",
      "algorithm": "TD3",
      "description": "TD3 configuration 287 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 1000000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0288",
      "algorithm": "A2C",
      "description": "A2C configuration 288 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0289",
      "algorithm": "TD3",
      "description": "TD3 configuration 289 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0290",
      "algorithm": "A2C",
      "description": "A2C configuration 290 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0291",
      "algorithm": "TD3",
      "description": "TD3 configuration 291 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0292",
      "algorithm": "A2C",
      "description": "A2C configuration 292 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.001,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0293",
      "algorithm": "A2C",
      "description": "A2C configuration 293 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0294",
      "algorithm": "A2C",
      "description": "A2C configuration 294 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0295",
      "algorithm": "TD3",
      "description": "TD3 configuration 295 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0296",
      "algorithm": "A2C",
      "description": "A2C configuration 296 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.01,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0297",
      "algorithm": "TD3",
      "description": "TD3 configuration 297 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 1000000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0298",
      "algorithm": "TD3",
      "description": "TD3 configuration 298 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0299",
      "algorithm": "TD3",
      "description": "TD3 configuration 299 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0300",
      "algorithm": "TD3",
      "description": "TD3 configuration 300 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0301",
      "algorithm": "TD3",
      "description": "TD3 configuration 301 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0302",
      "algorithm": "TD3",
      "description": "TD3 configuration 302 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0303",
      "algorithm": "A2C",
      "description": "A2C configuration 303 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0304",
      "algorithm": "A2C",
      "description": "A2C configuration 304 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0305",
      "algorithm": "TD3",
      "description": "TD3 configuration 305 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0306",
      "algorithm": "TD3",
      "description": "TD3 configuration 306 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0307",
      "algorithm": "A2C",
      "description": "A2C configuration 307 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0308",
      "algorithm": "A2C",
      "description": "A2C configuration 308 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0309",
      "algorithm": "TD3",
      "description": "TD3 configuration 309 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 500000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0310",
      "algorithm": "TD3",
      "description": "TD3 configuration 310 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0311",
      "algorithm": "A2C",
      "description": "A2C configuration 311 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 2048,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0312",
      "algorithm": "A2C",
      "description": "A2C configuration 312 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0313",
      "algorithm": "TD3",
      "description": "TD3 configuration 313 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0314",
      "algorithm": "A2C",
      "description": "A2C configuration 314 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 512,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0315",
      "algorithm": "TD3",
      "description": "TD3 configuration 315 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0316",
      "algorithm": "A2C",
      "description": "A2C configuration 316 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 2048,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0317",
      "algorithm": "A2C",
      "description": "A2C configuration 317 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0318",
      "algorithm": "A2C",
      "description": "A2C configuration 318 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0319",
      "algorithm": "A2C",
      "description": "A2C configuration 319 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 512,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0320",
      "algorithm": "A2C",
      "description": "A2C configuration 320 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0321",
      "algorithm": "TD3",
      "description": "TD3 configuration 321 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0322",
      "algorithm": "TD3",
      "description": "TD3 configuration 322 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0323",
      "algorithm": "TD3",
      "description": "TD3 configuration 323 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0324",
      "algorithm": "TD3",
      "description": "TD3 configuration 324 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 1000000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0325",
      "algorithm": "A2C",
      "description": "A2C configuration 325 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0326",
      "algorithm": "TD3",
      "description": "TD3 configuration 326 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0327",
      "algorithm": "A2C",
      "description": "A2C configuration 327 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0328",
      "algorithm": "TD3",
      "description": "TD3 configuration 328 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 100000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0329",
      "algorithm": "TD3",
      "description": "TD3 configuration 329 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0330",
      "algorithm": "TD3",
      "description": "TD3 configuration 330 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 500000,
        "batch_size": 256,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0331",
      "algorithm": "A2C",
      "description": "A2C configuration 331 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0332",
      "algorithm": "A2C",
      "description": "A2C configuration 332 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 512,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0333",
      "algorithm": "A2C",
      "description": "A2C configuration 333 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 2048,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0334",
      "algorithm": "A2C",
      "description": "A2C configuration 334 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 512,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0335",
      "algorithm": "TD3",
      "description": "TD3 configuration 335 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0336",
      "algorithm": "TD3",
      "description": "TD3 configuration 336 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 1000000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0337",
      "algorithm": "A2C",
      "description": "A2C configuration 337 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0338",
      "algorithm": "TD3",
      "description": "TD3 configuration 338 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0339",
      "algorithm": "TD3",
      "description": "TD3 configuration 339 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0340",
      "algorithm": "A2C",
      "description": "A2C configuration 340 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 512,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0341",
      "algorithm": "TD3",
      "description": "TD3 configuration 341 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 100000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0342",
      "algorithm": "TD3",
      "description": "TD3 configuration 342 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0343",
      "algorithm": "TD3",
      "description": "TD3 configuration 343 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0344",
      "algorithm": "TD3",
      "description": "TD3 configuration 344 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0345",
      "algorithm": "TD3",
      "description": "TD3 configuration 345 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0346",
      "algorithm": "TD3",
      "description": "TD3 configuration 346 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0347",
      "algorithm": "TD3",
      "description": "TD3 configuration 347 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0348",
      "algorithm": "A2C",
      "description": "A2C configuration 348 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0349",
      "algorithm": "A2C",
      "description": "A2C configuration 349 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 512,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.001,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0350",
      "algorithm": "A2C",
      "description": "A2C configuration 350 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0351",
      "algorithm": "A2C",
      "description": "A2C configuration 351 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0352",
      "algorithm": "TD3",
      "description": "TD3 configuration 352 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0353",
      "algorithm": "A2C",
      "description": "A2C configuration 353 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 512,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0354",
      "algorithm": "TD3",
      "description": "TD3 configuration 354 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0355",
      "algorithm": "A2C",
      "description": "A2C configuration 355 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0356",
      "algorithm": "TD3",
      "description": "TD3 configuration 356 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0357",
      "algorithm": "TD3",
      "description": "TD3 configuration 357 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0358",
      "algorithm": "TD3",
      "description": "TD3 configuration 358 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 500000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0359",
      "algorithm": "A2C",
      "description": "A2C configuration 359 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0360",
      "algorithm": "TD3",
      "description": "TD3 configuration 360 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0361",
      "algorithm": "A2C",
      "description": "A2C configuration 361 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 2048,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0362",
      "algorithm": "TD3",
      "description": "TD3 configuration 362 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 64,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0363",
      "algorithm": "TD3",
      "description": "TD3 configuration 363 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0364",
      "algorithm": "TD3",
      "description": "TD3 configuration 364 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0365",
      "algorithm": "TD3",
      "description": "TD3 configuration 365 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 100000,
        "batch_size": 512,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0366",
      "algorithm": "A2C",
      "description": "A2C configuration 366 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0367",
      "algorithm": "A2C",
      "description": "A2C configuration 367 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0368",
      "algorithm": "TD3",
      "description": "TD3 configuration 368 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0369",
      "algorithm": "TD3",
      "description": "TD3 configuration 369 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0370",
      "algorithm": "A2C",
      "description": "A2C configuration 370 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0371",
      "algorithm": "A2C",
      "description": "A2C configuration 371 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0372",
      "algorithm": "A2C",
      "description": "A2C configuration 372 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0373",
      "algorithm": "TD3",
      "description": "TD3 configuration 373 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0374",
      "algorithm": "TD3",
      "description": "TD3 configuration 374 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0375",
      "algorithm": "TD3",
      "description": "TD3 configuration 375 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0376",
      "algorithm": "A2C",
      "description": "A2C configuration 376 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0377",
      "algorithm": "TD3",
      "description": "TD3 configuration 377 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0378",
      "algorithm": "A2C",
      "description": "A2C configuration 378 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0379",
      "algorithm": "TD3",
      "description": "TD3 configuration 379 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 64,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0380",
      "algorithm": "A2C",
      "description": "A2C configuration 380 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0381",
      "algorithm": "A2C",
      "description": "A2C configuration 381 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0382",
      "algorithm": "TD3",
      "description": "TD3 configuration 382 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 500000,
        "batch_size": 256,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0383",
      "algorithm": "A2C",
      "description": "A2C configuration 383 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0384",
      "algorithm": "TD3",
      "description": "TD3 configuration 384 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0385",
      "algorithm": "A2C",
      "description": "A2C configuration 385 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0386",
      "algorithm": "TD3",
      "description": "TD3 configuration 386 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0387",
      "algorithm": "A2C",
      "description": "A2C configuration 387 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0388",
      "algorithm": "A2C",
      "description": "A2C configuration 388 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0389",
      "algorithm": "TD3",
      "description": "TD3 configuration 389 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0390",
      "algorithm": "TD3",
      "description": "TD3 configuration 390 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0391",
      "algorithm": "TD3",
      "description": "TD3 configuration 391 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0392",
      "algorithm": "TD3",
      "description": "TD3 configuration 392 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0393",
      "algorithm": "TD3",
      "description": "TD3 configuration 393 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0394",
      "algorithm": "TD3",
      "description": "TD3 configuration 394 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0395",
      "algorithm": "A2C",
      "description": "A2C configuration 395 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0396",
      "algorithm": "TD3",
      "description": "TD3 configuration 396 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0397",
      "algorithm": "TD3",
      "description": "TD3 configuration 397 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0398",
      "algorithm": "TD3",
      "description": "TD3 configuration 398 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0399",
      "algorithm": "TD3",
      "description": "TD3 configuration 399 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 1000000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0400",
      "algorithm": "A2C",
      "description": "A2C configuration 400 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 512,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.01,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0401",
      "algorithm": "A2C",
      "description": "A2C configuration 401 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0402",
      "algorithm": "A2C",
      "description": "A2C configuration 402 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0403",
      "algorithm": "A2C",
      "description": "A2C configuration 403 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0404",
      "algorithm": "A2C",
      "description": "A2C configuration 404 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 512,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0405",
      "algorithm": "TD3",
      "description": "TD3 configuration 405 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0406",
      "algorithm": "TD3",
      "description": "TD3 configuration 406 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0407",
      "algorithm": "A2C",
      "description": "A2C configuration 407 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0408",
      "algorithm": "A2C",
      "description": "A2C configuration 408 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0409",
      "algorithm": "A2C",
      "description": "A2C configuration 409 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0410",
      "algorithm": "A2C",
      "description": "A2C configuration 410 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0411",
      "algorithm": "A2C",
      "description": "A2C configuration 411 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0412",
      "algorithm": "A2C",
      "description": "A2C configuration 412 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0413",
      "algorithm": "TD3",
      "description": "TD3 configuration 413 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0414",
      "algorithm": "A2C",
      "description": "A2C configuration 414 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0415",
      "algorithm": "A2C",
      "description": "A2C configuration 415 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0416",
      "algorithm": "TD3",
      "description": "TD3 configuration 416 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0417",
      "algorithm": "A2C",
      "description": "A2C configuration 417 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0418",
      "algorithm": "A2C",
      "description": "A2C configuration 418 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0419",
      "algorithm": "A2C",
      "description": "A2C configuration 419 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0420",
      "algorithm": "A2C",
      "description": "A2C configuration 420 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0421",
      "algorithm": "A2C",
      "description": "A2C configuration 421 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 512,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0422",
      "algorithm": "TD3",
      "description": "TD3 configuration 422 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0423",
      "algorithm": "TD3",
      "description": "TD3 configuration 423 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0424",
      "algorithm": "A2C",
      "description": "A2C configuration 424 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0425",
      "algorithm": "A2C",
      "description": "A2C configuration 425 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0426",
      "algorithm": "A2C",
      "description": "A2C configuration 426 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0427",
      "algorithm": "A2C",
      "description": "A2C configuration 427 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0428",
      "algorithm": "A2C",
      "description": "A2C configuration 428 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0429",
      "algorithm": "TD3",
      "description": "TD3 configuration 429 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0430",
      "algorithm": "TD3",
      "description": "TD3 configuration 430 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0431",
      "algorithm": "TD3",
      "description": "TD3 configuration 431 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 100000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0432",
      "algorithm": "TD3",
      "description": "TD3 configuration 432 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 500000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0433",
      "algorithm": "TD3",
      "description": "TD3 configuration 433 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0434",
      "algorithm": "TD3",
      "description": "TD3 configuration 434 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 500000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0435",
      "algorithm": "TD3",
      "description": "TD3 configuration 435 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0436",
      "algorithm": "TD3",
      "description": "TD3 configuration 436 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0437",
      "algorithm": "A2C",
      "description": "A2C configuration 437 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0438",
      "algorithm": "TD3",
      "description": "TD3 configuration 438 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0439",
      "algorithm": "TD3",
      "description": "TD3 configuration 439 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0440",
      "algorithm": "TD3",
      "description": "TD3 configuration 440 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0441",
      "algorithm": "TD3",
      "description": "TD3 configuration 441 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0442",
      "algorithm": "TD3",
      "description": "TD3 configuration 442 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0443",
      "algorithm": "A2C",
      "description": "A2C configuration 443 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0444",
      "algorithm": "A2C",
      "description": "A2C configuration 444 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0445",
      "algorithm": "A2C",
      "description": "A2C configuration 445 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0446",
      "algorithm": "A2C",
      "description": "A2C configuration 446 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0447",
      "algorithm": "A2C",
      "description": "A2C configuration 447 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 2048,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0448",
      "algorithm": "A2C",
      "description": "A2C configuration 448 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0449",
      "algorithm": "TD3",
      "description": "TD3 configuration 449 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0450",
      "algorithm": "A2C",
      "description": "A2C configuration 450 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 512,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0451",
      "algorithm": "A2C",
      "description": "A2C configuration 451 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0452",
      "algorithm": "A2C",
      "description": "A2C configuration 452 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0453",
      "algorithm": "A2C",
      "description": "A2C configuration 453 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0454",
      "algorithm": "TD3",
      "description": "TD3 configuration 454 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0455",
      "algorithm": "A2C",
      "description": "A2C configuration 455 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0456",
      "algorithm": "A2C",
      "description": "A2C configuration 456 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0457",
      "algorithm": "A2C",
      "description": "A2C configuration 457 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0458",
      "algorithm": "TD3",
      "description": "TD3 configuration 458 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0459",
      "algorithm": "TD3",
      "description": "TD3 configuration 459 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0460",
      "algorithm": "A2C",
      "description": "A2C configuration 460 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.001,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0461",
      "algorithm": "A2C",
      "description": "A2C configuration 461 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 2048,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0462",
      "algorithm": "A2C",
      "description": "A2C configuration 462 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0463",
      "algorithm": "TD3",
      "description": "TD3 configuration 463 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0464",
      "algorithm": "TD3",
      "description": "TD3 configuration 464 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0465",
      "algorithm": "A2C",
      "description": "A2C configuration 465 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0466",
      "algorithm": "A2C",
      "description": "A2C configuration 466 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0467",
      "algorithm": "A2C",
      "description": "A2C configuration 467 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0468",
      "algorithm": "TD3",
      "description": "TD3 configuration 468 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 1000000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0469",
      "algorithm": "TD3",
      "description": "TD3 configuration 469 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0470",
      "algorithm": "A2C",
      "description": "A2C configuration 470 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0471",
      "algorithm": "TD3",
      "description": "TD3 configuration 471 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0472",
      "algorithm": "TD3",
      "description": "TD3 configuration 472 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0473",
      "algorithm": "A2C",
      "description": "A2C configuration 473 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0474",
      "algorithm": "A2C",
      "description": "A2C configuration 474 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.001,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0475",
      "algorithm": "TD3",
      "description": "TD3 configuration 475 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0476",
      "algorithm": "A2C",
      "description": "A2C configuration 476 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0477",
      "algorithm": "TD3",
      "description": "TD3 configuration 477 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0478",
      "algorithm": "A2C",
      "description": "A2C configuration 478 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0479",
      "algorithm": "A2C",
      "description": "A2C configuration 479 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0480",
      "algorithm": "A2C",
      "description": "A2C configuration 480 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 2048,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0481",
      "algorithm": "TD3",
      "description": "TD3 configuration 481 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0482",
      "algorithm": "TD3",
      "description": "TD3 configuration 482 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0483",
      "algorithm": "A2C",
      "description": "A2C configuration 483 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 512,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0484",
      "algorithm": "A2C",
      "description": "A2C configuration 484 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 512,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0485",
      "algorithm": "TD3",
      "description": "TD3 configuration 485 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0486",
      "algorithm": "TD3",
      "description": "TD3 configuration 486 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0487",
      "algorithm": "A2C",
      "description": "A2C configuration 487 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0488",
      "algorithm": "TD3",
      "description": "TD3 configuration 488 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0489",
      "algorithm": "A2C",
      "description": "A2C configuration 489 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0490",
      "algorithm": "TD3",
      "description": "TD3 configuration 490 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0491",
      "algorithm": "TD3",
      "description": "TD3 configuration 491 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0492",
      "algorithm": "TD3",
      "description": "TD3 configuration 492 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 100000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0493",
      "algorithm": "TD3",
      "description": "TD3 configuration 493 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 500000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0494",
      "algorithm": "TD3",
      "description": "TD3 configuration 494 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 500000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0495",
      "algorithm": "A2C",
      "description": "A2C configuration 495 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 512,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0496",
      "algorithm": "A2C",
      "description": "A2C configuration 496 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0497",
      "algorithm": "TD3",
      "description": "TD3 configuration 497 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0498",
      "algorithm": "A2C",
      "description": "A2C configuration 498 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.001,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0499",
      "algorithm": "TD3",
      "description": "TD3 configuration 499 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0500",
      "algorithm": "A2C",
      "description": "A2C configuration 500 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 512,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0501",
      "algorithm": "A2C",
      "description": "A2C configuration 501 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.01,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0502",
      "algorithm": "TD3",
      "description": "TD3 configuration 502 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0503",
      "algorithm": "A2C",
      "description": "A2C configuration 503 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 512,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0504",
      "algorithm": "A2C",
      "description": "A2C configuration 504 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0505",
      "algorithm": "A2C",
      "description": "A2C configuration 505 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 512,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0506",
      "algorithm": "TD3",
      "description": "TD3 configuration 506 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 100000,
        "batch_size": 64,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0507",
      "algorithm": "TD3",
      "description": "TD3 configuration 507 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0508",
      "algorithm": "TD3",
      "description": "TD3 configuration 508 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0509",
      "algorithm": "TD3",
      "description": "TD3 configuration 509 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0510",
      "algorithm": "A2C",
      "description": "A2C configuration 510 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0511",
      "algorithm": "A2C",
      "description": "A2C configuration 511 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0512",
      "algorithm": "A2C",
      "description": "A2C configuration 512 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 512,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0513",
      "algorithm": "A2C",
      "description": "A2C configuration 513 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 2048,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0514",
      "algorithm": "A2C",
      "description": "A2C configuration 514 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0515",
      "algorithm": "TD3",
      "description": "TD3 configuration 515 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0516",
      "algorithm": "A2C",
      "description": "A2C configuration 516 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 512,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0517",
      "algorithm": "TD3",
      "description": "TD3 configuration 517 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0518",
      "algorithm": "A2C",
      "description": "A2C configuration 518 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0519",
      "algorithm": "A2C",
      "description": "A2C configuration 519 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 2048,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0520",
      "algorithm": "TD3",
      "description": "TD3 configuration 520 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0521",
      "algorithm": "TD3",
      "description": "TD3 configuration 521 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0522",
      "algorithm": "TD3",
      "description": "TD3 configuration 522 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 500000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0523",
      "algorithm": "A2C",
      "description": "A2C configuration 523 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0524",
      "algorithm": "A2C",
      "description": "A2C configuration 524 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0525",
      "algorithm": "A2C",
      "description": "A2C configuration 525 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0526",
      "algorithm": "A2C",
      "description": "A2C configuration 526 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.001,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0527",
      "algorithm": "TD3",
      "description": "TD3 configuration 527 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 100000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0528",
      "algorithm": "A2C",
      "description": "A2C configuration 528 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0529",
      "algorithm": "TD3",
      "description": "TD3 configuration 529 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0530",
      "algorithm": "TD3",
      "description": "TD3 configuration 530 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0531",
      "algorithm": "A2C",
      "description": "A2C configuration 531 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0532",
      "algorithm": "TD3",
      "description": "TD3 configuration 532 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 500000,
        "batch_size": 256,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0533",
      "algorithm": "A2C",
      "description": "A2C configuration 533 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 2048,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0534",
      "algorithm": "A2C",
      "description": "A2C configuration 534 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0535",
      "algorithm": "TD3",
      "description": "TD3 configuration 535 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0536",
      "algorithm": "A2C",
      "description": "A2C configuration 536 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0537",
      "algorithm": "A2C",
      "description": "A2C configuration 537 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0538",
      "algorithm": "TD3",
      "description": "TD3 configuration 538 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 500000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0539",
      "algorithm": "TD3",
      "description": "TD3 configuration 539 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0540",
      "algorithm": "A2C",
      "description": "A2C configuration 540 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0541",
      "algorithm": "A2C",
      "description": "A2C configuration 541 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0542",
      "algorithm": "TD3",
      "description": "TD3 configuration 542 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0543",
      "algorithm": "A2C",
      "description": "A2C configuration 543 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0544",
      "algorithm": "A2C",
      "description": "A2C configuration 544 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0545",
      "algorithm": "A2C",
      "description": "A2C configuration 545 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0546",
      "algorithm": "TD3",
      "description": "TD3 configuration 546 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0547",
      "algorithm": "A2C",
      "description": "A2C configuration 547 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.01,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0548",
      "algorithm": "TD3",
      "description": "TD3 configuration 548 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0549",
      "algorithm": "A2C",
      "description": "A2C configuration 549 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0550",
      "algorithm": "A2C",
      "description": "A2C configuration 550 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0551",
      "algorithm": "TD3",
      "description": "TD3 configuration 551 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0552",
      "algorithm": "TD3",
      "description": "TD3 configuration 552 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0553",
      "algorithm": "TD3",
      "description": "TD3 configuration 553 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0554",
      "algorithm": "TD3",
      "description": "TD3 configuration 554 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0555",
      "algorithm": "TD3",
      "description": "TD3 configuration 555 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0556",
      "algorithm": "A2C",
      "description": "A2C configuration 556 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 512,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0557",
      "algorithm": "TD3",
      "description": "TD3 configuration 557 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0558",
      "algorithm": "A2C",
      "description": "A2C configuration 558 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 512,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0559",
      "algorithm": "TD3",
      "description": "TD3 configuration 559 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0560",
      "algorithm": "A2C",
      "description": "A2C configuration 560 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0561",
      "algorithm": "TD3",
      "description": "TD3 configuration 561 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 500000,
        "batch_size": 256,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0562",
      "algorithm": "TD3",
      "description": "TD3 configuration 562 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0563",
      "algorithm": "A2C",
      "description": "A2C configuration 563 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 512,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0564",
      "algorithm": "A2C",
      "description": "A2C configuration 564 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0565",
      "algorithm": "A2C",
      "description": "A2C configuration 565 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0566",
      "algorithm": "TD3",
      "description": "TD3 configuration 566 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0567",
      "algorithm": "A2C",
      "description": "A2C configuration 567 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0568",
      "algorithm": "A2C",
      "description": "A2C configuration 568 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0569",
      "algorithm": "A2C",
      "description": "A2C configuration 569 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0570",
      "algorithm": "TD3",
      "description": "TD3 configuration 570 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0571",
      "algorithm": "TD3",
      "description": "TD3 configuration 571 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0572",
      "algorithm": "A2C",
      "description": "A2C configuration 572 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 512,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0573",
      "algorithm": "TD3",
      "description": "TD3 configuration 573 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 1000000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0574",
      "algorithm": "A2C",
      "description": "A2C configuration 574 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0575",
      "algorithm": "TD3",
      "description": "TD3 configuration 575 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0576",
      "algorithm": "TD3",
      "description": "TD3 configuration 576 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0577",
      "algorithm": "A2C",
      "description": "A2C configuration 577 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.001,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0578",
      "algorithm": "A2C",
      "description": "A2C configuration 578 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0579",
      "algorithm": "A2C",
      "description": "A2C configuration 579 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0580",
      "algorithm": "TD3",
      "description": "TD3 configuration 580 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0581",
      "algorithm": "A2C",
      "description": "A2C configuration 581 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0582",
      "algorithm": "A2C",
      "description": "A2C configuration 582 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0583",
      "algorithm": "TD3",
      "description": "TD3 configuration 583 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0584",
      "algorithm": "A2C",
      "description": "A2C configuration 584 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0585",
      "algorithm": "TD3",
      "description": "TD3 configuration 585 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0586",
      "algorithm": "TD3",
      "description": "TD3 configuration 586 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0587",
      "algorithm": "TD3",
      "description": "TD3 configuration 587 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 1000000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0588",
      "algorithm": "TD3",
      "description": "TD3 configuration 588 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0589",
      "algorithm": "TD3",
      "description": "TD3 configuration 589 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0590",
      "algorithm": "TD3",
      "description": "TD3 configuration 590 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0591",
      "algorithm": "TD3",
      "description": "TD3 configuration 591 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 500000,
        "batch_size": 256,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0592",
      "algorithm": "TD3",
      "description": "TD3 configuration 592 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0593",
      "algorithm": "A2C",
      "description": "A2C configuration 593 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0594",
      "algorithm": "A2C",
      "description": "A2C configuration 594 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 512,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0595",
      "algorithm": "A2C",
      "description": "A2C configuration 595 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0596",
      "algorithm": "TD3",
      "description": "TD3 configuration 596 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0597",
      "algorithm": "TD3",
      "description": "TD3 configuration 597 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0598",
      "algorithm": "TD3",
      "description": "TD3 configuration 598 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0599",
      "algorithm": "TD3",
      "description": "TD3 configuration 599 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0600",
      "algorithm": "TD3",
      "description": "TD3 configuration 600 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0601",
      "algorithm": "A2C",
      "description": "A2C configuration 601 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0602",
      "algorithm": "TD3",
      "description": "TD3 configuration 602 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0603",
      "algorithm": "A2C",
      "description": "A2C configuration 603 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0604",
      "algorithm": "A2C",
      "description": "A2C configuration 604 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0605",
      "algorithm": "A2C",
      "description": "A2C configuration 605 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0606",
      "algorithm": "TD3",
      "description": "TD3 configuration 606 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 500000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0607",
      "algorithm": "A2C",
      "description": "A2C configuration 607 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0608",
      "algorithm": "TD3",
      "description": "TD3 configuration 608 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0609",
      "algorithm": "A2C",
      "description": "A2C configuration 609 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0610",
      "algorithm": "TD3",
      "description": "TD3 configuration 610 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0611",
      "algorithm": "A2C",
      "description": "A2C configuration 611 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0612",
      "algorithm": "TD3",
      "description": "TD3 configuration 612 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0613",
      "algorithm": "A2C",
      "description": "A2C configuration 613 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0614",
      "algorithm": "TD3",
      "description": "TD3 configuration 614 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 1000000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0615",
      "algorithm": "TD3",
      "description": "TD3 configuration 615 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 100000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0616",
      "algorithm": "A2C",
      "description": "A2C configuration 616 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 512,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0617",
      "algorithm": "TD3",
      "description": "TD3 configuration 617 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0618",
      "algorithm": "TD3",
      "description": "TD3 configuration 618 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0619",
      "algorithm": "A2C",
      "description": "A2C configuration 619 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0620",
      "algorithm": "A2C",
      "description": "A2C configuration 620 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0621",
      "algorithm": "A2C",
      "description": "A2C configuration 621 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 2048,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0622",
      "algorithm": "TD3",
      "description": "TD3 configuration 622 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0623",
      "algorithm": "A2C",
      "description": "A2C configuration 623 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 512,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0624",
      "algorithm": "TD3",
      "description": "TD3 configuration 624 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0625",
      "algorithm": "A2C",
      "description": "A2C configuration 625 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 512,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0626",
      "algorithm": "A2C",
      "description": "A2C configuration 626 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 2048,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.01,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0627",
      "algorithm": "TD3",
      "description": "TD3 configuration 627 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0628",
      "algorithm": "TD3",
      "description": "TD3 configuration 628 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0629",
      "algorithm": "A2C",
      "description": "A2C configuration 629 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0630",
      "algorithm": "A2C",
      "description": "A2C configuration 630 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0631",
      "algorithm": "TD3",
      "description": "TD3 configuration 631 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0632",
      "algorithm": "A2C",
      "description": "A2C configuration 632 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 2048,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0633",
      "algorithm": "A2C",
      "description": "A2C configuration 633 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0634",
      "algorithm": "A2C",
      "description": "A2C configuration 634 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0635",
      "algorithm": "A2C",
      "description": "A2C configuration 635 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0636",
      "algorithm": "TD3",
      "description": "TD3 configuration 636 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0637",
      "algorithm": "TD3",
      "description": "TD3 configuration 637 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 500000,
        "batch_size": 256,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0638",
      "algorithm": "TD3",
      "description": "TD3 configuration 638 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 100000,
        "batch_size": 512,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0639",
      "algorithm": "TD3",
      "description": "TD3 configuration 639 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0640",
      "algorithm": "TD3",
      "description": "TD3 configuration 640 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0641",
      "algorithm": "A2C",
      "description": "A2C configuration 641 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0642",
      "algorithm": "A2C",
      "description": "A2C configuration 642 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0643",
      "algorithm": "TD3",
      "description": "TD3 configuration 643 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0644",
      "algorithm": "TD3",
      "description": "TD3 configuration 644 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0645",
      "algorithm": "TD3",
      "description": "TD3 configuration 645 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0646",
      "algorithm": "TD3",
      "description": "TD3 configuration 646 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 100000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0647",
      "algorithm": "A2C",
      "description": "A2C configuration 647 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0648",
      "algorithm": "TD3",
      "description": "TD3 configuration 648 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0649",
      "algorithm": "A2C",
      "description": "A2C configuration 649 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0650",
      "algorithm": "TD3",
      "description": "TD3 configuration 650 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 100000,
        "batch_size": 64,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0651",
      "algorithm": "TD3",
      "description": "TD3 configuration 651 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0652",
      "algorithm": "TD3",
      "description": "TD3 configuration 652 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0653",
      "algorithm": "A2C",
      "description": "A2C configuration 653 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 512,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0654",
      "algorithm": "TD3",
      "description": "TD3 configuration 654 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 500000,
        "batch_size": 256,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0655",
      "algorithm": "TD3",
      "description": "TD3 configuration 655 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0656",
      "algorithm": "TD3",
      "description": "TD3 configuration 656 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 100000,
        "batch_size": 512,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0657",
      "algorithm": "A2C",
      "description": "A2C configuration 657 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0658",
      "algorithm": "TD3",
      "description": "TD3 configuration 658 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 1000000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0659",
      "algorithm": "TD3",
      "description": "TD3 configuration 659 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 100000,
        "batch_size": 512,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0660",
      "algorithm": "A2C",
      "description": "A2C configuration 660 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0661",
      "algorithm": "TD3",
      "description": "TD3 configuration 661 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0662",
      "algorithm": "TD3",
      "description": "TD3 configuration 662 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 100000,
        "batch_size": 512,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0663",
      "algorithm": "A2C",
      "description": "A2C configuration 663 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0664",
      "algorithm": "A2C",
      "description": "A2C configuration 664 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0665",
      "algorithm": "TD3",
      "description": "TD3 configuration 665 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0666",
      "algorithm": "TD3",
      "description": "TD3 configuration 666 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0667",
      "algorithm": "TD3",
      "description": "TD3 configuration 667 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 500000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0668",
      "algorithm": "TD3",
      "description": "TD3 configuration 668 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0669",
      "algorithm": "A2C",
      "description": "A2C configuration 669 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0670",
      "algorithm": "TD3",
      "description": "TD3 configuration 670 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0671",
      "algorithm": "A2C",
      "description": "A2C configuration 671 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.01,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0672",
      "algorithm": "A2C",
      "description": "A2C configuration 672 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0673",
      "algorithm": "TD3",
      "description": "TD3 configuration 673 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0674",
      "algorithm": "TD3",
      "description": "TD3 configuration 674 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0675",
      "algorithm": "A2C",
      "description": "A2C configuration 675 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0676",
      "algorithm": "TD3",
      "description": "TD3 configuration 676 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0677",
      "algorithm": "TD3",
      "description": "TD3 configuration 677 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0678",
      "algorithm": "A2C",
      "description": "A2C configuration 678 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 512,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0679",
      "algorithm": "TD3",
      "description": "TD3 configuration 679 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0680",
      "algorithm": "A2C",
      "description": "A2C configuration 680 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 2048,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0681",
      "algorithm": "A2C",
      "description": "A2C configuration 681 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0682",
      "algorithm": "A2C",
      "description": "A2C configuration 682 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.001,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0683",
      "algorithm": "A2C",
      "description": "A2C configuration 683 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0684",
      "algorithm": "TD3",
      "description": "TD3 configuration 684 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0685",
      "algorithm": "A2C",
      "description": "A2C configuration 685 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0686",
      "algorithm": "A2C",
      "description": "A2C configuration 686 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0687",
      "algorithm": "TD3",
      "description": "TD3 configuration 687 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 100000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0688",
      "algorithm": "A2C",
      "description": "A2C configuration 688 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0689",
      "algorithm": "A2C",
      "description": "A2C configuration 689 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0690",
      "algorithm": "TD3",
      "description": "TD3 configuration 690 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0691",
      "algorithm": "TD3",
      "description": "TD3 configuration 691 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0692",
      "algorithm": "TD3",
      "description": "TD3 configuration 692 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0693",
      "algorithm": "A2C",
      "description": "A2C configuration 693 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.001,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0694",
      "algorithm": "TD3",
      "description": "TD3 configuration 694 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0695",
      "algorithm": "TD3",
      "description": "TD3 configuration 695 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 100000,
        "batch_size": 64,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0696",
      "algorithm": "A2C",
      "description": "A2C configuration 696 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0697",
      "algorithm": "A2C",
      "description": "A2C configuration 697 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0698",
      "algorithm": "A2C",
      "description": "A2C configuration 698 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0699",
      "algorithm": "A2C",
      "description": "A2C configuration 699 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0700",
      "algorithm": "TD3",
      "description": "TD3 configuration 700 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0701",
      "algorithm": "TD3",
      "description": "TD3 configuration 701 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0702",
      "algorithm": "TD3",
      "description": "TD3 configuration 702 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0703",
      "algorithm": "A2C",
      "description": "A2C configuration 703 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0704",
      "algorithm": "TD3",
      "description": "TD3 configuration 704 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0705",
      "algorithm": "TD3",
      "description": "TD3 configuration 705 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 64,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0706",
      "algorithm": "A2C",
      "description": "A2C configuration 706 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0707",
      "algorithm": "A2C",
      "description": "A2C configuration 707 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0708",
      "algorithm": "A2C",
      "description": "A2C configuration 708 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0709",
      "algorithm": "TD3",
      "description": "TD3 configuration 709 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0710",
      "algorithm": "TD3",
      "description": "TD3 configuration 710 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0711",
      "algorithm": "A2C",
      "description": "A2C configuration 711 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0712",
      "algorithm": "TD3",
      "description": "TD3 configuration 712 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0713",
      "algorithm": "A2C",
      "description": "A2C configuration 713 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.001,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0714",
      "algorithm": "TD3",
      "description": "TD3 configuration 714 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0715",
      "algorithm": "A2C",
      "description": "A2C configuration 715 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0716",
      "algorithm": "A2C",
      "description": "A2C configuration 716 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0717",
      "algorithm": "TD3",
      "description": "TD3 configuration 717 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0718",
      "algorithm": "TD3",
      "description": "TD3 configuration 718 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0719",
      "algorithm": "TD3",
      "description": "TD3 configuration 719 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0720",
      "algorithm": "TD3",
      "description": "TD3 configuration 720 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0721",
      "algorithm": "A2C",
      "description": "A2C configuration 721 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 2048,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0722",
      "algorithm": "TD3",
      "description": "TD3 configuration 722 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0723",
      "algorithm": "TD3",
      "description": "TD3 configuration 723 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0724",
      "algorithm": "A2C",
      "description": "A2C configuration 724 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0725",
      "algorithm": "A2C",
      "description": "A2C configuration 725 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0726",
      "algorithm": "A2C",
      "description": "A2C configuration 726 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0727",
      "algorithm": "A2C",
      "description": "A2C configuration 727 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0728",
      "algorithm": "A2C",
      "description": "A2C configuration 728 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0729",
      "algorithm": "TD3",
      "description": "TD3 configuration 729 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0730",
      "algorithm": "A2C",
      "description": "A2C configuration 730 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0731",
      "algorithm": "TD3",
      "description": "TD3 configuration 731 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0732",
      "algorithm": "A2C",
      "description": "A2C configuration 732 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 512,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0733",
      "algorithm": "A2C",
      "description": "A2C configuration 733 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0734",
      "algorithm": "A2C",
      "description": "A2C configuration 734 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0735",
      "algorithm": "TD3",
      "description": "TD3 configuration 735 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 100000,
        "batch_size": 64,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0736",
      "algorithm": "TD3",
      "description": "TD3 configuration 736 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0737",
      "algorithm": "A2C",
      "description": "A2C configuration 737 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0738",
      "algorithm": "TD3",
      "description": "TD3 configuration 738 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0739",
      "algorithm": "A2C",
      "description": "A2C configuration 739 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 2048,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0740",
      "algorithm": "TD3",
      "description": "TD3 configuration 740 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0741",
      "algorithm": "TD3",
      "description": "TD3 configuration 741 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 1000000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0742",
      "algorithm": "TD3",
      "description": "TD3 configuration 742 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0743",
      "algorithm": "TD3",
      "description": "TD3 configuration 743 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0744",
      "algorithm": "A2C",
      "description": "A2C configuration 744 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 2048,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0745",
      "algorithm": "TD3",
      "description": "TD3 configuration 745 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0746",
      "algorithm": "TD3",
      "description": "TD3 configuration 746 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 500000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0747",
      "algorithm": "A2C",
      "description": "A2C configuration 747 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0748",
      "algorithm": "A2C",
      "description": "A2C configuration 748 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0749",
      "algorithm": "A2C",
      "description": "A2C configuration 749 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0750",
      "algorithm": "TD3",
      "description": "TD3 configuration 750 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0751",
      "algorithm": "TD3",
      "description": "TD3 configuration 751 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0752",
      "algorithm": "A2C",
      "description": "A2C configuration 752 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0753",
      "algorithm": "A2C",
      "description": "A2C configuration 753 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0754",
      "algorithm": "TD3",
      "description": "TD3 configuration 754 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 100000,
        "batch_size": 64,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0755",
      "algorithm": "TD3",
      "description": "TD3 configuration 755 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0756",
      "algorithm": "TD3",
      "description": "TD3 configuration 756 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 500000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0757",
      "algorithm": "A2C",
      "description": "A2C configuration 757 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 512,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0758",
      "algorithm": "A2C",
      "description": "A2C configuration 758 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 512,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0759",
      "algorithm": "TD3",
      "description": "TD3 configuration 759 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 500000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0760",
      "algorithm": "TD3",
      "description": "TD3 configuration 760 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 1000000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0761",
      "algorithm": "TD3",
      "description": "TD3 configuration 761 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0762",
      "algorithm": "TD3",
      "description": "TD3 configuration 762 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0763",
      "algorithm": "A2C",
      "description": "A2C configuration 763 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0764",
      "algorithm": "TD3",
      "description": "TD3 configuration 764 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0765",
      "algorithm": "TD3",
      "description": "TD3 configuration 765 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 100000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0766",
      "algorithm": "A2C",
      "description": "A2C configuration 766 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0767",
      "algorithm": "A2C",
      "description": "A2C configuration 767 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 2048,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0768",
      "algorithm": "TD3",
      "description": "TD3 configuration 768 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0769",
      "algorithm": "TD3",
      "description": "TD3 configuration 769 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0770",
      "algorithm": "A2C",
      "description": "A2C configuration 770 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 512,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0771",
      "algorithm": "TD3",
      "description": "TD3 configuration 771 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0772",
      "algorithm": "A2C",
      "description": "A2C configuration 772 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0773",
      "algorithm": "TD3",
      "description": "TD3 configuration 773 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0774",
      "algorithm": "A2C",
      "description": "A2C configuration 774 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0775",
      "algorithm": "TD3",
      "description": "TD3 configuration 775 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0776",
      "algorithm": "TD3",
      "description": "TD3 configuration 776 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0777",
      "algorithm": "A2C",
      "description": "A2C configuration 777 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0778",
      "algorithm": "TD3",
      "description": "TD3 configuration 778 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0779",
      "algorithm": "TD3",
      "description": "TD3 configuration 779 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 100000,
        "batch_size": 64,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0780",
      "algorithm": "TD3",
      "description": "TD3 configuration 780 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0781",
      "algorithm": "TD3",
      "description": "TD3 configuration 781 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0782",
      "algorithm": "TD3",
      "description": "TD3 configuration 782 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0783",
      "algorithm": "TD3",
      "description": "TD3 configuration 783 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0784",
      "algorithm": "A2C",
      "description": "A2C configuration 784 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0785",
      "algorithm": "TD3",
      "description": "TD3 configuration 785 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 500000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0786",
      "algorithm": "TD3",
      "description": "TD3 configuration 786 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0787",
      "algorithm": "A2C",
      "description": "A2C configuration 787 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0788",
      "algorithm": "A2C",
      "description": "A2C configuration 788 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0789",
      "algorithm": "A2C",
      "description": "A2C configuration 789 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0790",
      "algorithm": "A2C",
      "description": "A2C configuration 790 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 512,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0791",
      "algorithm": "TD3",
      "description": "TD3 configuration 791 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0792",
      "algorithm": "TD3",
      "description": "TD3 configuration 792 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0793",
      "algorithm": "TD3",
      "description": "TD3 configuration 793 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0794",
      "algorithm": "TD3",
      "description": "TD3 configuration 794 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 500000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0795",
      "algorithm": "TD3",
      "description": "TD3 configuration 795 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 1000000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0796",
      "algorithm": "TD3",
      "description": "TD3 configuration 796 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 500000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0797",
      "algorithm": "TD3",
      "description": "TD3 configuration 797 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 500000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0798",
      "algorithm": "A2C",
      "description": "A2C configuration 798 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0799",
      "algorithm": "A2C",
      "description": "A2C configuration 799 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0800",
      "algorithm": "TD3",
      "description": "TD3 configuration 800 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0801",
      "algorithm": "TD3",
      "description": "TD3 configuration 801 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0802",
      "algorithm": "TD3",
      "description": "TD3 configuration 802 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0803",
      "algorithm": "A2C",
      "description": "A2C configuration 803 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0804",
      "algorithm": "TD3",
      "description": "TD3 configuration 804 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0805",
      "algorithm": "TD3",
      "description": "TD3 configuration 805 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0806",
      "algorithm": "TD3",
      "description": "TD3 configuration 806 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0807",
      "algorithm": "A2C",
      "description": "A2C configuration 807 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0808",
      "algorithm": "TD3",
      "description": "TD3 configuration 808 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0809",
      "algorithm": "TD3",
      "description": "TD3 configuration 809 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 100000,
        "batch_size": 512,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0810",
      "algorithm": "A2C",
      "description": "A2C configuration 810 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0811",
      "algorithm": "A2C",
      "description": "A2C configuration 811 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0812",
      "algorithm": "A2C",
      "description": "A2C configuration 812 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0813",
      "algorithm": "TD3",
      "description": "TD3 configuration 813 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 500000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0814",
      "algorithm": "TD3",
      "description": "TD3 configuration 814 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0815",
      "algorithm": "TD3",
      "description": "TD3 configuration 815 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 100000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0816",
      "algorithm": "TD3",
      "description": "TD3 configuration 816 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0817",
      "algorithm": "TD3",
      "description": "TD3 configuration 817 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0818",
      "algorithm": "TD3",
      "description": "TD3 configuration 818 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0819",
      "algorithm": "A2C",
      "description": "A2C configuration 819 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0820",
      "algorithm": "A2C",
      "description": "A2C configuration 820 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0821",
      "algorithm": "A2C",
      "description": "A2C configuration 821 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 512,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0822",
      "algorithm": "A2C",
      "description": "A2C configuration 822 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0823",
      "algorithm": "A2C",
      "description": "A2C configuration 823 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.001,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0824",
      "algorithm": "TD3",
      "description": "TD3 configuration 824 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0825",
      "algorithm": "TD3",
      "description": "TD3 configuration 825 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0826",
      "algorithm": "A2C",
      "description": "A2C configuration 826 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 2048,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.001,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0827",
      "algorithm": "A2C",
      "description": "A2C configuration 827 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0828",
      "algorithm": "A2C",
      "description": "A2C configuration 828 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0829",
      "algorithm": "TD3",
      "description": "TD3 configuration 829 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 512,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0830",
      "algorithm": "A2C",
      "description": "A2C configuration 830 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0831",
      "algorithm": "TD3",
      "description": "TD3 configuration 831 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0832",
      "algorithm": "A2C",
      "description": "A2C configuration 832 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0833",
      "algorithm": "TD3",
      "description": "TD3 configuration 833 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0834",
      "algorithm": "A2C",
      "description": "A2C configuration 834 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0835",
      "algorithm": "TD3",
      "description": "TD3 configuration 835 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0836",
      "algorithm": "A2C",
      "description": "A2C configuration 836 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0837",
      "algorithm": "A2C",
      "description": "A2C configuration 837 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0838",
      "algorithm": "A2C",
      "description": "A2C configuration 838 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 512,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0839",
      "algorithm": "TD3",
      "description": "TD3 configuration 839 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0840",
      "algorithm": "A2C",
      "description": "A2C configuration 840 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0841",
      "algorithm": "A2C",
      "description": "A2C configuration 841 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0842",
      "algorithm": "A2C",
      "description": "A2C configuration 842 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.001,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0843",
      "algorithm": "TD3",
      "description": "TD3 configuration 843 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0844",
      "algorithm": "A2C",
      "description": "A2C configuration 844 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0845",
      "algorithm": "A2C",
      "description": "A2C configuration 845 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0846",
      "algorithm": "A2C",
      "description": "A2C configuration 846 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 512,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0847",
      "algorithm": "A2C",
      "description": "A2C configuration 847 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0848",
      "algorithm": "A2C",
      "description": "A2C configuration 848 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 512,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0849",
      "algorithm": "TD3",
      "description": "TD3 configuration 849 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 100000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0850",
      "algorithm": "TD3",
      "description": "TD3 configuration 850 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0851",
      "algorithm": "TD3",
      "description": "TD3 configuration 851 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0852",
      "algorithm": "A2C",
      "description": "A2C configuration 852 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0853",
      "algorithm": "A2C",
      "description": "A2C configuration 853 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0854",
      "algorithm": "TD3",
      "description": "TD3 configuration 854 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 500000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0855",
      "algorithm": "A2C",
      "description": "A2C configuration 855 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 512,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.001,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0856",
      "algorithm": "A2C",
      "description": "A2C configuration 856 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0857",
      "algorithm": "TD3",
      "description": "TD3 configuration 857 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0858",
      "algorithm": "A2C",
      "description": "A2C configuration 858 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0859",
      "algorithm": "TD3",
      "description": "TD3 configuration 859 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0860",
      "algorithm": "TD3",
      "description": "TD3 configuration 860 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 500000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0861",
      "algorithm": "A2C",
      "description": "A2C configuration 861 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 512,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0862",
      "algorithm": "A2C",
      "description": "A2C configuration 862 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 512,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0863",
      "algorithm": "A2C",
      "description": "A2C configuration 863 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0864",
      "algorithm": "TD3",
      "description": "TD3 configuration 864 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0865",
      "algorithm": "A2C",
      "description": "A2C configuration 865 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.001,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0866",
      "algorithm": "A2C",
      "description": "A2C configuration 866 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0867",
      "algorithm": "TD3",
      "description": "TD3 configuration 867 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0868",
      "algorithm": "TD3",
      "description": "TD3 configuration 868 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0869",
      "algorithm": "A2C",
      "description": "A2C configuration 869 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0870",
      "algorithm": "A2C",
      "description": "A2C configuration 870 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0871",
      "algorithm": "TD3",
      "description": "TD3 configuration 871 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0872",
      "algorithm": "TD3",
      "description": "TD3 configuration 872 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 100000,
        "batch_size": 512,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0873",
      "algorithm": "TD3",
      "description": "TD3 configuration 873 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0874",
      "algorithm": "A2C",
      "description": "A2C configuration 874 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0875",
      "algorithm": "A2C",
      "description": "A2C configuration 875 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0876",
      "algorithm": "A2C",
      "description": "A2C configuration 876 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0877",
      "algorithm": "TD3",
      "description": "TD3 configuration 877 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 1000000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0878",
      "algorithm": "TD3",
      "description": "TD3 configuration 878 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0879",
      "algorithm": "TD3",
      "description": "TD3 configuration 879 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0880",
      "algorithm": "A2C",
      "description": "A2C configuration 880 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0881",
      "algorithm": "A2C",
      "description": "A2C configuration 881 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0882",
      "algorithm": "TD3",
      "description": "TD3 configuration 882 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 500000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0883",
      "algorithm": "A2C",
      "description": "A2C configuration 883 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0884",
      "algorithm": "A2C",
      "description": "A2C configuration 884 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0885",
      "algorithm": "A2C",
      "description": "A2C configuration 885 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0886",
      "algorithm": "A2C",
      "description": "A2C configuration 886 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0887",
      "algorithm": "A2C",
      "description": "A2C configuration 887 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 2048,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0888",
      "algorithm": "A2C",
      "description": "A2C configuration 888 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0889",
      "algorithm": "A2C",
      "description": "A2C configuration 889 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.001,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0890",
      "algorithm": "A2C",
      "description": "A2C configuration 890 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 2048,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0891",
      "algorithm": "TD3",
      "description": "TD3 configuration 891 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0892",
      "algorithm": "TD3",
      "description": "TD3 configuration 892 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 1000000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0893",
      "algorithm": "A2C",
      "description": "A2C configuration 893 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0894",
      "algorithm": "A2C",
      "description": "A2C configuration 894 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0895",
      "algorithm": "TD3",
      "description": "TD3 configuration 895 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 100000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0896",
      "algorithm": "TD3",
      "description": "TD3 configuration 896 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0897",
      "algorithm": "TD3",
      "description": "TD3 configuration 897 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0898",
      "algorithm": "A2C",
      "description": "A2C configuration 898 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0899",
      "algorithm": "TD3",
      "description": "TD3 configuration 899 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 500000,
        "batch_size": 256,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0900",
      "algorithm": "TD3",
      "description": "TD3 configuration 900 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 1000000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0901",
      "algorithm": "A2C",
      "description": "A2C configuration 901 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0902",
      "algorithm": "TD3",
      "description": "TD3 configuration 902 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 1000000,
        "batch_size": 128,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0903",
      "algorithm": "A2C",
      "description": "A2C configuration 903 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0904",
      "algorithm": "TD3",
      "description": "TD3 configuration 904 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 100000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0905",
      "algorithm": "A2C",
      "description": "A2C configuration 905 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0906",
      "algorithm": "TD3",
      "description": "TD3 configuration 906 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 100000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0907",
      "algorithm": "A2C",
      "description": "A2C configuration 907 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 512,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0908",
      "algorithm": "A2C",
      "description": "A2C configuration 908 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0909",
      "algorithm": "A2C",
      "description": "A2C configuration 909 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0910",
      "algorithm": "A2C",
      "description": "A2C configuration 910 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0911",
      "algorithm": "TD3",
      "description": "TD3 configuration 911 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0912",
      "algorithm": "A2C",
      "description": "A2C configuration 912 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0913",
      "algorithm": "A2C",
      "description": "A2C configuration 913 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0914",
      "algorithm": "A2C",
      "description": "A2C configuration 914 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 512,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0915",
      "algorithm": "A2C",
      "description": "A2C configuration 915 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 512,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0916",
      "algorithm": "TD3",
      "description": "TD3 configuration 916 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0917",
      "algorithm": "A2C",
      "description": "A2C configuration 917 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0918",
      "algorithm": "A2C",
      "description": "A2C configuration 918 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0919",
      "algorithm": "A2C",
      "description": "A2C configuration 919 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0920",
      "algorithm": "TD3",
      "description": "TD3 configuration 920 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0921",
      "algorithm": "A2C",
      "description": "A2C configuration 921 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 512,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0922",
      "algorithm": "TD3",
      "description": "TD3 configuration 922 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0923",
      "algorithm": "TD3",
      "description": "TD3 configuration 923 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0924",
      "algorithm": "A2C",
      "description": "A2C configuration 924 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0925",
      "algorithm": "TD3",
      "description": "TD3 configuration 925 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0926",
      "algorithm": "A2C",
      "description": "A2C configuration 926 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 2048,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0927",
      "algorithm": "TD3",
      "description": "TD3 configuration 927 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 500000,
        "batch_size": 256,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0928",
      "algorithm": "TD3",
      "description": "TD3 configuration 928 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0929",
      "algorithm": "A2C",
      "description": "A2C configuration 929 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0930",
      "algorithm": "TD3",
      "description": "TD3 configuration 930 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0931",
      "algorithm": "TD3",
      "description": "TD3 configuration 931 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 1000000,
        "batch_size": 512,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0932",
      "algorithm": "TD3",
      "description": "TD3 configuration 932 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0933",
      "algorithm": "A2C",
      "description": "A2C configuration 933 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0934",
      "algorithm": "TD3",
      "description": "TD3 configuration 934 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0935",
      "algorithm": "A2C",
      "description": "A2C configuration 935 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 512,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0936",
      "algorithm": "A2C",
      "description": "A2C configuration 936 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0937",
      "algorithm": "TD3",
      "description": "TD3 configuration 937 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 500000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0938",
      "algorithm": "A2C",
      "description": "A2C configuration 938 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0939",
      "algorithm": "TD3",
      "description": "TD3 configuration 939 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0940",
      "algorithm": "TD3",
      "description": "TD3 configuration 940 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 500000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0941",
      "algorithm": "A2C",
      "description": "A2C configuration 941 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0942",
      "algorithm": "TD3",
      "description": "TD3 configuration 942 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 512,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0943",
      "algorithm": "A2C",
      "description": "A2C configuration 943 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 4096,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0944",
      "algorithm": "A2C",
      "description": "A2C configuration 944 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0945",
      "algorithm": "A2C",
      "description": "A2C configuration 945 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 512,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0946",
      "algorithm": "TD3",
      "description": "TD3 configuration 946 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0947",
      "algorithm": "A2C",
      "description": "A2C configuration 947 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 2048,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0948",
      "algorithm": "A2C",
      "description": "A2C configuration 948 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0949",
      "algorithm": "A2C",
      "description": "A2C configuration 949 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0950",
      "algorithm": "A2C",
      "description": "A2C configuration 950 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 512,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0951",
      "algorithm": "TD3",
      "description": "TD3 configuration 951 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 100000,
        "batch_size": 64,
        "tau": 0.01,
        "gamma": 0.995,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0952",
      "algorithm": "A2C",
      "description": "A2C configuration 952 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0953",
      "algorithm": "A2C",
      "description": "A2C configuration 953 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 512,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0954",
      "algorithm": "TD3",
      "description": "TD3 configuration 954 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0955",
      "algorithm": "TD3",
      "description": "TD3 configuration 955 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "td3_0956",
      "algorithm": "TD3",
      "description": "TD3 configuration 956 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 100000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0957",
      "algorithm": "TD3",
      "description": "TD3 configuration 957 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0958",
      "algorithm": "A2C",
      "description": "A2C configuration 958 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0959",
      "algorithm": "TD3",
      "description": "TD3 configuration 959 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0960",
      "algorithm": "TD3",
      "description": "TD3 configuration 960 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0961",
      "algorithm": "TD3",
      "description": "TD3 configuration 961 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0962",
      "algorithm": "A2C",
      "description": "A2C configuration 962 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0963",
      "algorithm": "TD3",
      "description": "TD3 configuration 963 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 100000,
        "batch_size": 64,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 2
      }
    },
    {
      "config_id": "td3_0964",
      "algorithm": "TD3",
      "description": "TD3 configuration 964 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "buffer_size": 1000000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0965",
      "algorithm": "A2C",
      "description": "A2C configuration 965 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 512,
        "gamma": 0.99,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0966",
      "algorithm": "TD3",
      "description": "TD3 configuration 966 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 100000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0967",
      "algorithm": "A2C",
      "description": "A2C configuration 967 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0968",
      "algorithm": "A2C",
      "description": "A2C configuration 968 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 2048,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0969",
      "algorithm": "A2C",
      "description": "A2C configuration 969 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.9,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0970",
      "algorithm": "A2C",
      "description": "A2C configuration 970 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0971",
      "algorithm": "TD3",
      "description": "TD3 configuration 971 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0972",
      "algorithm": "A2C",
      "description": "A2C configuration 972 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0973",
      "algorithm": "TD3",
      "description": "TD3 configuration 973 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 500000,
        "batch_size": 64,
        "tau": 0.01,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0974",
      "algorithm": "A2C",
      "description": "A2C configuration 974 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 512,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0975",
      "algorithm": "TD3",
      "description": "TD3 configuration 975 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 64,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0976",
      "algorithm": "A2C",
      "description": "A2C configuration 976 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 512,
        "gamma": 0.95,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0977",
      "algorithm": "TD3",
      "description": "TD3 configuration 977 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 1000000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 2
      }
    },
    {
      "config_id": "a2c_0978",
      "algorithm": "A2C",
      "description": "A2C configuration 978 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 2048,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0979",
      "algorithm": "TD3",
      "description": "TD3 configuration 979 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 20.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 100000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.99,
        "noise_std": 0.2,
        "target_noise": 0.3,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0980",
      "algorithm": "TD3",
      "description": "TD3 configuration 980 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0981",
      "algorithm": "A2C",
      "description": "A2C configuration 981 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.001,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0982",
      "algorithm": "A2C",
      "description": "A2C configuration 982 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0983",
      "algorithm": "A2C",
      "description": "A2C configuration 983 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0003,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "td3_0984",
      "algorithm": "TD3",
      "description": "TD3 configuration 984 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.01,
        "gamma": 0.95,
        "noise_std": 0.2,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0985",
      "algorithm": "A2C",
      "description": "A2C configuration 985 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 512,
        "gamma": 0.995,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "a2c_0986",
      "algorithm": "A2C",
      "description": "A2C configuration 986 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "a2c_0987",
      "algorithm": "A2C",
      "description": "A2C configuration 987 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 1024,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.02,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0988",
      "algorithm": "A2C",
      "description": "A2C configuration 988 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 512,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0989",
      "algorithm": "A2C",
      "description": "A2C configuration 989 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 200.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 4096,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0990",
      "algorithm": "TD3",
      "description": "TD3 configuration 990 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.005,
        "activity_reward_weight": 1.0
      },
      "training": {
        "total_timesteps": 200000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 100000,
        "batch_size": 64,
        "tau": 0.005,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.3,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0991",
      "algorithm": "A2C",
      "description": "A2C configuration 991 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 1e-05,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.98,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0992",
      "algorithm": "A2C",
      "description": "A2C configuration 992 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.5,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.01,
        "vf_coef": 0.75,
        "max_grad_norm": 0.1
      }
    },
    {
      "config_id": "a2c_0993",
      "algorithm": "A2C",
      "description": "A2C configuration 993 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 50.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 1.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.001,
        "n_steps": 1024,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.01,
        "vf_coef": 0.75,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0994",
      "algorithm": "TD3",
      "description": "TD3 configuration 994 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.1,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 9
      },
      "risk_management": {
        "max_position_change": 0.15,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 0.0001,
        "buffer_size": 500000,
        "batch_size": 128,
        "tau": 0.001,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.2,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_0995",
      "algorithm": "A2C",
      "description": "A2C configuration 995 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 120.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 2.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.02,
        "activity_reward_weight": 0.0
      },
      "training": {
        "total_timesteps": 300000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.15,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 512,
        "gamma": 0.99,
        "gae_lambda": 0.98,
        "ent_coef": 0.001,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      }
    },
    {
      "config_id": "td3_0996",
      "algorithm": "TD3",
      "description": "TD3 configuration 996 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.0,
        "transaction_penalty_multiplier": 1.0,
        "drawdown_threshold": 0.1,
        "drawdown_penalty": 10.0,
        "holding_penalty": 0.0,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 5e-05,
        "buffer_size": 1000000,
        "batch_size": 64,
        "tau": 0.001,
        "gamma": 0.95,
        "noise_std": 0.1,
        "target_noise": 0.2,
        "policy_delay": 1
      }
    },
    {
      "config_id": "a2c_0997",
      "algorithm": "A2C",
      "description": "A2C configuration 997 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.15,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.5
      },
      "training": {
        "total_timesteps": 50000,
        "episode_length": 40320,
        "rolling_window_months": 6
      },
      "risk_management": {
        "max_position_change": 0.2,
        "max_drawdown_limit": 0.2,
        "position_size_limit": 0.8
      },
      "model_params": {
        "learning_rate": 0.0005,
        "n_steps": 4096,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "ent_coef": 0.001,
        "vf_coef": 0.5,
        "max_grad_norm": 1.0
      }
    },
    {
      "config_id": "td3_0998",
      "algorithm": "TD3",
      "description": "TD3 configuration 998 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 100.0,
        "sharpe_reward_weight": 0.2,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.2,
        "drawdown_penalty": 100.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 150000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 1e-05,
        "buffer_size": 500000,
        "batch_size": 512,
        "tau": 0.001,
        "gamma": 0.99,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 1
      }
    },
    {
      "config_id": "td3_0999",
      "algorithm": "TD3",
      "description": "TD3 configuration 999 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 80.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 0.5,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 50.0,
        "holding_penalty": 0.001,
        "activity_reward_weight": 0.2
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.1,
        "max_drawdown_limit": 0.25,
        "position_size_limit": 1.2
      },
      "model_params": {
        "learning_rate": 0.0005,
        "buffer_size": 1000000,
        "batch_size": 256,
        "tau": 0.005,
        "gamma": 0.995,
        "noise_std": 0.05,
        "target_noise": 0.1,
        "policy_delay": 3
      }
    },
    {
      "config_id": "a2c_1000",
      "algorithm": "A2C",
      "description": "A2C configuration 1000 for reward function optimization",
      "reward_components": {
        "pnl_reward_scale": 150.0,
        "sharpe_reward_weight": 0.3,
        "transaction_penalty_multiplier": 3.0,
        "drawdown_threshold": 0.05,
        "drawdown_penalty": 200.0,
        "holding_penalty": 0.01,
        "activity_reward_weight": 0.1
      },
      "training": {
        "total_timesteps": 100000,
        "episode_length": 40320,
        "rolling_window_months": 12
      },
      "risk_management": {
        "max_position_change": 0.05,
        "max_drawdown_limit": 0.3,
        "position_size_limit": 1.0
      },
      "model_params": {
        "learning_rate": 5e-05,
        "n_steps": 1024,
        "gamma": 0.995,
        "gae_lambda": 0.95,
        "ent_coef": 0.01,
        "vf_coef": 0.25,
        "max_grad_norm": 1.0
      }
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-27T05:32:03.901331Z",
     "start_time": "2025-09-27T05:31:14.583776Z"
    }
   },
   "source": [
    "# cell 1=== Grab yesterday's Ethereum-trading Reddit posts (UTC) -> CSV ===\n",
    "# Dependencies: praw, pandas, python-dotenv (will be installed automatically)\n",
    "import os, sys, time, re, datetime as dt\n",
    "from datetime import timezone\n",
    "from typing import List\n",
    "\n",
    "def _pip_install(pkgs: List[str]):\n",
    "    import importlib, subprocess\n",
    "    for p in pkgs:\n",
    "        try:\n",
    "            importlib.import_module(p.split(\"==\")[0])\n",
    "        except Exception:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", p])\n",
    "\n",
    "_pip_install([\"praw\", \"pandas\", \"python-dotenv\"])\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Load .env from the project root; if your notebook is in a subfolder, os.chdir to root first\n",
    "import praw\n",
    "import pandas as pd\n",
    "\n",
    "# Read credentials (from .env)\n",
    "CLIENT_ID     = os.getenv(\"REDDIT_CLIENT_ID\")\n",
    "CLIENT_SECRET = os.getenv(\"REDDIT_CLIENT_SECRET\")\n",
    "USER_AGENT    = os.getenv(\"REDDIT_USER_AGENT\", \"eth-sentiment-bot/0.1\")\n",
    "\n",
    "if not CLIENT_ID or not CLIENT_SECRET or not USER_AGENT:\n",
    "    raise RuntimeError(\"Missing Reddit credentials: set REDDIT_CLIENT_ID / REDDIT_CLIENT_SECRET / REDDIT_USER_AGENT in .env\")\n",
    "\n",
    "# Subreddits & keywords (adjust as needed)\n",
    "SUBREDDITS = [\"ethereum\", \"ethfinance\", \"ethtrader\", \"CryptoCurrency\", \"defi\", \"ethdev\"]\n",
    "KEYWORDS = [\n",
    "    r\"\\beth\\b\", r\"\\bethereum\\b\", r\"\\beth/usdt\\b\", r\"\\bethusd\\b\", r\"\\bethusdt\\b\",\n",
    "    r\"\\bspot\\b\", r\"\\bfutures?\\b\", r\"\\bperps?\\b\", r\"\\btrade|trading|trader\\b\",\n",
    "    r\"\\bposition\\b\", r\"\\blong\\b\", r\"\\bshort\\b\", r\"\\bentry\\b\", r\"\\bexit\\b\",\n",
    "    r\"\\bmarket\\b\", r\"\\border\\b\", r\"\\bliquidation\\b\", r\"\\bhedge\\b\", r\"\\bleverage\\b\"\n",
    "]\n",
    "KW_REGEX = re.compile(\"|\".join(KEYWORDS), flags=re.IGNORECASE)\n",
    "\n",
    "MAX_PER_SUBREDDIT = 5000\n",
    "REQUEST_SLEEP = 0.5\n",
    "SAVE_DIR = \"./data/reddit/yesterday\"\n",
    "\n",
    "# Yesterday (UTC) time window\n",
    "now_utc = dt.datetime.now(timezone.utc)\n",
    "day_end = dt.datetime(year=now_utc.year, month=now_utc.month, day=now_utc.day, tzinfo=timezone.utc)\n",
    "day_start = day_end - dt.timedelta(days=1)\n",
    "D = day_start.date()\n",
    "print(f\"[UTC window] {day_start.isoformat()} -> {day_end.isoformat()} (D={D})\")\n",
    "\n",
    "# Reddit client (read-only)\n",
    "reddit = praw.Reddit(\n",
    "    client_id=CLIENT_ID,\n",
    "    client_secret=CLIENT_SECRET,\n",
    "    user_agent=USER_AGENT,\n",
    "    ratelimit_seconds=5,\n",
    ")\n",
    "reddit.read_only = True\n",
    "\n",
    "def _to_utc(ts: float) -> dt.datetime:\n",
    "    return dt.datetime.fromtimestamp(ts, tz=timezone.utc)\n",
    "\n",
    "def _match_eth_trading(title: str, selftext: str) -> bool:\n",
    "    return bool(KW_REGEX.search(f\"{title or ''}\\n{selftext or ''}\"))\n",
    "\n",
    "# Collect\n",
    "rows = []\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "for sub in SUBREDDITS:\n",
    "    print(f\"\\n[collect] r/{sub} ...\")\n",
    "    count = 0\n",
    "    try:\n",
    "        for submission in reddit.subreddit(sub).new(limit=None):\n",
    "            created = _to_utc(getattr(submission, \"created_utc\", 0.0))\n",
    "            if created >= day_end:\n",
    "                continue\n",
    "            if created < day_start:\n",
    "                break\n",
    "\n",
    "            if not _match_eth_trading(submission.title, submission.selftext):\n",
    "                continue\n",
    "\n",
    "            rows.append(dict(\n",
    "                id=submission.id,\n",
    "                subreddit=submission.subreddit.display_name,\n",
    "                created_utc=created.isoformat(),\n",
    "                title=submission.title or \"\",\n",
    "                selftext=submission.selftext or \"\",\n",
    "                url=submission.url or \"\",\n",
    "                is_self=bool(submission.is_self),\n",
    "                author=str(submission.author) if submission.author else None,\n",
    "                score=int(submission.score or 0),\n",
    "                upvote_ratio=float(submission.upvote_ratio or 0.0),\n",
    "                num_comments=int(submission.num_comments or 0),\n",
    "                over_18=bool(getattr(submission, \"over_18\", False)),\n",
    "                stickied=bool(getattr(submission, \"stickied\", False)),\n",
    "                crosspost_parent=getattr(submission, \"crosspost_parent\", None),\n",
    "                permalink=f\"https://www.reddit.com{submission.permalink}\" if getattr(submission, \"permalink\", None) else \"\",\n",
    "                fetched_at=dt.datetime.now(timezone.utc).isoformat(),\n",
    "                fetch_window_start=day_start.isoformat(),\n",
    "                fetch_window_end=day_end.isoformat(),\n",
    "                fetch_version=\"submissions_only_v1\"\n",
    "            ))\n",
    "            count += 1\n",
    "            if count % 200 == 0:\n",
    "                print(f\"  r/{sub}: matched {count} ...\")\n",
    "            if count >= MAX_PER_SUBREDDIT:\n",
    "                print(f\"  r/{sub}: hit MAX_PER_SUBREDDIT={MAX_PER_SUBREDDIT}, stop.\")\n",
    "                break\n",
    "            time.sleep(REQUEST_SLEEP)\n",
    "    except Exception as e:\n",
    "        print(f\"  [warn] subreddit {sub} failed: {e}\")\n",
    "\n",
    "# Deduplicate and save CSV\n",
    "df = pd.DataFrame(rows)\n",
    "csv_path = os.path.join(SAVE_DIR, f\"reddit_eth_submissions_{D}.csv\")\n",
    "\n",
    "if df.empty:\n",
    "    print(\"\\n[summary] No matched submissions found. You can tweak keywords/subreddits and try again.\")\n",
    "else:\n",
    "    df.sort_values([\"id\", \"score\", \"fetched_at\"], ascending=[True, False, False], inplace=True)\n",
    "    df = df.drop_duplicates(subset=[\"id\"], keep=\"first\").reset_index(drop=True)\n",
    "    df[\"is_crosspost\"] = df[\"crosspost_parent\"].notna()\n",
    "    df.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    dt_min = pd.to_datetime(df[\"created_utc\"]).min()\n",
    "    dt_max = pd.to_datetime(df[\"created_utc\"]).max()\n",
    "    print(\"\\n[summary]\")\n",
    "    print(f\"  window (UTC): {day_start.isoformat()} → {day_end.isoformat()}\")\n",
    "    print(f\"  collected rows: {len(df)}  (after de-dup)\")\n",
    "    print(f\"  time coverage:  {dt_min} → {dt_max}\")\n",
    "    print(f\"  subreddits:     {', '.join(sorted(df['subreddit'].unique()))}\")\n",
    "    print(f\"  saved csv:      {csv_path}\")\n",
    "\n",
    "    # Preview top 10\n",
    "    preview = (\n",
    "        df[[\"created_utc\",\"subreddit\",\"score\",\"num_comments\",\"title\"]]\n",
    "        .sort_values([\"score\",\"num_comments\"], ascending=False)\n",
    "        .head(10)\n",
    "    )\n",
    "    print(\"\\n[top 10 by score/num_comments]\")\n",
    "    for _, r in preview.iterrows():\n",
    "        print(f\"- [{r['created_utc']}] r/{r['subreddit']} | score={r['score']} com={r['num_comments']} | {r['title'][:140]}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UTC window] 2025-09-26T00:00:00+00:00 -> 2025-09-27T00:00:00+00:00 (D=2025-09-26)\n",
      "\n",
      "[collect] r/ethereum ...\n",
      "\n",
      "[collect] r/ethfinance ...\n",
      "\n",
      "[collect] r/ethtrader ...\n",
      "\n",
      "[collect] r/CryptoCurrency ...\n",
      "\n",
      "[collect] r/defi ...\n",
      "\n",
      "[collect] r/ethdev ...\n",
      "\n",
      "[summary]\n",
      "  window (UTC): 2025-09-26T00:00:00+00:00 → 2025-09-27T00:00:00+00:00\n",
      "  collected rows: 32  (after de-dup)\n",
      "  time coverage:  2025-09-26 00:00:56+00:00 → 2025-09-26 23:02:28+00:00\n",
      "  subreddits:     CryptoCurrency, defi, ethdev, ethereum, ethfinance, ethtrader\n",
      "  saved csv:      ./data/reddit/yesterday\\reddit_eth_submissions_2025-09-26.csv\n",
      "\n",
      "[top 10 by score/num_comments]\n",
      "- [2025-09-26T05:01:29+00:00] r/ethereum | score=133 com=162 | Daily General Discussion September 26, 2025\n",
      "- [2025-09-26T10:16:53+00:00] r/ethtrader | score=125 com=47 | Whales accumulate while retail panics, same old story for ETH.\n",
      "- [2025-09-26T18:44:29+00:00] r/ethtrader | score=107 com=14 | ethereum co founder sells $6M but whales scoop up $1.6B whats really happening here?\n",
      "- [2025-09-26T11:36:09+00:00] r/CryptoCurrency | score=93 com=119 | Are you prepared to hold 24 months on your crypto portfolio?\n",
      "- [2025-09-26T06:37:06+00:00] r/CryptoCurrency | score=92 com=216 | When did cryptocurrency stop being a currency and became a virtual stock?\n",
      "- [2025-09-26T23:02:28+00:00] r/CryptoCurrency | score=91 com=35 | Buying altcoins in 2025 is like buying a lottery ticket at the gas station that sold the winning ticket\n",
      "- [2025-09-26T21:59:43+00:00] r/ethtrader | score=66 com=13 | The Race To Rewire Wall Street: Is Ethereum The Safest Bet?\n",
      "- [2025-09-26T10:54:59+00:00] r/CryptoCurrency | score=53 com=7 | Bitcoin sinks below $109k wiping $170 billion from crypto market after FOMC shock\n",
      "- [2025-09-26T03:40:26+00:00] r/ethtrader | score=38 com=22 | REX-Osprey launch ETH staking ETF\n",
      "- [2025-09-26T01:03:38+00:00] r/CryptoCurrency | score=37 com=23 | BTC Perpetual Trading Goes Live on Cardano\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T05:32:29.429732Z",
     "start_time": "2025-09-27T05:32:29.383732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# cell 2=== Normalize yesterday's grabbed CSV to your legacy KEEP_FIELDS schema ===\n",
    "# Input:  data/reddit/yesterday/reddit_eth_submissions_YYYY-MM-DD.csv\n",
    "# Output: data/reddit/processed/reddit_eth_standard_YYYY-MM-DD.csv\n",
    "# Purpose: Map/fill the newly grabbed columns into the legacy 17-field schema so downstream scoring/aggregation code can be reused\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "# Your legacy KEEP_FIELDS (from sentiment.ipynb)\n",
    "KEEP_FIELDS = [\n",
    "    \"id\", \"author\", \"subreddit\",\n",
    "    \"created_utc\", \"created\", \"created_time_utc\",\n",
    "    \"title\", \"selftext\", \"body\",\n",
    "    \"url\", \"permalink\",\n",
    "    \"score\", \"upvote_ratio\", \"num_comments\", \"num_crossposts\",\n",
    "    \"over_18\", \"is_self\"\n",
    "]\n",
    "\n",
    "# Yesterday's UTC filename (aligned with the grabbing script)\n",
    "D = (dt.datetime.now(dt.timezone.utc) - dt.timedelta(days=1)).date()\n",
    "in_dir  = os.path.join(\"data\", \"reddit\", \"yesterday\")\n",
    "in_csv  = os.path.join(in_dir, f\"reddit_eth_submissions_{D}.csv\")\n",
    "\n",
    "out_dir = os.path.join(\"data\", \"reddit\", \"processed\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "out_csv = os.path.join(out_dir, f\"reddit_eth_standard_{D}.csv\")\n",
    "\n",
    "if not os.path.exists(in_csv):\n",
    "    raise FileNotFoundError(f\"找不到输入文件: {os.path.abspath(in_csv)}\")\n",
    "\n",
    "df = pd.read_csv(in_csv)\n",
    "\n",
    "# ---- Field mapping/filling (minimal changes, reuse existing columns whenever possible) ----\n",
    "out = pd.DataFrame()\n",
    "\n",
    "# Directly mapped columns (if missing, use defaults)\n",
    "def pick(col, default=None):\n",
    "    return df[col] if col in df.columns else default\n",
    "\n",
    "out[\"id\"]         = pick(\"id\")\n",
    "out[\"author\"]     = pick(\"author\")\n",
    "out[\"subreddit\"]  = pick(\"subreddit\")\n",
    "out[\"created_utc\"]= pick(\"created_utc\")  # grabbed as ISO string\n",
    "\n",
    "# Because the legacy flow had both created_utc and created/created_time_utc:\n",
    "# For compatibility: created = naive local display; created_time_utc = same as created_utc\n",
    "ts = pd.to_datetime(out[\"created_utc\"], errors=\"coerce\", utc=True)\n",
    "out[\"created\"]          = ts.dt.tz_convert(None).dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "out[\"created_time_utc\"] = out[\"created_utc\"]\n",
    "\n",
    "out[\"title\"]    = pick(\"title\", \"\")\n",
    "out[\"selftext\"] = pick(\"selftext\", \"\")\n",
    "out[\"body\"]     = \"\"  # legacy schema for comments; we grabbed submissions, so leave blank\n",
    "\n",
    "out[\"url\"]       = pick(\"url\", \"\")\n",
    "out[\"permalink\"] = pick(\"permalink\", \"\")\n",
    "\n",
    "out[\"score\"]        = pick(\"score\", 0).fillna(0).astype(\"Int64\")\n",
    "out[\"upvote_ratio\"] = pick(\"upvote_ratio\", 0.0).fillna(0.0)\n",
    "out[\"num_comments\"] = pick(\"num_comments\", 0).fillna(0).astype(\"Int64\")\n",
    "\n",
    "# num_crossposts: not explicitly grabbed; approximate via presence of crosspost_parent\n",
    "if \"num_crossposts\" in df.columns:\n",
    "    out[\"num_crossposts\"] = df[\"num_crossposts\"].fillna(0).astype(\"Int64\")\n",
    "else:\n",
    "    out[\"num_crossposts\"] = df.get(\"crosspost_parent\").notna().astype(int)\n",
    "\n",
    "out[\"over_18\"] = pick(\"over_18\", False).fillna(False).astype(bool)\n",
    "out[\"is_self\"] = pick(\"is_self\", False).fillna(False).astype(bool)\n",
    "\n",
    "# Keep only the legacy KEEP_FIELDS order and names\n",
    "out = out[KEEP_FIELDS]\n",
    "\n",
    "# De-duplicate (by id)\n",
    "out = out.sort_values([\"id\", \"score\"], ascending=[True, False]).drop_duplicates(\"id\").reset_index(drop=True)\n",
    "\n",
    "# Save CSV\n",
    "out.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Summary\n",
    "print(\"[normalize] input file :\", os.path.abspath(in_csv))\n",
    "print(\"[normalize] output file:\", os.path.abspath(out_csv))\n",
    "print(\"[normalize] rows       :\", len(out))\n",
    "print(\"[normalize] columns    :\", list(out.columns))\n",
    "print(out[[\"created_utc\",\"subreddit\",\"score\",\"num_comments\",\"title\"]].head(5))\n"
   ],
   "id": "e8f0d7b507e6ed0b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[normalize] input file : C:\\Users\\Jimmy\\Desktop\\760\\data\\reddit\\yesterday\\reddit_eth_submissions_2025-09-26.csv\n",
      "[normalize] output file: C:\\Users\\Jimmy\\Desktop\\760\\data\\reddit\\processed\\reddit_eth_standard_2025-09-26.csv\n",
      "[normalize] rows       : 32\n",
      "[normalize] columns    : ['id', 'author', 'subreddit', 'created_utc', 'created', 'created_time_utc', 'title', 'selftext', 'body', 'url', 'permalink', 'score', 'upvote_ratio', 'num_comments', 'num_crossposts', 'over_18', 'is_self']\n",
      "                 created_utc       subreddit  score  num_comments  \\\n",
      "0  2025-09-26T00:00:56+00:00       ethtrader      9            53   \n",
      "1  2025-09-26T00:01:00+00:00  CryptoCurrency     21           495   \n",
      "2  2025-09-26T01:03:38+00:00  CryptoCurrency     37            23   \n",
      "3  2025-09-26T01:06:58+00:00          ethdev      5             2   \n",
      "4  2025-09-26T02:13:10+00:00  CryptoCurrency      0            14   \n",
      "\n",
      "                                               title  \n",
      "0  Daily General Discussion - September 26, 2025 ...  \n",
      "1  Daily Crypto Discussion - September 26, 2025 (...  \n",
      "2         BTC Perpetual Trading Goes Live on Cardano  \n",
      "3  Professional, non-custodial, stablecoin invoic...  \n",
      "4      What’s your view in ASTER? Are you buying it?  \n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T05:32:33.558014Z",
     "start_time": "2025-09-27T05:32:33.481177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Cell 3: Clean processed file -> cleaned/reddit_eth_standard_{D}_clean.csv ===\n",
    "import os, re, glob\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from datetime import timezone\n",
    "\n",
    "# ---- Auto-locate input file ----\n",
    "D = (dt.datetime.now(timezone.utc) - dt.timedelta(days=1)).date()\n",
    "default_path = f\"data/reddit/processed/reddit_eth_standard_{D}.csv\"\n",
    "if os.path.exists(default_path):\n",
    "    INPUT_CSV = default_path\n",
    "else:\n",
    "    # Fallback: find the latest reddit_eth_standard_*.csv under processed directory\n",
    "    cand = sorted(glob.glob(\"data/reddit/processed/reddit_eth_standard_*.csv\"))\n",
    "    if not cand:\n",
    "        raise FileNotFoundError(\"找不到 processed 文件：data/reddit/processed/reddit_eth_standard_*.csv\")\n",
    "    INPUT_CSV = cand[-1]\n",
    "    # Sync D (extract date from filename)\n",
    "    try:\n",
    "        D = os.path.basename(INPUT_CSV).split(\"_\")[-1].split(\".\")[0]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "OUT_DIR   = \"cleaned\"\n",
    "MAX_LEN   = 20000\n",
    "MIN_LEN   = 5\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    s = str(text)\n",
    "    s = re.sub(r\"\\[([^\\]]+)\\]\\(([^)]+)\\)\", r\"\\1\", s)               # markdown link -> plain text\n",
    "    s = re.sub(r\"http\\S+|www\\.\\S+\", \" \", s, flags=re.IGNORECASE)   # URLs\n",
    "    s = re.sub(r\"<[^>]+>\", \" \", s)                                 # HTML tags\n",
    "    s = re.sub(r\"[^A-Za-z0-9\\s]\", \" \", s)                          # keep only letters, numbers, spaces\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip().lower()\n",
    "    return s\n",
    "\n",
    "# Read\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_CSV, low_memory=False)\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(INPUT_CSV, encoding=\"latin-1\", low_memory=False)\n",
    "\n",
    "# Ensure text columns\n",
    "if \"title\" not in df.columns:\n",
    "    raise ValueError(\"CSV 缺少 'title' 列\")\n",
    "if \"selftext\" not in df.columns: df[\"selftext\"] = \"\"\n",
    "if \"body\" not in df.columns: df[\"body\"] = \"\"\n",
    "\n",
    "# Drop extraneous columns\n",
    "df = df.drop(columns=[c for c in df.columns if str(c).startswith(\"Unnamed\")], errors=\"ignore\")\n",
    "df = df.dropna(subset=[\"title\"])\n",
    "\n",
    "# Concatenate raw text\n",
    "df[\"text_raw\"] = (df[\"title\"].astype(str).fillna(\"\") + \" \" + df[\"selftext\"].astype(str).fillna(\"\"))\n",
    "mask_short = df[\"text_raw\"].str.len().fillna(0) < 3\n",
    "df.loc[mask_short, \"text_raw\"] = df.loc[mask_short, \"text_raw\"] + \" \" + df.loc[mask_short, \"body\"].astype(str)\n",
    "\n",
    "# Truncate & clean\n",
    "df[\"text_raw\"] = df[\"text_raw\"].astype(str).str.slice(0, MAX_LEN)\n",
    "df[\"text_clean\"] = df[\"text_raw\"].map(clean_text)\n",
    "df = df[df[\"text_clean\"].str.len() >= MIN_LEN].copy()\n",
    "\n",
    "# Parse time (supports ISO or numeric timestamp)\n",
    "def parse_created_any(s):\n",
    "    ts = pd.to_datetime(s, errors=\"coerce\", utc=True)  # try ISO first\n",
    "    if ts.isna().mean() > 0.5:  # if mostly NaT, try numeric\n",
    "        c = pd.to_numeric(s, errors=\"coerce\")\n",
    "        if c.notna().any():\n",
    "            unit = \"ms\" if (c.dropna().median() > 10**12) else \"s\"\n",
    "            ts = pd.to_datetime(c, unit=unit, errors=\"coerce\", utc=True)\n",
    "    return ts\n",
    "\n",
    "if \"created_time_utc\" not in df.columns:\n",
    "    if \"created_utc\" in df.columns:\n",
    "        df[\"created_time_utc\"] = parse_created_any(df[\"created_utc\"])\n",
    "    elif \"created\" in df.columns:\n",
    "        df[\"created_time_utc\"] = parse_created_any(df[\"created\"])\n",
    "    else:\n",
    "        df[\"created_time_utc\"] = pd.NaT\n",
    "\n",
    "# year_month\n",
    "if df[\"created_time_utc\"].notna().any():\n",
    "    df[\"year_month\"] = pd.to_datetime(df[\"created_time_utc\"]).dt.strftime(\"%Y-%m\")\n",
    "elif \"source_file\" in df.columns:\n",
    "    df[\"year_month\"] = df[\"source_file\"].str.extract(r'((20\\d{2})[-_](\\d{2}))')[0]\n",
    "else:\n",
    "    df[\"year_month\"] = None\n",
    "\n",
    "# De-duplicate\n",
    "before = len(df)\n",
    "if \"id\" in df.columns:\n",
    "    df = df.sort_values([\"id\",\"score\"] if \"score\" in df.columns else [\"id\"]) \\\n",
    "           .drop_duplicates(\"id\")\n",
    "else:\n",
    "    keys = [k for k in [\"title\",\"created_time_utc\",\"subreddit\"] if k in df.columns]\n",
    "    df = df.drop_duplicates(subset=keys) if keys else df.drop_duplicates()\n",
    "after = len(df)\n",
    "\n",
    "# Save\n",
    "base = os.path.splitext(os.path.basename(INPUT_CSV))[0]\n",
    "out_path = os.path.join(OUT_DIR, f\"{base}_clean.csv\")\n",
    "df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"✅ Cleaning completed\")\n",
    "print(\" - Input :\", os.path.abspath(INPUT_CSV))\n",
    "print(\" - Output:\", os.path.abspath(out_path))\n",
    "print(f\" - Records: {after} (before de-dup {before})\")\n",
    "print(\" - Key columns present:\", [c for c in [\"text_raw\",\"text_clean\",\"created_time_utc\",\"year_month\"] if c in df.columns])\n",
    "df.head(3)\n"
   ],
   "id": "bc8ed06da8739185",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 清洗完成\n",
      " - 输入: C:\\Users\\Jimmy\\Desktop\\760\\data\\reddit\\processed\\reddit_eth_standard_2025-09-26.csv\n",
      " - 输出: C:\\Users\\Jimmy\\Desktop\\760\\cleaned\\reddit_eth_standard_2025-09-26_clean.csv\n",
      " - 记录数: 32（去重前 32）\n",
      " - 关键列存在： ['text_raw', 'text_clean', 'created_time_utc', 'year_month']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "        id         author       subreddit                created_utc  \\\n",
       "0  1nqmorm  AutoModerator       ethtrader  2025-09-26T00:00:56+00:00   \n",
       "1  1nqmotq  AutoModerator  CryptoCurrency  2025-09-26T00:01:00+00:00   \n",
       "2  1nqo0l7       LazyJury  CryptoCurrency  2025-09-26T01:03:38+00:00   \n",
       "\n",
       "               created           created_time_utc  \\\n",
       "0  2025-09-26 00:00:56  2025-09-26T00:00:56+00:00   \n",
       "1  2025-09-26 00:01:00  2025-09-26T00:01:00+00:00   \n",
       "2  2025-09-26 01:03:38  2025-09-26T01:03:38+00:00   \n",
       "\n",
       "                                               title  \\\n",
       "0  Daily General Discussion - September 26, 2025 ...   \n",
       "1  Daily Crypto Discussion - September 26, 2025 (...   \n",
       "2         BTC Perpetual Trading Goes Live on Cardano   \n",
       "\n",
       "                                            selftext  body  \\\n",
       "0  Welcome to the Daily General Discussion thread...   NaN   \n",
       "1  **Welcome to the Daily Crypto Discussion threa...   NaN   \n",
       "2  Strike Finance has officially gone live with B...   NaN   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.reddit.com/r/ethtrader/comments/1n...   \n",
       "1  https://www.reddit.com/r/CryptoCurrency/commen...   \n",
       "2       https://app.strikefinance.org/perpetuals/btc   \n",
       "\n",
       "                                           permalink  score  upvote_ratio  \\\n",
       "0  https://www.reddit.com/r/ethtrader/comments/1n...      9          1.00   \n",
       "1  https://www.reddit.com/r/CryptoCurrency/commen...     21          0.89   \n",
       "2  https://www.reddit.com/r/CryptoCurrency/commen...     37          0.81   \n",
       "\n",
       "   num_comments  num_crossposts  over_18  is_self  \\\n",
       "0            53               0    False     True   \n",
       "1           495               0    False     True   \n",
       "2            23               0    False    False   \n",
       "\n",
       "                                            text_raw  \\\n",
       "0  Daily General Discussion - September 26, 2025 ...   \n",
       "1  Daily Crypto Discussion - September 26, 2025 (...   \n",
       "2  BTC Perpetual Trading Goes Live on Cardano Str...   \n",
       "\n",
       "                                          text_clean year_month  \n",
       "0  daily general discussion september 26 2025 utc...    2025-09  \n",
       "1  daily crypto discussion september 26 2025 gmt ...    2025-09  \n",
       "2  btc perpetual trading goes live on cardano str...    2025-09  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>created</th>\n",
       "      <th>created_time_utc</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>body</th>\n",
       "      <th>url</th>\n",
       "      <th>permalink</th>\n",
       "      <th>score</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>num_crossposts</th>\n",
       "      <th>over_18</th>\n",
       "      <th>is_self</th>\n",
       "      <th>text_raw</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>year_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1nqmorm</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>ethtrader</td>\n",
       "      <td>2025-09-26T00:00:56+00:00</td>\n",
       "      <td>2025-09-26 00:00:56</td>\n",
       "      <td>2025-09-26T00:00:56+00:00</td>\n",
       "      <td>Daily General Discussion - September 26, 2025 ...</td>\n",
       "      <td>Welcome to the Daily General Discussion thread...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reddit.com/r/ethtrader/comments/1n...</td>\n",
       "      <td>https://www.reddit.com/r/ethtrader/comments/1n...</td>\n",
       "      <td>9</td>\n",
       "      <td>1.00</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Daily General Discussion - September 26, 2025 ...</td>\n",
       "      <td>daily general discussion september 26 2025 utc...</td>\n",
       "      <td>2025-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1nqmotq</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>CryptoCurrency</td>\n",
       "      <td>2025-09-26T00:01:00+00:00</td>\n",
       "      <td>2025-09-26 00:01:00</td>\n",
       "      <td>2025-09-26T00:01:00+00:00</td>\n",
       "      <td>Daily Crypto Discussion - September 26, 2025 (...</td>\n",
       "      <td>**Welcome to the Daily Crypto Discussion threa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reddit.com/r/CryptoCurrency/commen...</td>\n",
       "      <td>https://www.reddit.com/r/CryptoCurrency/commen...</td>\n",
       "      <td>21</td>\n",
       "      <td>0.89</td>\n",
       "      <td>495</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Daily Crypto Discussion - September 26, 2025 (...</td>\n",
       "      <td>daily crypto discussion september 26 2025 gmt ...</td>\n",
       "      <td>2025-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1nqo0l7</td>\n",
       "      <td>LazyJury</td>\n",
       "      <td>CryptoCurrency</td>\n",
       "      <td>2025-09-26T01:03:38+00:00</td>\n",
       "      <td>2025-09-26 01:03:38</td>\n",
       "      <td>2025-09-26T01:03:38+00:00</td>\n",
       "      <td>BTC Perpetual Trading Goes Live on Cardano</td>\n",
       "      <td>Strike Finance has officially gone live with B...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://app.strikefinance.org/perpetuals/btc</td>\n",
       "      <td>https://www.reddit.com/r/CryptoCurrency/commen...</td>\n",
       "      <td>37</td>\n",
       "      <td>0.81</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>BTC Perpetual Trading Goes Live on Cardano Str...</td>\n",
       "      <td>btc perpetual trading goes live on cardano str...</td>\n",
       "      <td>2025-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T05:34:16.053033Z",
     "start_time": "2025-09-27T05:34:15.106746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Cell 4: Score the cleaned file with VADER and write to master posts_scores_{D}.csv ===\n",
    "import os, pandas as pd, datetime as dt\n",
    "from datetime import timezone\n",
    "\n",
    "# Input (output from previous cell)\n",
    "D = (dt.datetime.now(timezone.utc) - dt.timedelta(days=1)).date()\n",
    "clean_in = f\"cleaned/reddit_eth_standard_{D}_clean.csv\"\n",
    "if not os.path.exists(clean_in):\n",
    "    # If you used the \"latest file fallback\", the name may not match yesterday's date; find latest *_clean.csv\n",
    "    import glob\n",
    "    cand = sorted(glob.glob(\"cleaned/*_clean.csv\"))\n",
    "    if not cand:\n",
    "        raise FileNotFoundError(\"No cleaned file found under cleaned/*.csv\")\n",
    "    clean_in = cand[-1]\n",
    "\n",
    "# Read\n",
    "df = pd.read_csv(clean_in)\n",
    "\n",
    "# Select base columns for the master table\n",
    "base_cols = [c for c in [\n",
    "    \"id\",\"subreddit\",\"created_time_utc\",\"title\",\"selftext\",\"body\",\n",
    "    \"text_clean\",\"score\",\"num_comments\",\"upvote_ratio\",\"permalink\",\"url\"\n",
    "] if c in df.columns]\n",
    "dfb = df[base_cols].copy()\n",
    "\n",
    "# VADER\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "try:\n",
    "    _ = nltk.data.find(\"sentiment/vader_lexicon.zip\")\n",
    "except LookupError:\n",
    "    nltk.download(\"vader_lexicon\")\n",
    "\n",
    "text_col = \"text_clean\" if \"text_clean\" in dfb.columns else None\n",
    "if text_col is None:\n",
    "    def _join(*xs): return \" \".join([str(x) for x in xs if pd.notna(x)])\n",
    "    dfb[\"__text_tmp__\"] = [_join(t, s, b) for t, s, b in zip(\n",
    "        dfb.get(\"title\",\"\"), dfb.get(\"selftext\",\"\"), dfb.get(\"body\",\"\")\n",
    "    )]\n",
    "    text_col = \"__text_tmp__\"\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "dfb[\"vader\"] = dfb[text_col].map(lambda t: sia.polarity_scores(str(t))[\"compound\"])\n",
    "\n",
    "# Save master table (you can append s1…s5 columns later)\n",
    "os.makedirs(\"data/reddit/scored\", exist_ok=True)\n",
    "master_p = f\"data/reddit/scored/posts_scores_{D}.csv\"\n",
    "tmp_p = master_p + \".tmp\"\n",
    "dfb.to_csv(tmp_p, index=False, encoding=\"utf-8-sig\")\n",
    "os.replace(tmp_p, master_p)\n",
    "\n",
    "print(\"✅ VADER scoring completed & master file saved:\", os.path.abspath(master_p))\n",
    "print(\" - Columns:\", list(dfb.columns))\n",
    "print(\" - Rows   :\", len(dfb))\n",
    "dfb[[\"id\",\"vader\",\"title\"]].head(3)\n"
   ],
   "id": "b5699c9b1712f6d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ VADER scoring completed & master file saved: C:\\Users\\Jimmy\\Desktop\\760\\data\\reddit\\scored\\posts_scores_2025-09-26.csv\n",
      " - Columns: ['id', 'subreddit', 'created_time_utc', 'title', 'selftext', 'body', 'text_clean', 'score', 'num_comments', 'upvote_ratio', 'permalink', 'url', 'vader']\n",
      " - Rows   : 32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "        id   vader                                              title\n",
       "0  1nqmorm  0.9509  Daily General Discussion - September 26, 2025 ...\n",
       "1  1nqmotq  0.9801  Daily Crypto Discussion - September 26, 2025 (...\n",
       "2  1nqo0l7  0.2500         BTC Perpetual Trading Goes Live on Cardano"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>vader</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1nqmorm</td>\n",
       "      <td>0.9509</td>\n",
       "      <td>Daily General Discussion - September 26, 2025 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1nqmotq</td>\n",
       "      <td>0.9801</td>\n",
       "      <td>Daily Crypto Discussion - September 26, 2025 (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1nqo0l7</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>BTC Perpetual Trading Goes Live on Cardano</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T05:36:47.390633Z",
     "start_time": "2025-09-27T05:36:41.268176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# cell 5: Score—— meta-llama-3.1-8b-instruct——s1\n",
    "# =========================\n",
    "# Dependencies: pip install openai pandas numpy tqdm nest_asyncio\n",
    "\n",
    "import os, re, glob, time, asyncio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "from tqdm.auto import tqdm\n",
    "from openai import AsyncOpenAI\n",
    "import datetime as dt\n",
    "from datetime import timezone\n",
    "\n",
    "# -------- LM Studio basic configuration --------\n",
    "LMSTUDIO_BASE_URL = \"http://127.0.0.1:1234/v1\"\n",
    "LMSTUDIO_API_KEY = \"lm-studio\"\n",
    "MODEL_NAME = \"meta-llama-3.1-8b-instruct\"  # API identifier shown in LM Studio (right-side Info)\n",
    "OUT_COL = \"s1\"  # Output column for this model (your first small model)\n",
    "\n",
    "# -------- Master table path (must already contain the 'vader' column) --------\n",
    "D = (dt.datetime.now(timezone.utc) - dt.timedelta(days=1)).date()\n",
    "INPUT_CSV = f\"data/reddit/scored/posts_scores_{D}.csv\"  # Master table (VADER written in previous step)\n",
    "OUTPUT_CSV = INPUT_CSV  # In-place atomic write\n",
    "\n",
    "if not os.path.exists(INPUT_CSV):\n",
    "    # Fallback: pick the newest posts_scores_*.csv under the scored directory\n",
    "    cands = sorted(glob.glob(\"data/reddit/scored/posts_scores_*.csv\"), key=os.path.getmtime)\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(\"Master table posts_scores_{D}.csv not found. Please complete the VADER step first.\")\n",
    "    INPUT_CSV = OUTPUT_CSV = cands[-1]\n",
    "\n",
    "TEXT_COL = \"text_clean\"  # Prefer the cleaned text\n",
    "VADER_COL = \"vader\"  # Can be used to select “hard cases” only (optional)\n",
    "\n",
    "# -------- Speed & control parameters --------\n",
    "MAX_TEXT_LEN = 256  # Truncate to avoid overly long context\n",
    "VADER_EDGE = None  # If set, only score samples with |vader| < threshold; None = score all\n",
    "CONCURRENCY = 8  # Concurrency\n",
    "RETRY = 3\n",
    "\n",
    "# —— Prompt (note: curly braces must be escaped as double braces in Python format strings) ——\n",
    "PROMPT = (\n",
    "    \"Classify the sentiment as exactly one token from {{POS, NEU, NEG}}.\\n\"\n",
    "    \"Text:\\n{text}\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "# Restrict outputs to POS/NEU/NEG (llama.cpp grammar)\n",
    "GRAMMAR = r'root ::= \"POS\" | \"NEU\" | \"NEG\"'\n",
    "\n",
    "\n",
    "def map_label_to_score(label: str) -> float:\n",
    "    s = (label or \"\").strip().upper()\n",
    "    first = re.split(r\"\\s+\", s)[0] if s else \"\"\n",
    "    if first in {\"POS\", \"NEU\", \"NEG\"}:\n",
    "        return 1.0 if first == \"POS\" else (0.0 if first == \"NEU\" else -1.0)\n",
    "    if \"POS\" in s: return 1.0\n",
    "    if \"NEG\" in s: return -1.0\n",
    "    if \"NEU\" in s: return 0.0\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "# -------- Read master table & select samples to score --------\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_CSV, low_memory=False)\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(INPUT_CSV, encoding=\"latin-1\", low_memory=False)\n",
    "\n",
    "# Text column fallback: if no text_clean, join title+selftext+body\n",
    "if TEXT_COL not in df.columns:\n",
    "    def _join(*xs): return \" \".join([str(x) for x in xs if pd.notna(x)])\n",
    "\n",
    "\n",
    "    df[\"__text_tmp__\"] = [_join(t, s, b) for t, s, b in zip(\n",
    "        df.get(\"title\", \"\"), df.get(\"selftext\", \"\"), df.get(\"body\", \"\")\n",
    "    )]\n",
    "    TEXT_COL = \"__text_tmp__\"\n",
    "\n",
    "# Ensure the output column exists (initialize with NaN)\n",
    "if OUT_COL not in df.columns:\n",
    "    df[OUT_COL] = np.nan\n",
    "\n",
    "mask = df[OUT_COL].isna()\n",
    "if VADER_EDGE is not None and VADER_COL in df.columns:\n",
    "    mask &= df[VADER_COL].abs() < VADER_EDGE  # only (re)score ambiguous samples\n",
    "\n",
    "todo = df[mask].copy()\n",
    "if todo.empty:\n",
    "    print(f\"No new samples to score ({OUT_COL} already exists or filtered by threshold). File: {INPUT_CSV}\")\n",
    "else:\n",
    "    # De-duplicate texts to reduce requests\n",
    "    todo[\"__txt\"] = todo[TEXT_COL].astype(str).str.slice(0, MAX_TEXT_LEN)\n",
    "    groups = todo.groupby(\"__txt\").indices\n",
    "    unique_texts = list(groups.keys())\n",
    "    print(f\"Master table: {os.path.basename(INPUT_CSV)}\")\n",
    "    print(f\"Unique texts to score: {len(unique_texts)} (raw samples {len(todo)} / total {len(df)})\")\n",
    "\n",
    "    aclient = AsyncOpenAI(base_url=LMSTUDIO_BASE_URL, api_key=LMSTUDIO_API_KEY)\n",
    "\n",
    "\n",
    "    async def classify_text(t: str) -> float:\n",
    "        msg = PROMPT.format(text=t)\n",
    "        for attempt in range(RETRY):\n",
    "            try:\n",
    "                resp = await aclient.chat.completions.create(\n",
    "                    model=MODEL_NAME,\n",
    "                    messages=[{\"role\": \"user\", \"content\": msg}],\n",
    "                    temperature=0,\n",
    "                    max_tokens=1,\n",
    "                    top_p=1,\n",
    "                    stop=[\"\\n\"],\n",
    "                    extra_body={\"grammar\": GRAMMAR}\n",
    "                )\n",
    "                return map_label_to_score(resp.choices[0].message.content)\n",
    "            except Exception:\n",
    "                if attempt == RETRY - 1:\n",
    "                    return np.nan\n",
    "                await asyncio.sleep(0.6 * (attempt + 1))\n",
    "\n",
    "\n",
    "    async def run_all(texts, concurrency=8):\n",
    "        sem = asyncio.Semaphore(concurrency)\n",
    "        results = {}\n",
    "\n",
    "        async def bound_task(t):\n",
    "            async with sem:\n",
    "                score = await classify_text(t)\n",
    "            return t, score\n",
    "\n",
    "        tasks = [asyncio.create_task(bound_task(t)) for t in texts]\n",
    "        with tqdm(total=len(tasks), desc=f\"{MODEL_NAME} → {OUT_COL}\", unit=\"req\") as pbar:\n",
    "            for fut in asyncio.as_completed(tasks):\n",
    "                t, sc = await fut\n",
    "                results[t] = sc\n",
    "                pbar.update(1)\n",
    "        return results\n",
    "\n",
    "\n",
    "    loop = asyncio.get_event_loop()\n",
    "    scores_map = loop.run_until_complete(run_all(unique_texts, CONCURRENCY))\n",
    "\n",
    "    # Fill back & atomic save\n",
    "    for t, idxs in groups.items():\n",
    "        df.loc[idxs, OUT_COL] = scores_map.get(t, np.nan)\n",
    "\n",
    "    tmp_out = OUTPUT_CSV + \".tmp\"\n",
    "    df.to_csv(tmp_out, index=False, encoding=\"utf-8-sig\")\n",
    "    os.replace(tmp_out, OUTPUT_CSV)\n",
    "\n",
    "    print(f\"✅ Column {OUT_COL} written back -> {OUTPUT_CSV}\")\n"
   ],
   "id": "1980a9e8c299e765",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master table: posts_scores_2025-09-26.csv\n",
      "Unique texts to score: 31 (raw samples 32 / total 32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "meta-llama-3.1-8b-instruct → s1:   0%|          | 0/31 [00:00<?, ?req/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2c3810363b6b4d01833171f4bcce2521"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Column s1 written back -> data/reddit/scored/posts_scores_2025-09-26.csv\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T05:40:56.667003Z",
     "start_time": "2025-09-27T05:40:28.948781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# cell6 score——google/gemma-2-9b——s2===\n",
    "import os, re, glob, asyncio, numpy as np, pandas as pd\n",
    "import nest_asyncio;\n",
    "\n",
    "nest_asyncio.apply()\n",
    "from tqdm.auto import tqdm\n",
    "from openai import AsyncOpenAI\n",
    "import datetime as dt\n",
    "from datetime import timezone\n",
    "\n",
    "LMSTUDIO_BASE_URL = \"http://127.0.0.1:1234/v1\"\n",
    "LMSTUDIO_API_KEY = \"lm-studio\"\n",
    "MODEL_NAME = \"google/gemma-2-9b\"\n",
    "OUT_COL = \"s2\"\n",
    "\n",
    "# — Configuration: score all rows, lower concurrency, increase retries —\n",
    "VADER_EDGE = None  # no filtering\n",
    "DEDUP_UNIQUE_TEXTS = True  # de-duplicate texts (faster; failures affect all identical texts)\n",
    "MAX_TEXT_LEN = 256\n",
    "CONCURRENCY = 4  # lower for stability (tune as needed)\n",
    "RETRY = 5  # higher for stability\n",
    "FALLBACK_TO_VADER_SIGN = True  # if still NaN, fallback to sign of VADER\n",
    "\n",
    "# — Paths —\n",
    "D = (dt.datetime.now(timezone.utc) - dt.timedelta(days=1)).date()\n",
    "INPUT_CSV = f\"data/reddit/scored/posts_scores_{D}.csv\"\n",
    "if not os.path.exists(INPUT_CSV):\n",
    "    cands = sorted(glob.glob(\"data/reddit/scored/posts_scores_*.csv\"), key=os.path.getmtime)\n",
    "    if not cands: raise FileNotFoundError(\"Master table posts_scores_*.csv not found\")\n",
    "    INPUT_CSV = cands[-1]\n",
    "OUTPUT_CSV = INPUT_CSV\n",
    "\n",
    "df = pd.read_csv(INPUT_CSV, low_memory=False)\n",
    "\n",
    "# Text column fallback\n",
    "TEXT_COL = \"text_clean\"\n",
    "if TEXT_COL not in df.columns:\n",
    "    def _join(*xs): return \" \".join([str(x) for x in xs if pd.notna(x)])\n",
    "\n",
    "\n",
    "    df[\"__text_tmp__\"] = [_join(t, s, b) for t, s, b in zip(\n",
    "        df.get(\"title\", \"\"), df.get(\"selftext\", \"\"), df.get(\"body\", \"\")\n",
    "    )]\n",
    "    TEXT_COL = \"__text_tmp__\"\n",
    "\n",
    "# Prepare rows to score\n",
    "if OUT_COL not in df.columns: df[OUT_COL] = np.nan\n",
    "mask = df[OUT_COL].isna()\n",
    "if (VADER_EDGE is not None) and (\"vader\" in df.columns):\n",
    "    mask &= df[\"vader\"].abs() < VADER_EDGE\n",
    "\n",
    "todo = df.loc[mask].copy()\n",
    "if todo.empty:\n",
    "    print(f\"No samples to score ({OUT_COL} already exists or filtered by threshold). File: {INPUT_CSV}\")\n",
    "else:\n",
    "    todo[\"__txt\"] = todo[TEXT_COL].astype(str).str.slice(0, MAX_TEXT_LEN)\n",
    "\n",
    "    if DEDUP_UNIQUE_TEXTS:\n",
    "        groups = todo.groupby(\"__txt\").indices\n",
    "        work_items = list(groups.keys())  # unique texts\n",
    "\n",
    "\n",
    "        def write_back(scores_map):\n",
    "            for t, idxs in groups.items():\n",
    "                df.loc[idxs, OUT_COL] = scores_map.get(t, np.nan)\n",
    "    else:\n",
    "        work_items = list(todo.index)  # row indices\n",
    "\n",
    "\n",
    "        def write_back(scores_map):\n",
    "            for idx, sc in scores_map.items():\n",
    "                df.loc[idx, OUT_COL] = sc\n",
    "\n",
    "    print(f\"Master table: {os.path.basename(INPUT_CSV)} | to score: {len(work_items)} (dedup={DEDUP_UNIQUE_TEXTS})\")\n",
    "\n",
    "    PROMPT = (\n",
    "        \"Classify the sentiment as exactly one token from {{POS, NEU, NEG}}.\\n\"\n",
    "        \"Text:\\n{text}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    GRAMMAR = r'root ::= \"POS\" | \"NEU\" | \"NEG\"'\n",
    "\n",
    "\n",
    "    def map_label_to_score(label: str) -> float:\n",
    "        s = (label or \"\").strip().upper()\n",
    "        first = re.split(r\"\\s+\", s)[0] if s else \"\"\n",
    "        if first in {\"POS\", \"NEU\", \"NEG\"}:\n",
    "            return 1.0 if first == \"POS\" else (0.0 if first == \"NEU\" else -1.0)\n",
    "        if \"POS\" in s: return 1.0\n",
    "        if \"NEG\" in s: return -1.0\n",
    "        if \"NEU\" in s: return 0.0\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "    aclient = AsyncOpenAI(base_url=LMSTUDIO_BASE_URL, api_key=LMSTUDIO_API_KEY)\n",
    "\n",
    "\n",
    "    async def ask(text) -> float:\n",
    "        msg = PROMPT.format(text=text)\n",
    "        for attempt in range(RETRY):\n",
    "            try:\n",
    "                r = await aclient.chat.completions.create(\n",
    "                    model=MODEL_NAME,\n",
    "                    messages=[{\"role\": \"user\", \"content\": msg}],\n",
    "                    temperature=0,\n",
    "                    max_tokens=1,\n",
    "                    top_p=1,\n",
    "                    stop=[\"\\n\"],\n",
    "                    extra_body={\"grammar\": GRAMMAR}\n",
    "                )\n",
    "                return map_label_to_score(r.choices[0].message.content)\n",
    "            except Exception:\n",
    "                # exponential backoff\n",
    "                await asyncio.sleep(0.7 * (attempt + 1))\n",
    "                if attempt == RETRY - 1:\n",
    "                    return np.nan\n",
    "\n",
    "\n",
    "    async def run_batch(items, desc):\n",
    "        sem = asyncio.Semaphore(CONCURRENCY)\n",
    "        results = {}\n",
    "\n",
    "        async def one(item):\n",
    "            text = item if DEDUP_UNIQUE_TEXTS else todo.at[item, \"__txt\"]\n",
    "            async with sem:\n",
    "                sc = await ask(text)\n",
    "            return item, sc\n",
    "\n",
    "        tasks = [asyncio.create_task(one(x)) for x in items]\n",
    "        with tqdm(total=len(tasks), desc=desc, unit=\"req\") as pbar:\n",
    "            for fut in asyncio.as_completed(tasks):\n",
    "                k, sc = await fut\n",
    "                results[k] = sc\n",
    "                pbar.update(1)\n",
    "        return results\n",
    "\n",
    "\n",
    "    loop = asyncio.get_event_loop()\n",
    "    scores = loop.run_until_complete(run_batch(work_items, f\"{MODEL_NAME} → {OUT_COL}\"))\n",
    "\n",
    "    # Second pass: retry failed (NaN) items with reduced concurrency\n",
    "    failed = [k for k, v in scores.items() if (v is None) or (isinstance(v, float) and np.isnan(v))]\n",
    "    if failed:\n",
    "        print(f\"⚠️ {len(failed)} failed in the first pass, reducing concurrency and retrying…\")\n",
    "        CONCURRENCY = max(2, CONCURRENCY // 2)\n",
    "        retry_scores = loop.run_until_complete(run_batch(failed, f\"retry {MODEL_NAME} → {OUT_COL}\"))\n",
    "        scores.update(retry_scores)\n",
    "\n",
    "    # Write back\n",
    "    write_back(scores)\n",
    "\n",
    "    # Final fallback: remaining NaNs -> sign(vader) or 0\n",
    "    still_nan = df[OUT_COL].isna().sum()\n",
    "    if still_nan and FALLBACK_TO_VADER_SIGN:\n",
    "        print(f\"⚠️ {still_nan} rows still NaN; filling with sign of VADER.\")\n",
    "        sign = np.sign(df.get(\"vader\", 0.0).fillna(0.0))\n",
    "        df.loc[df[OUT_COL].isna(), OUT_COL] = sign.replace(0, 0.0)\n",
    "\n",
    "    tmp = OUTPUT_CSV + \".tmp\"\n",
    "    df.to_csv(tmp, index=False, encoding=\"utf-8-sig\")\n",
    "    os.replace(tmp, OUTPUT_CSV)\n",
    "    print(f\"✅ Column {OUT_COL} written back -> {OUTPUT_CSV}\")\n"
   ],
   "id": "d6c7e83f662467d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master table: posts_scores_2025-09-26.csv | to score: 31 (dedup=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "google/gemma-2-9b → s2:   0%|          | 0/31 [00:00<?, ?req/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aad843c059414db1afa93c04093ce1e6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Column s2 written back -> data/reddit/scored/posts_scores_2025-09-26.csv\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T05:43:14.111793Z",
     "start_time": "2025-09-27T05:43:09.852725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# cell 7 score——qwen2.5-7b-instruct-1m——s3\n",
    "\n",
    "import os, re, glob, asyncio, numpy as np, pandas as pd\n",
    "import nest_asyncio;\n",
    "\n",
    "nest_asyncio.apply()\n",
    "from tqdm.auto import tqdm\n",
    "from openai import AsyncOpenAI\n",
    "import datetime as dt\n",
    "\n",
    "# ---- Paths (relative to project root) ----\n",
    "D = (dt.datetime.utcnow().date() - dt.timedelta(days=1))\n",
    "MASTER_DIR = \"data/reddit/scored\"\n",
    "PREF = os.path.join(MASTER_DIR, f\"posts_scores_{D}.csv\")\n",
    "\n",
    "if os.path.exists(PREF):\n",
    "    MASTER_PATH = PREF\n",
    "else:\n",
    "    cands = sorted(glob.glob(os.path.join(MASTER_DIR, \"posts_scores_*.csv\")), key=os.path.getmtime)\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(\n",
    "            \"Master table not found: data/reddit/scored/posts_scores_*.csv. Run the VADER step first.\")\n",
    "    MASTER_PATH = cands[-1]\n",
    "\n",
    "# ---- LM Studio configuration ----\n",
    "LMSTUDIO_BASE_URL = \"http://127.0.0.1:1234/v1\"\n",
    "LMSTUDIO_API_KEY = \"lm-studio\"\n",
    "MODEL_NAME = \"qwen2.5-7b-instruct-1m\"  # ← set to the API identifier shown in LM Studio (right panel)\n",
    "OUT_COL = \"s3\"\n",
    "\n",
    "# ---- Scoring parameters (full coverage) ----\n",
    "TEXT_COL = \"text_clean\"\n",
    "MAX_TEXT_LEN = 256\n",
    "CONCURRENCY = 8\n",
    "RETRY = 3\n",
    "\n",
    "PROMPT = (\n",
    "    \"Classify the sentiment as exactly one token from {{POS, NEU, NEG}}.\\n\"\n",
    "    \"Text:\\n{text}\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "GRAMMAR = r'root ::= \"POS\" | \"NEU\" | \"NEG\"'\n",
    "\n",
    "\n",
    "def map_label_to_score(label: str) -> float:\n",
    "    s = (label or \"\").strip().upper()\n",
    "    first = re.split(r\"\\s+\", s)[0] if s else \"\"\n",
    "    if first in {\"POS\", \"NEU\", \"NEG\"}:\n",
    "        return 1.0 if first == \"POS\" else (0.0 if first == \"NEU\" else -1.0)\n",
    "    if \"POS\" in s: return 1.0\n",
    "    if \"NEG\" in s: return -1.0\n",
    "    if \"NEU\" in s: return 0.0\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "# ---- Read master table & select samples to score (full; fill missing only) ----\n",
    "df = pd.read_csv(MASTER_PATH, low_memory=False)\n",
    "\n",
    "# Fallback for text column\n",
    "if TEXT_COL not in df.columns:\n",
    "    def _join(*xs): return \" \".join([str(x) for x in xs if pd.notna(x)])\n",
    "\n",
    "\n",
    "    df[\"__text_tmp__\"] = [_join(t, s, b) for t, s, b in zip(\n",
    "        df.get(\"title\", \"\"), df.get(\"selftext\", \"\"), df.get(\"body\", \"\")\n",
    "    )]\n",
    "    TEXT_COL = \"__text_tmp__\"\n",
    "\n",
    "# Ensure output column exists\n",
    "if OUT_COL not in df.columns:\n",
    "    df[OUT_COL] = np.nan\n",
    "\n",
    "# Only fill missing values\n",
    "mask = df[OUT_COL].isna()\n",
    "todo = df.loc[mask].copy()\n",
    "\n",
    "if todo.empty:\n",
    "    print(f\"No samples to score ({OUT_COL} already exists with no missing). File: {MASTER_PATH}\")\n",
    "else:\n",
    "    # De-duplicate by text to speed up requests\n",
    "    todo[\"__txt\"] = todo[TEXT_COL].astype(str).str.slice(0, MAX_TEXT_LEN)\n",
    "    groups = todo.groupby(\"__txt\").indices\n",
    "    unique_texts = list(groups.keys())\n",
    "    print(f\"Target file: {MASTER_PATH}\")\n",
    "    print(f\"Unique texts to score: {len(unique_texts)} (raw {len(todo)} / total {len(df)})\")\n",
    "\n",
    "    aclient = AsyncOpenAI(base_url=LMSTUDIO_BASE_URL, api_key=LMSTUDIO_API_KEY)\n",
    "\n",
    "\n",
    "    async def classify_text(t: str) -> float:\n",
    "        msg = PROMPT.format(text=t)\n",
    "        for attempt in range(RETRY):\n",
    "            try:\n",
    "                r = await aclient.chat.completions.create(\n",
    "                    model=MODEL_NAME,\n",
    "                    messages=[{\"role\": \"user\", \"content\": msg}],\n",
    "                    temperature=0,\n",
    "                    max_tokens=1,\n",
    "                    top_p=1,\n",
    "                    stop=[\"\\n\"],\n",
    "                    extra_body={\"grammar\": GRAMMAR}\n",
    "                )\n",
    "                return map_label_to_score(r.choices[0].message.content)\n",
    "            except Exception:\n",
    "                if attempt == RETRY - 1:\n",
    "                    return np.nan\n",
    "                await asyncio.sleep(0.6 * (attempt + 1))\n",
    "\n",
    "\n",
    "    async def run_all(texts, concurrency=8):\n",
    "        sem = asyncio.Semaphore(concurrency)\n",
    "        results = {}\n",
    "\n",
    "        async def bound_task(t):\n",
    "            async with sem:\n",
    "                sc = await classify_text(t)\n",
    "            return t, sc\n",
    "\n",
    "        tasks = [asyncio.create_task(bound_task(t)) for t in texts]\n",
    "        with tqdm(total=len(tasks), desc=f\"{MODEL_NAME} → {OUT_COL}\", unit=\"req\") as pbar:\n",
    "            for fut in asyncio.as_completed(tasks):\n",
    "                t, sc = await fut\n",
    "                results[t] = sc\n",
    "                pbar.update(1)\n",
    "        return results\n",
    "\n",
    "\n",
    "    loop = asyncio.get_event_loop()\n",
    "    scores_map = loop.run_until_complete(run_all(unique_texts, CONCURRENCY))\n",
    "\n",
    "    # Write back & atomically persist (same file)\n",
    "    for t, idxs in groups.items():\n",
    "        df.loc[idxs, OUT_COL] = scores_map.get(t, np.nan)\n",
    "\n",
    "    tmp_out = MASTER_PATH + \".tmp\"\n",
    "    df.to_csv(tmp_out, index=False, encoding=\"utf-8-sig\")\n",
    "    os.replace(tmp_out, MASTER_PATH)\n",
    "\n",
    "    print(f\"✅ Column {OUT_COL} written back -> {MASTER_PATH}\")\n"
   ],
   "id": "9e9cea083fece5ec",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jimmy\\AppData\\Local\\Temp\\ipykernel_36984\\1668030362.py:12: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  D = (dt.datetime.utcnow().date() - dt.timedelta(days=1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target file: data/reddit/scored\\posts_scores_2025-09-26.csv\n",
      "Unique texts to score: 31 (raw 32 / total 32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "qwen2.5-7b-instruct-1m → s3:   0%|          | 0/31 [00:00<?, ?req/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "75b1aa77195147d688e89a8348c90fcb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Column s3 written back -> data/reddit/scored\\posts_scores_2025-09-26.csv\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T05:45:46.734570Z",
     "start_time": "2025-09-27T05:45:42.695873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# cell8 score mistralai/mistral-7b-instruct-v0.3-s4\n",
    "import os, re, glob, asyncio, numpy as np, pandas as pd\n",
    "import nest_asyncio;\n",
    "\n",
    "nest_asyncio.apply()\n",
    "from tqdm.auto import tqdm\n",
    "from openai import AsyncOpenAI\n",
    "import datetime as dt\n",
    "\n",
    "# ---- Paths (relative to project root) ----\n",
    "D = (dt.datetime.utcnow().date() - dt.timedelta(days=1))\n",
    "MASTER_DIR = \"data/reddit/scored\"\n",
    "PREF = os.path.join(MASTER_DIR, f\"posts_scores_{D}.csv\")\n",
    "MASTER_PATH = PREF if os.path.exists(PREF) else sorted(\n",
    "    glob.glob(os.path.join(MASTER_DIR, \"posts_scores_*.csv\")),\n",
    "    key=os.path.getmtime\n",
    ")[-1]\n",
    "\n",
    "# ---- LM Studio configuration (set MODEL_NAME to your LM Studio API identifier) ----\n",
    "LMSTUDIO_BASE_URL = \"http://127.0.0.1:1234/v1\"\n",
    "LMSTUDIO_API_KEY = \"lm-studio\"\n",
    "MODEL_NAME = \"mistralai/mistral-7b-instruct-v0.3\"  # ← change to the API identifier shown in LM Studio (right panel)\n",
    "OUT_COL = \"s4\"\n",
    "\n",
    "# ---- Scoring parameters ----\n",
    "TEXT_COL = \"text_clean\"\n",
    "MAX_TEXT_LEN = 256\n",
    "CONCURRENCY = 8\n",
    "RETRY = 3\n",
    "DEDUP_UNIQUE_TEXTS = True  # True: score each unique text once (faster); False: score every row\n",
    "\n",
    "PROMPT = (\n",
    "    \"Classify the sentiment as exactly one token from {{POS, NEU, NEG}}.\\n\"\n",
    "    \"Text:\\n{text}\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "GRAMMAR = r'root ::= \"POS\" | \"NEU\" | \"NEG\"'\n",
    "\n",
    "\n",
    "def map_label_to_score(label: str) -> float:\n",
    "    s = (label or \"\").strip().upper()\n",
    "    first = re.split(r\"\\s+\", s)[0] if s else \"\"\n",
    "    if first in {\"POS\", \"NEU\", \"NEG\"}:\n",
    "        return 1.0 if first == \"POS\" else (0.0 if first == \"NEU\" else -1.0)\n",
    "    if \"POS\" in s: return 1.0\n",
    "    if \"NEG\" in s: return -1.0\n",
    "    if \"NEU\" in s: return 0.0\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "# ---- Read master table & select rows to score (fill missing only) ----\n",
    "df = pd.read_csv(MASTER_PATH, low_memory=False)\n",
    "\n",
    "if TEXT_COL not in df.columns:\n",
    "    def _join(*xs): return \" \".join([str(x) for x in xs if pd.notna(x)])\n",
    "\n",
    "\n",
    "    df[\"__text_tmp__\"] = [_join(t, s, b) for t, s, b in zip(\n",
    "        df.get(\"title\", \"\"), df.get(\"selftext\", \"\"), df.get(\"body\", \"\")\n",
    "    )]\n",
    "    TEXT_COL = \"__text_tmp__\"\n",
    "\n",
    "if OUT_COL not in df.columns:\n",
    "    df[OUT_COL] = np.nan\n",
    "\n",
    "todo = df.loc[df[OUT_COL].isna()].copy()\n",
    "if todo.empty:\n",
    "    print(f\"No samples to score ({OUT_COL} already exists with no missing). File: {MASTER_PATH}\")\n",
    "else:\n",
    "    if DEDUP_UNIQUE_TEXTS:\n",
    "        todo[\"__txt\"] = todo[TEXT_COL].astype(str).str.slice(0, MAX_TEXT_LEN)\n",
    "        groups = todo.groupby(\"__txt\").indices\n",
    "        work_items = list(groups.keys())  # set of unique texts\n",
    "\n",
    "\n",
    "        def write_back(scores_map):\n",
    "            for t, idxs in groups.items():\n",
    "                df.loc[idxs, OUT_COL] = scores_map.get(t, np.nan)\n",
    "    else:\n",
    "        todo[\"__txt\"] = todo[TEXT_COL].astype(str).str.slice(0, MAX_TEXT_LEN)\n",
    "        work_items = list(todo.index)  # set of row indices\n",
    "\n",
    "\n",
    "        def write_back(scores_map):\n",
    "            for idx, sc in scores_map.items():\n",
    "                df.loc[idx, OUT_COL] = sc\n",
    "\n",
    "    print(f\"Target file: {MASTER_PATH}\")\n",
    "    print(f\"To score: {len(work_items)} (dedup={DEDUP_UNIQUE_TEXTS})\")\n",
    "\n",
    "    aclient = AsyncOpenAI(base_url=LMSTUDIO_BASE_URL, api_key=LMSTUDIO_API_KEY)\n",
    "\n",
    "\n",
    "    async def classify_text(text: str) -> float:\n",
    "        msg = PROMPT.format(text=text)\n",
    "        for attempt in range(RETRY):\n",
    "            try:\n",
    "                r = await aclient.chat.completions.create(\n",
    "                    model=MODEL_NAME,\n",
    "                    messages=[{\"role\": \"user\", \"content\": msg}],\n",
    "                    temperature=0,\n",
    "                    max_tokens=1,\n",
    "                    top_p=1,\n",
    "                    stop=[\"\\n\"],\n",
    "                    extra_body={\"grammar\": GRAMMAR}\n",
    "                )\n",
    "                return map_label_to_score(r.choices[0].message.content)\n",
    "            except Exception:\n",
    "                if attempt == RETRY - 1:\n",
    "                    return np.nan\n",
    "                await asyncio.sleep(0.6 * (attempt + 1))\n",
    "\n",
    "\n",
    "    async def run_all(items):\n",
    "        sem = asyncio.Semaphore(CONCURRENCY)\n",
    "        results = {}\n",
    "\n",
    "        async def run_one(item):\n",
    "            text = item if DEDUP_UNIQUE_TEXTS else todo.at[item, \"__txt\"]\n",
    "            async with sem:\n",
    "                sc = await classify_text(text)\n",
    "            return item, sc\n",
    "\n",
    "        tasks = [asyncio.create_task(run_one(x)) for x in items]\n",
    "        with tqdm(total=len(tasks), desc=f\"{MODEL_NAME} → {OUT_COL}\", unit=\"req\") as pbar:\n",
    "            for fut in asyncio.as_completed(tasks):\n",
    "                k, sc = await fut\n",
    "                results[k] = sc\n",
    "                pbar.update(1)\n",
    "        return results\n",
    "\n",
    "\n",
    "    loop = asyncio.get_event_loop()\n",
    "    scores_map = loop.run_until_complete(run_all(work_items))\n",
    "\n",
    "    # Write back & atomic persist\n",
    "    write_back(scores_map)\n",
    "    tmp_out = MASTER_PATH + \".tmp\"\n",
    "    df.to_csv(tmp_out, index=False, encoding=\"utf-8-sig\")\n",
    "    os.replace(tmp_out, MASTER_PATH)\n",
    "\n",
    "    print(f\"✅ Column {OUT_COL} written back -> {MASTER_PATH}\")\n"
   ],
   "id": "c3f6f9da352e532d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jimmy\\AppData\\Local\\Temp\\ipykernel_36984\\3267470759.py:12: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  D = (dt.datetime.utcnow().date() - dt.timedelta(days=1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target file: data/reddit/scored\\posts_scores_2025-09-26.csv\n",
      "To score: 31 (dedup=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mistralai/mistral-7b-instruct-v0.3 → s4:   0%|          | 0/31 [00:00<?, ?req/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "988adaa7f86748baa1b29ee34970d3f9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Column s4 written back -> data/reddit/scored\\posts_scores_2025-09-26.csv\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T05:46:54.661857Z",
     "start_time": "2025-09-27T05:46:50.095386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# cell 9 score nous-hermes-2-mistral-7b-dpo——s5\n",
    "\n",
    "import os, re, glob, asyncio, numpy as np, pandas as pd\n",
    "import nest_asyncio;\n",
    "\n",
    "nest_asyncio.apply()\n",
    "from tqdm.auto import tqdm\n",
    "from openai import AsyncOpenAI\n",
    "import datetime as dt\n",
    "\n",
    "# ---- Paths (relative to project root) ----\n",
    "D = (dt.datetime.utcnow().date() - dt.timedelta(days=1))\n",
    "MASTER_DIR = \"data/reddit/scored\"\n",
    "PREF = os.path.join(MASTER_DIR, f\"posts_scores_{D}.csv\")\n",
    "MASTER_PATH = PREF if os.path.exists(PREF) else sorted(\n",
    "    glob.glob(os.path.join(MASTER_DIR, \"posts_scores_*.csv\")),\n",
    "    key=os.path.getmtime\n",
    ")[-1]\n",
    "\n",
    "# ---- LM Studio configuration (set MODEL_NAME to your LM Studio API identifier) ----\n",
    "LMSTUDIO_BASE_URL = \"http://127.0.0.1:1234/v1\"\n",
    "LMSTUDIO_API_KEY = \"lm-studio\"\n",
    "MODEL_NAME = \"nous-hermes-2-mistral-7b-dpo\"  # ← change to the API identifier shown in LM Studio (right panel)\n",
    "OUT_COL = \"s5\"\n",
    "\n",
    "# ---- Scoring parameters ----\n",
    "TEXT_COL = \"text_clean\"\n",
    "MAX_TEXT_LEN = 256\n",
    "CONCURRENCY = 8\n",
    "RETRY = 3\n",
    "DEDUP_UNIQUE_TEXTS = True  # True: score each unique text once (faster); False: score every row\n",
    "\n",
    "PROMPT = (\n",
    "    \"Classify the sentiment as exactly one token from {{POS, NEU, NEG}}.\\n\"\n",
    "    \"Text:\\n{text}\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "GRAMMAR = r'root ::= \"POS\" | \"NEU\" | \"NEG\"'\n",
    "\n",
    "\n",
    "def map_label_to_score(label: str) -> float:\n",
    "    s = (label or \"\").strip().upper()\n",
    "    first = re.split(r\"\\s+\", s)[0] if s else \"\"\n",
    "    if first in {\"POS\", \"NEU\", \"NEG\"}:\n",
    "        return 1.0 if first == \"POS\" else (0.0 if first == \"NEU\" else -1.0)\n",
    "    if \"POS\" in s: return 1.0\n",
    "    if \"NEG\" in s: return -1.0\n",
    "    if \"NEU\" in s: return 0.0\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "# ---- Read master table & select rows to score (fill missing only) ----\n",
    "df = pd.read_csv(MASTER_PATH, low_memory=False)\n",
    "\n",
    "if TEXT_COL not in df.columns:\n",
    "    def _join(*xs): return \" \".join([str(x) for x in xs if pd.notna(x)])\n",
    "\n",
    "\n",
    "    df[\"__text_tmp__\"] = [_join(t, s, b) for t, s, b in zip(\n",
    "        df.get(\"title\", \"\"), df.get(\"selftext\", \"\"), df.get(\"body\", \"\")\n",
    "    )]\n",
    "    TEXT_COL = \"__text_tmp__\"\n",
    "\n",
    "if OUT_COL not in df.columns:\n",
    "    df[OUT_COL] = np.nan\n",
    "\n",
    "todo = df.loc[df[OUT_COL].isna()].copy()\n",
    "if todo.empty:\n",
    "    print(f\"No samples to score ({OUT_COL} already exists with no missing). File: {MASTER_PATH}\")\n",
    "else:\n",
    "    if DEDUP_UNIQUE_TEXTS:\n",
    "        todo[\"__txt\"] = todo[TEXT_COL].astype(str).str.slice(0, MAX_TEXT_LEN)\n",
    "        groups = todo.groupby(\"__txt\").indices\n",
    "        work_items = list(groups.keys())\n",
    "\n",
    "\n",
    "        def write_back(scores_map):\n",
    "            for t, idxs in groups.items():\n",
    "                df.loc[idxs, OUT_COL] = scores_map.get(t, np.nan)\n",
    "    else:\n",
    "        todo[\"__txt\"] = todo[TEXT_COL].astype(str).str.slice(0, MAX_TEXT_LEN)\n",
    "        work_items = list(todo.index)\n",
    "\n",
    "\n",
    "        def write_back(scores_map):\n",
    "            for idx, sc in scores_map.items():\n",
    "                df.loc[idx, OUT_COL] = sc\n",
    "\n",
    "    print(f\"Target file: {MASTER_PATH}\")\n",
    "    print(f\"To score: {len(work_items)} (dedup={DEDUP_UNIQUE_TEXTS})\")\n",
    "\n",
    "    aclient = AsyncOpenAI(base_url=LMSTUDIO_BASE_URL, api_key=LMSTUDIO_API_KEY)\n",
    "\n",
    "\n",
    "    async def classify_text(text: str) -> float:\n",
    "        msg = PROMPT.format(text=text)\n",
    "        for attempt in range(RETRY):\n",
    "            try:\n",
    "                r = await aclient.chat.completions.create(\n",
    "                    model=MODEL_NAME,\n",
    "                    messages=[{\"role\": \"user\", \"content\": msg}],\n",
    "                    temperature=0,\n",
    "                    max_tokens=1,\n",
    "                    top_p=1,\n",
    "                    stop=[\"\\n\"],\n",
    "                    extra_body={\"grammar\": GRAMMAR}\n",
    "                )\n",
    "                return map_label_to_score(r.choices[0].message.content)\n",
    "            except Exception:\n",
    "                if attempt == RETRY - 1:\n",
    "                    return np.nan\n",
    "                await asyncio.sleep(0.6 * (attempt + 1))\n",
    "\n",
    "\n",
    "    async def run_all(items):\n",
    "        sem = asyncio.Semaphore(CONCURRENCY)\n",
    "        results = {}\n",
    "\n",
    "        async def run_one(item):\n",
    "            text = item if DEDUP_UNIQUE_TEXTS else todo.at[item, \"__txt\"]\n",
    "            async with sem:\n",
    "                sc = await classify_text(text)\n",
    "            return item, sc\n",
    "\n",
    "        tasks = [asyncio.create_task(run_one(x)) for x in items]\n",
    "        with tqdm(total=len(tasks), desc=f\"{MODEL_NAME} → {OUT_COL}\", unit=\"req\") as pbar:\n",
    "            for fut in asyncio.as_completed(tasks):\n",
    "                k, sc = await fut\n",
    "                results[k] = sc\n",
    "                pbar.update(1)\n",
    "        return results\n",
    "\n",
    "\n",
    "    loop = asyncio.get_event_loop()\n",
    "    scores_map = loop.run_until_complete(run_all(work_items))\n",
    "\n",
    "    # Write back & atomic persist\n",
    "    write_back(scores_map)\n",
    "    tmp_out = MASTER_PATH + \".tmp\"\n",
    "    df.to_csv(tmp_out, index=False, encoding=\"utf-8-sig\")\n",
    "    os.replace(tmp_out, MASTER_PATH)\n",
    "\n",
    "    print(f\"✅ Column {OUT_COL} written back -> {MASTER_PATH}\")\n"
   ],
   "id": "963dca94bbe997dc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jimmy\\AppData\\Local\\Temp\\ipykernel_36984\\2045195372.py:13: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  D = (dt.datetime.utcnow().date() - dt.timedelta(days=1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target file: data/reddit/scored\\posts_scores_2025-09-26.csv\n",
      "To score: 31 (dedup=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nous-hermes-2-mistral-7b-dpo → s5:   0%|          | 0/31 [00:00<?, ?req/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c398addc162546c480cf944e5df74b55"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Column s5 written back -> data/reddit/scored\\posts_scores_2025-09-26.csv\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T05:47:32.144142Z",
     "start_time": "2025-09-27T05:47:31.929516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# Weighted daily -> 1min forward-fill (vader + s1..s5)\n",
    "# Aggregate from data/reddit/scored to daily values; expand to 1-minute with forward-fill\n",
    "# =========================\n",
    "# pip install pandas numpy\n",
    "\n",
    "import os, glob, math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---- Paths ----\n",
    "SCORED_DIR = \"data/reddit/scored\"\n",
    "OUT_DIR    = \"data/reddit/weighted\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ---- Read all posts_scores_*.csv ----\n",
    "paths = sorted(glob.glob(os.path.join(SCORED_DIR, \"posts_scores_*.csv\")))\n",
    "if not paths:\n",
    "    raise FileNotFoundError(\"No posts_scores_*.csv found under data/reddit/scored/. Please generate the master table first.\")\n",
    "\n",
    "dfs = []\n",
    "for p in paths:\n",
    "    try:\n",
    "        dfp = pd.read_csv(p, low_memory=False)\n",
    "    except UnicodeDecodeError:\n",
    "        dfp = pd.read_csv(p, encoding=\"latin-1\", low_memory=False)\n",
    "    dfp[\"__source\"] = os.path.basename(p)\n",
    "    dfs.append(dfp)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# ---- Sentiment columns: support both naming styles (s1..s5 or sent_s1..sent_s5); normalize to s1..s5 ----\n",
    "sent_cols = []\n",
    "if \"vader\" in df.columns:\n",
    "    sent_cols.append(\"vader\")\n",
    "\n",
    "# Prefer s1..s5; if missing, map sent_s1..sent_s5 -> s1..s5\n",
    "for k in [\"s1\",\"s2\",\"s3\",\"s4\",\"s5\"]:\n",
    "    if k in df.columns:\n",
    "        sent_cols.append(k)\n",
    "    elif f\"sent_{k}\" in df.columns:\n",
    "        df[k] = pd.to_numeric(df[f\"sent_{k}\"], errors=\"coerce\")\n",
    "        sent_cols.append(k)\n",
    "\n",
    "if not sent_cols:\n",
    "    raise ValueError(\"No sentiment columns found (expect at least one among vader + s1..s5).\")\n",
    "\n",
    "# ---- Timestamps (prefer created_time_utc, else created_utc seconds) ----\n",
    "if \"created_time_utc\" in df.columns:\n",
    "    t = pd.to_datetime(df[\"created_time_utc\"], errors=\"coerce\", utc=True)\n",
    "elif \"created_utc\" in df.columns:\n",
    "    t = pd.to_datetime(df[\"created_utc\"], unit=\"s\", errors=\"coerce\", utc=True)\n",
    "else:\n",
    "    raise ValueError(\"Requires 'created_time_utc' or 'created_utc' column.\")\n",
    "\n",
    "df[\"created_time_utc\"] = t\n",
    "df = df.dropna(subset=[\"created_time_utc\"])\n",
    "df = df[df[\"created_time_utc\"] >= \"2005-01-01\"]\n",
    "\n",
    "# ---- De-duplicate (by id if available) ----\n",
    "if \"id\" in df.columns:\n",
    "    df = df.sort_values(\"created_time_utc\").drop_duplicates(\"id\", keep=\"last\")\n",
    "\n",
    "# ---- Weights: log(1+score) + 0.5*log(1+num_comments) ----\n",
    "score = pd.to_numeric(df.get(\"score\", 0), errors=\"coerce\").fillna(0).clip(lower=0)\n",
    "if \"num_comments\" in df.columns:\n",
    "    numc = pd.to_numeric(df[\"num_comments\"], errors=\"coerce\").fillna(0).clip(lower=0)\n",
    "    weight = score.apply(math.log1p) + 0.5 * numc.apply(math.log1p)\n",
    "else:\n",
    "    weight = score.apply(math.log1p)\n",
    "df[\"__w\"] = weight\n",
    "\n",
    "# ---- Cast sentiment columns to numeric and clip to [-1, 1] ----\n",
    "for c in sent_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\").clip(-1, 1)\n",
    "\n",
    "# ---- Daily aggregation (UTC day): weighted average; if total weight=0, fall back to simple mean ----\n",
    "df[\"date_utc\"] = df[\"created_time_utc\"].dt.date\n",
    "\n",
    "def wavg(series: pd.Series, w: pd.Series) -> float:\n",
    "    s = series.astype(float)\n",
    "    w = w.astype(float)\n",
    "    den = np.nansum(w)\n",
    "    return float(np.nansum(s * w) / den) if den > 0 else float(np.nanmean(s))\n",
    "\n",
    "daily_rows = []\n",
    "for d, g in df.groupby(\"date_utc\"):\n",
    "    row = {\"date_utc\": d}\n",
    "    for c in sent_cols:\n",
    "        row[c] = wavg(g[c], g[\"__w\"])\n",
    "    daily_rows.append(row)\n",
    "\n",
    "daily = pd.DataFrame(daily_rows).sort_values(\"date_utc\").reset_index(drop=True)\n",
    "\n",
    "# ---- Expand to 1-minute and forward-fill (merge_asof for stability) ----\n",
    "# 1) Set daily timestamps to 00:00 UTC\n",
    "daily_ff = daily.copy()\n",
    "daily_ff[\"ts_day\"] = pd.to_datetime(daily_ff[\"date_utc\"]).dt.tz_localize(\"UTC\")\n",
    "daily_ff = daily_ff.sort_values(\"ts_day\").reset_index(drop=True)\n",
    "\n",
    "# 2) Build minute index (UTC)\n",
    "start = daily_ff[\"ts_day\"].min()\n",
    "end   = daily_ff[\"ts_day\"].max() + pd.Timedelta(days=1) - pd.Timedelta(minutes=1)\n",
    "minute_df = pd.DataFrame({\"ts\": pd.date_range(start=start, end=end, freq=\"min\", tz=\"UTC\")})\n",
    "\n",
    "# 3) Backward asof-merge: each minute takes the most recent daily 00:00 value (not later than itself)\n",
    "joined = pd.merge_asof(\n",
    "    minute_df.sort_values(\"ts\"),\n",
    "    daily_ff[[\"ts_day\"] + sent_cols].sort_values(\"ts_day\"),\n",
    "    left_on=\"ts\", right_on=\"ts_day\",\n",
    "    direction=\"backward\"\n",
    ")\n",
    "\n",
    "# 4) Clean columns: keep ts + sentiment columns only\n",
    "minute_df = joined.drop(columns=[\"ts_day\"])\n",
    "minute_df = minute_df[[\"ts\"] + sent_cols]\n",
    "\n",
    "# ---- Save ----\n",
    "daily_out  = os.path.join(OUT_DIR, \"sentiment_daily_vader_s1_s5.csv\")\n",
    "minute_out = os.path.join(OUT_DIR, \"sentiment_1min_vader_s1_s5.csv\")\n",
    "\n",
    "daily_out_df = daily.rename(columns={\"date_utc\": \"ts\"})\n",
    "daily_out_df[\"ts\"] = pd.to_datetime(daily_out_df[\"ts\"]).dt.tz_localize(\"UTC\")\n",
    "\n",
    "daily_out_df.to_csv(daily_out, index=False, encoding=\"utf-8\")\n",
    "minute_df.to_csv(minute_out, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"✅ Done.\")\n",
    "print(f\"Days   : {daily_out_df['ts'].min().date()} ~ {daily_out_df['ts'].max().date()}\")\n",
    "print(f\"Daily  : {daily_out}\")\n",
    "print(f\"1-min  : {minute_out}\")\n",
    "print(\"\\nPreview (daily):\")\n",
    "display(daily_out_df.head(3))\n",
    "print(\"\\nPreview (1-min):\")\n",
    "display(minute_df.head(3))\n",
    "\n"
   ],
   "id": "c2e0b49cef057f33",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done.\n",
      "Days   : 2025-09-24 ~ 2025-09-26\n",
      "Daily  : data/reddit/weighted\\sentiment_daily_vader_s1_s5.csv\n",
      "1-min  : data/reddit/weighted\\sentiment_1min_vader_s1_s5.csv\n",
      "\n",
      "Preview (daily):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                         ts     vader        s1        s2        s3        s4  \\\n",
       "0 2025-09-24 00:00:00+00:00  0.404800 -0.239790  0.035424 -0.152483 -0.232451   \n",
       "1 2025-09-26 00:00:00+00:00  0.489822 -0.225582 -0.042975 -0.204605 -0.303092   \n",
       "\n",
       "         s5  \n",
       "0 -0.167206  \n",
       "1 -0.252197  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>vader</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-09-24 00:00:00+00:00</td>\n",
       "      <td>0.404800</td>\n",
       "      <td>-0.239790</td>\n",
       "      <td>0.035424</td>\n",
       "      <td>-0.152483</td>\n",
       "      <td>-0.232451</td>\n",
       "      <td>-0.167206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-09-26 00:00:00+00:00</td>\n",
       "      <td>0.489822</td>\n",
       "      <td>-0.225582</td>\n",
       "      <td>-0.042975</td>\n",
       "      <td>-0.204605</td>\n",
       "      <td>-0.303092</td>\n",
       "      <td>-0.252197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preview (1-min):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                         ts   vader       s1        s2        s3        s4  \\\n",
       "0 2025-09-24 00:00:00+00:00  0.4048 -0.23979  0.035424 -0.152483 -0.232451   \n",
       "1 2025-09-24 00:01:00+00:00  0.4048 -0.23979  0.035424 -0.152483 -0.232451   \n",
       "2 2025-09-24 00:02:00+00:00  0.4048 -0.23979  0.035424 -0.152483 -0.232451   \n",
       "\n",
       "         s5  \n",
       "0 -0.167206  \n",
       "1 -0.167206  \n",
       "2 -0.167206  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>vader</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-09-24 00:00:00+00:00</td>\n",
       "      <td>0.4048</td>\n",
       "      <td>-0.23979</td>\n",
       "      <td>0.035424</td>\n",
       "      <td>-0.152483</td>\n",
       "      <td>-0.232451</td>\n",
       "      <td>-0.167206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-09-24 00:01:00+00:00</td>\n",
       "      <td>0.4048</td>\n",
       "      <td>-0.23979</td>\n",
       "      <td>0.035424</td>\n",
       "      <td>-0.152483</td>\n",
       "      <td>-0.232451</td>\n",
       "      <td>-0.167206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-09-24 00:02:00+00:00</td>\n",
       "      <td>0.4048</td>\n",
       "      <td>-0.23979</td>\n",
       "      <td>0.035424</td>\n",
       "      <td>-0.152483</td>\n",
       "      <td>-0.232451</td>\n",
       "      <td>-0.167206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

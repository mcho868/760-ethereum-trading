{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-25T07:55:10.057405Z",
     "start_time": "2025-09-25T07:54:21.295206Z"
    }
   },
   "source": [
    "# === Grab yesterday's Ethereum-trading Reddit posts (UTC) -> CSV ===\n",
    "# 依赖：praw, pandas, python-dotenv（会自动安装）\n",
    "import os, sys, time, re, datetime as dt\n",
    "from datetime import timezone\n",
    "from typing import List\n",
    "\n",
    "def _pip_install(pkgs: List[str]):\n",
    "    import importlib, subprocess\n",
    "    for p in pkgs:\n",
    "        try:\n",
    "            importlib.import_module(p.split(\"==\")[0])\n",
    "        except Exception:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", p])\n",
    "\n",
    "_pip_install([\"praw\", \"pandas\", \"python-dotenv\"])\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # 读取项目根目录的 .env；若 Notebook 在子目录，先 os.chdir 到根目录\n",
    "import praw\n",
    "import pandas as pd\n",
    "\n",
    "# 读取凭证（来自 .env）\n",
    "CLIENT_ID     = os.getenv(\"REDDIT_CLIENT_ID\")\n",
    "CLIENT_SECRET = os.getenv(\"REDDIT_CLIENT_SECRET\")\n",
    "USER_AGENT    = os.getenv(\"REDDIT_USER_AGENT\", \"eth-sentiment-bot/0.1\")\n",
    "\n",
    "if not CLIENT_ID or not CLIENT_SECRET or not USER_AGENT:\n",
    "    raise RuntimeError(\"缺少 Reddit 凭证：请在 .env 中设置 REDDIT_CLIENT_ID / REDDIT_CLIENT_SECRET / REDDIT_USER_AGENT\")\n",
    "\n",
    "# 子版块与关键词（可按需增减）\n",
    "SUBREDDITS = [\"ethereum\", \"ethfinance\", \"ethtrader\", \"CryptoCurrency\", \"defi\", \"ethdev\"]\n",
    "KEYWORDS = [\n",
    "    r\"\\beth\\b\", r\"\\bethereum\\b\", r\"\\beth/usdt\\b\", r\"\\bethusd\\b\", r\"\\bethusdt\\b\",\n",
    "    r\"\\bspot\\b\", r\"\\bfutures?\\b\", r\"\\bperps?\\b\", r\"\\btrade|trading|trader\\b\",\n",
    "    r\"\\bposition\\b\", r\"\\blong\\b\", r\"\\bshort\\b\", r\"\\bentry\\b\", r\"\\bexit\\b\",\n",
    "    r\"\\bmarket\\b\", r\"\\border\\b\", r\"\\bliquidation\\b\", r\"\\bhedge\\b\", r\"\\bleverage\\b\"\n",
    "]\n",
    "KW_REGEX = re.compile(\"|\".join(KEYWORDS), flags=re.IGNORECASE)\n",
    "\n",
    "MAX_PER_SUBREDDIT = 5000\n",
    "REQUEST_SLEEP = 0.5\n",
    "SAVE_DIR = \"./data/reddit/yesterday\"\n",
    "\n",
    "# 昨日（UTC）窗口\n",
    "now_utc = dt.datetime.now(timezone.utc)\n",
    "day_end = dt.datetime(year=now_utc.year, month=now_utc.month, day=now_utc.day, tzinfo=timezone.utc)\n",
    "day_start = day_end - dt.timedelta(days=1)\n",
    "D = day_start.date()\n",
    "print(f\"[UTC window] {day_start.isoformat()} -> {day_end.isoformat()} (D={D})\")\n",
    "\n",
    "# Reddit 客户端（只读）\n",
    "reddit = praw.Reddit(\n",
    "    client_id=CLIENT_ID,\n",
    "    client_secret=CLIENT_SECRET,\n",
    "    user_agent=USER_AGENT,\n",
    "    ratelimit_seconds=5,\n",
    ")\n",
    "reddit.read_only = True\n",
    "\n",
    "def _to_utc(ts: float) -> dt.datetime:\n",
    "    return dt.datetime.fromtimestamp(ts, tz=timezone.utc)\n",
    "\n",
    "def _match_eth_trading(title: str, selftext: str) -> bool:\n",
    "    return bool(KW_REGEX.search(f\"{title or ''}\\n{selftext or ''}\"))\n",
    "\n",
    "# 采集\n",
    "rows = []\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "for sub in SUBREDDITS:\n",
    "    print(f\"\\n[collect] r/{sub} ...\")\n",
    "    count = 0\n",
    "    try:\n",
    "        for submission in reddit.subreddit(sub).new(limit=None):\n",
    "            created = _to_utc(getattr(submission, \"created_utc\", 0.0))\n",
    "            if created >= day_end:\n",
    "                continue\n",
    "            if created < day_start:\n",
    "                break\n",
    "\n",
    "            if not _match_eth_trading(submission.title, submission.selftext):\n",
    "                continue\n",
    "\n",
    "            rows.append(dict(\n",
    "                id=submission.id,\n",
    "                subreddit=submission.subreddit.display_name,\n",
    "                created_utc=created.isoformat(),\n",
    "                title=submission.title or \"\",\n",
    "                selftext=submission.selftext or \"\",\n",
    "                url=submission.url or \"\",\n",
    "                is_self=bool(submission.is_self),\n",
    "                author=str(submission.author) if submission.author else None,\n",
    "                score=int(submission.score or 0),\n",
    "                upvote_ratio=float(submission.upvote_ratio or 0.0),\n",
    "                num_comments=int(submission.num_comments or 0),\n",
    "                over_18=bool(getattr(submission, \"over_18\", False)),\n",
    "                stickied=bool(getattr(submission, \"stickied\", False)),\n",
    "                crosspost_parent=getattr(submission, \"crosspost_parent\", None),\n",
    "                permalink=f\"https://www.reddit.com{submission.permalink}\" if getattr(submission, \"permalink\", None) else \"\",\n",
    "                fetched_at=dt.datetime.now(timezone.utc).isoformat(),\n",
    "                fetch_window_start=day_start.isoformat(),\n",
    "                fetch_window_end=day_end.isoformat(),\n",
    "                fetch_version=\"submissions_only_v1\"\n",
    "            ))\n",
    "            count += 1\n",
    "            if count % 200 == 0:\n",
    "                print(f\"  r/{sub}: matched {count} ...\")\n",
    "            if count >= MAX_PER_SUBREDDIT:\n",
    "                print(f\"  r/{sub}: hit MAX_PER_SUBREDDIT={MAX_PER_SUBREDDIT}, stop.\")\n",
    "                break\n",
    "            time.sleep(REQUEST_SLEEP)\n",
    "    except Exception as e:\n",
    "        print(f\"  [warn] subreddit {sub} failed: {e}\")\n",
    "\n",
    "# 去重并保存 CSV\n",
    "df = pd.DataFrame(rows)\n",
    "csv_path = os.path.join(SAVE_DIR, f\"reddit_eth_submissions_{D}.csv\")\n",
    "\n",
    "if df.empty:\n",
    "    print(\"\\n[summary] No matched submissions found. 可调整关键词/子版块后重试。\")\n",
    "else:\n",
    "    df.sort_values([\"id\", \"score\", \"fetched_at\"], ascending=[True, False, False], inplace=True)\n",
    "    df = df.drop_duplicates(subset=[\"id\"], keep=\"first\").reset_index(drop=True)\n",
    "    df[\"is_crosspost\"] = df[\"crosspost_parent\"].notna()\n",
    "    df.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    dt_min = pd.to_datetime(df[\"created_utc\"]).min()\n",
    "    dt_max = pd.to_datetime(df[\"created_utc\"]).max()\n",
    "    print(\"\\n[summary]\")\n",
    "    print(f\"  window (UTC): {day_start.isoformat()} → {day_end.isoformat()}\")\n",
    "    print(f\"  collected rows: {len(df)}  (after de-dup)\")\n",
    "    print(f\"  time coverage:  {dt_min} → {dt_max}\")\n",
    "    print(f\"  subreddits:     {', '.join(sorted(df['subreddit'].unique()))}\")\n",
    "    print(f\"  saved csv:      {csv_path}\")\n",
    "\n",
    "    # 预览前 10 条\n",
    "    preview = (\n",
    "        df[[\"created_utc\",\"subreddit\",\"score\",\"num_comments\",\"title\"]]\n",
    "        .sort_values([\"score\",\"num_comments\"], ascending=False)\n",
    "        .head(10)\n",
    "    )\n",
    "    print(\"\\n[top 10 by score/num_comments]\")\n",
    "    for _, r in preview.iterrows():\n",
    "        print(f\"- [{r['created_utc']}] r/{r['subreddit']} | score={r['score']} com={r['num_comments']} | {r['title'][:140]}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UTC window] 2025-09-24T00:00:00+00:00 -> 2025-09-25T00:00:00+00:00 (D=2025-09-24)\n",
      "\n",
      "[collect] r/ethereum ...\n",
      "\n",
      "[collect] r/ethfinance ...\n",
      "\n",
      "[collect] r/ethtrader ...\n",
      "\n",
      "[collect] r/CryptoCurrency ...\n",
      "\n",
      "[collect] r/defi ...\n",
      "\n",
      "[collect] r/ethdev ...\n",
      "\n",
      "[summary]\n",
      "  window (UTC): 2025-09-24T00:00:00+00:00 → 2025-09-25T00:00:00+00:00\n",
      "  collected rows: 34  (after de-dup)\n",
      "  time coverage:  2025-09-24 00:00:42+00:00 → 2025-09-24 23:23:18+00:00\n",
      "  subreddits:     CryptoCurrency, defi, ethdev, ethereum, ethtrader\n",
      "  saved csv:      ./data/reddit/yesterday\\reddit_eth_submissions_2025-09-24.csv\n",
      "\n",
      "[top 10 by score/num_comments]\n",
      "- [2025-09-24T11:50:30+00:00] r/CryptoCurrency | score=168 com=44 | Over 30,000 BTC ($3.39B at $113K) were moved to exchanges at a loss as short-term holders capitulate\n",
      "- [2025-09-24T05:00:40+00:00] r/ethereum | score=157 com=123 | Daily General Discussion September 24, 2025\n",
      "- [2025-09-24T01:52:58+00:00] r/CryptoCurrency | score=70 com=4 | Morgan Stanley To Start Trading Bitcoin And Crypto In 2026\n",
      "- [2025-09-24T04:37:08+00:00] r/ethtrader | score=59 com=7 | Ethereum Founder Vitalik Buterin Says L2 Base Is ‘Doing Things the Right Way’\n",
      "- [2025-09-24T08:12:42+00:00] r/defi | score=52 com=22 | What decent exchange doesn't require kyc for basic trades (no on-ramp or off-ramp needed)?\n",
      "- [2025-09-24T16:30:02+00:00] r/ethereum | score=43 com=1 | Why Ethereum Treasuries Could Be The Next Big Business Strategy\n",
      "- [2025-09-24T16:26:27+00:00] r/CryptoCurrency | score=41 com=13 | Why Ethereum Treasuries Could Be The Next Big Business Strategy\n",
      "- [2025-09-24T12:54:04+00:00] r/ethtrader | score=37 com=11 | Anonymous Crypto Exchanges with No KYC\n",
      "- [2025-09-24T16:29:37+00:00] r/ethtrader | score=31 com=2 | Why Ethereum Treasuries Could Be The Next Big Business Strategy\n",
      "- [2025-09-24T09:16:01+00:00] r/CryptoCurrency | score=23 com=5 | Australia Urges Immediate Action on Post-Quantum Cryptography as CRQC Threat Looms\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T08:04:12.912272Z",
     "start_time": "2025-09-25T08:04:12.873274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Normalize yesterday's grabbed CSV to your legacy KEEP_FIELDS schema ===\n",
    "# 输入:  data/reddit/yesterday/reddit_eth_submissions_YYYY-MM-DD.csv\n",
    "# 输出:  data/reddit/processed/reddit_eth_standard_YYYY-MM-DD.csv\n",
    "# 目的:  将新抓取的列映射/补全为旧逻辑使用的 17 个标准字段，方便后续沿用原有打分/聚合代码\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "# 你的旧 KEEP_FIELDS（来自 sentiment.ipynb）\n",
    "KEEP_FIELDS = [\n",
    "    \"id\", \"author\", \"subreddit\",\n",
    "    \"created_utc\", \"created\", \"created_time_utc\",\n",
    "    \"title\", \"selftext\", \"body\",\n",
    "    \"url\", \"permalink\",\n",
    "    \"score\", \"upvote_ratio\", \"num_comments\", \"num_crossposts\",\n",
    "    \"over_18\", \"is_self\"\n",
    "]\n",
    "\n",
    "# 昨天 UTC 的文件名（和抓取脚本一致）\n",
    "D = (dt.datetime.now(dt.timezone.utc) - dt.timedelta(days=1)).date()\n",
    "in_dir  = os.path.join(\"data\", \"reddit\", \"yesterday\")\n",
    "in_csv  = os.path.join(in_dir, f\"reddit_eth_submissions_{D}.csv\")\n",
    "\n",
    "out_dir = os.path.join(\"data\", \"reddit\", \"processed\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "out_csv = os.path.join(out_dir, f\"reddit_eth_standard_{D}.csv\")\n",
    "\n",
    "if not os.path.exists(in_csv):\n",
    "    raise FileNotFoundError(f\"找不到输入文件: {os.path.abspath(in_csv)}\")\n",
    "\n",
    "df = pd.read_csv(in_csv)\n",
    "\n",
    "# ---- 字段映射/补全（最小改动，尽量复用已有列） ----\n",
    "out = pd.DataFrame()\n",
    "\n",
    "# 直接映射的列（若不存在则给空）\n",
    "def pick(col, default=None):\n",
    "    return df[col] if col in df.columns else default\n",
    "\n",
    "out[\"id\"]         = pick(\"id\")\n",
    "out[\"author\"]     = pick(\"author\")\n",
    "out[\"subreddit\"]  = pick(\"subreddit\")\n",
    "out[\"created_utc\"]= pick(\"created_utc\")  # 我们抓取时就是 ISO 字符串\n",
    "\n",
    "# 由于你旧流程里既有 created_utc，也有 created/created_time_utc：\n",
    "# 这里为兼容：created = 去掉时区的简洁显示；created_time_utc = 同 created_utc\n",
    "ts = pd.to_datetime(out[\"created_utc\"], errors=\"coerce\", utc=True)\n",
    "out[\"created\"]          = ts.dt.tz_convert(None).dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "out[\"created_time_utc\"] = out[\"created_utc\"]\n",
    "\n",
    "out[\"title\"]    = pick(\"title\", \"\")\n",
    "out[\"selftext\"] = pick(\"selftext\", \"\")\n",
    "out[\"body\"]     = \"\"  # 旧 schema 给评论字段，这里抓的是帖子，留空\n",
    "\n",
    "out[\"url\"]       = pick(\"url\", \"\")\n",
    "out[\"permalink\"] = pick(\"permalink\", \"\")\n",
    "\n",
    "out[\"score\"]        = pick(\"score\", 0).fillna(0).astype(\"Int64\")\n",
    "out[\"upvote_ratio\"] = pick(\"upvote_ratio\", 0.0).fillna(0.0)\n",
    "out[\"num_comments\"] = pick(\"num_comments\", 0).fillna(0).astype(\"Int64\")\n",
    "\n",
    "# num_crossposts：抓取里没有显式列；用 crosspost_parent 是否存在来近似\n",
    "if \"num_crossposts\" in df.columns:\n",
    "    out[\"num_crossposts\"] = df[\"num_crossposts\"].fillna(0).astype(\"Int64\")\n",
    "else:\n",
    "    out[\"num_crossposts\"] = df.get(\"crosspost_parent\").notna().astype(int)\n",
    "\n",
    "out[\"over_18\"] = pick(\"over_18\", False).fillna(False).astype(bool)\n",
    "out[\"is_self\"] = pick(\"is_self\", False).fillna(False).astype(bool)\n",
    "\n",
    "# 只保留旧 KEEP_FIELDS 的顺序与列名\n",
    "out = out[KEEP_FIELDS]\n",
    "\n",
    "# 去重（按 id）\n",
    "out = out.sort_values([\"id\", \"score\"], ascending=[True, False]).drop_duplicates(\"id\").reset_index(drop=True)\n",
    "\n",
    "# 保存 CSV\n",
    "out.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# 摘要\n",
    "print(\"[normalize] 输入:\", os.path.abspath(in_csv))\n",
    "print(\"[normalize] 输出:\", os.path.abspath(out_csv))\n",
    "print(\"[normalize] 行数:\", len(out))\n",
    "print(\"[normalize] 列:\", list(out.columns))\n",
    "print(out[[\"created_utc\",\"subreddit\",\"score\",\"num_comments\",\"title\"]].head(5))\n"
   ],
   "id": "e8f0d7b507e6ed0b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[normalize] 输入: C:\\Users\\Jimmy\\Desktop\\760\\data\\reddit\\yesterday\\reddit_eth_submissions_2025-09-24.csv\n",
      "[normalize] 输出: C:\\Users\\Jimmy\\Desktop\\760\\data\\reddit\\processed\\reddit_eth_standard_2025-09-24.csv\n",
      "[normalize] 行数: 34\n",
      "[normalize] 列: ['id', 'author', 'subreddit', 'created_utc', 'created', 'created_time_utc', 'title', 'selftext', 'body', 'url', 'permalink', 'score', 'upvote_ratio', 'num_comments', 'num_crossposts', 'over_18', 'is_self']\n",
      "                 created_utc       subreddit  score  num_comments  \\\n",
      "0  2025-09-24T00:00:42+00:00       ethtrader      7            48   \n",
      "1  2025-09-24T00:00:46+00:00  CryptoCurrency     22           390   \n",
      "2  2025-09-24T01:52:58+00:00  CryptoCurrency     70             4   \n",
      "3  2025-09-24T03:10:06+00:00  CryptoCurrency      5             2   \n",
      "4  2025-09-24T04:37:08+00:00       ethtrader     59             7   \n",
      "\n",
      "                                               title  \n",
      "0  Daily General Discussion - September 24, 2025 ...  \n",
      "1  Daily Crypto Discussion - September 24, 2025 (...  \n",
      "2  Morgan Stanley To Start Trading Bitcoin And Cr...  \n",
      "3  Ethereum ETFs now account for 15% of spot mark...  \n",
      "4  Ethereum Founder Vitalik Buterin Says L2 Base ...  \n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T08:17:12.630001Z",
     "start_time": "2025-09-25T08:17:12.579514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Cell 1: 清洗 processed 文件 -> cleaned/reddit_eth_standard_{D}_clean.csv ===\n",
    "import os, re, glob\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from datetime import timezone\n",
    "\n",
    "# ---- 自动定位输入文件 ----\n",
    "D = (dt.datetime.now(timezone.utc) - dt.timedelta(days=1)).date()\n",
    "default_path = f\"data/reddit/processed/reddit_eth_standard_{D}.csv\"\n",
    "if os.path.exists(default_path):\n",
    "    INPUT_CSV = default_path\n",
    "else:\n",
    "    # 兜底：找 processed 目录下最新的 reddit_eth_standard_*.csv\n",
    "    cand = sorted(glob.glob(\"data/reddit/processed/reddit_eth_standard_*.csv\"))\n",
    "    if not cand:\n",
    "        raise FileNotFoundError(\"找不到 processed 文件：data/reddit/processed/reddit_eth_standard_*.csv\")\n",
    "    INPUT_CSV = cand[-1]\n",
    "    # 同步 D（从文件名里取日期）\n",
    "    try:\n",
    "        D = os.path.basename(INPUT_CSV).split(\"_\")[-1].split(\".\")[0]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "OUT_DIR   = \"cleaned\"\n",
    "MAX_LEN   = 20000\n",
    "MIN_LEN   = 5\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    s = str(text)\n",
    "    s = re.sub(r\"\\[([^\\]]+)\\]\\(([^)]+)\\)\", r\"\\1\", s)               # markdown link -> text\n",
    "    s = re.sub(r\"http\\S+|www\\.\\S+\", \" \", s, flags=re.IGNORECASE)   # URLs\n",
    "    s = re.sub(r\"<[^>]+>\", \" \", s)                                 # HTML tags\n",
    "    s = re.sub(r\"[^A-Za-z0-9\\s]\", \" \", s)                          # 只保留字母数字空格\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip().lower()\n",
    "    return s\n",
    "\n",
    "# 读取\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_CSV, low_memory=False)\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(INPUT_CSV, encoding=\"latin-1\", low_memory=False)\n",
    "\n",
    "# 统一文本列\n",
    "if \"title\" not in df.columns:\n",
    "    raise ValueError(\"CSV 缺少 'title' 列\")\n",
    "if \"selftext\" not in df.columns: df[\"selftext\"] = \"\"\n",
    "if \"body\" not in df.columns: df[\"body\"] = \"\"\n",
    "\n",
    "# 清理多余列\n",
    "df = df.drop(columns=[c for c in df.columns if str(c).startswith(\"Unnamed\")], errors=\"ignore\")\n",
    "df = df.dropna(subset=[\"title\"])\n",
    "\n",
    "# 拼接原始文本\n",
    "df[\"text_raw\"] = (df[\"title\"].astype(str).fillna(\"\") + \" \" + df[\"selftext\"].astype(str).fillna(\"\"))\n",
    "mask_short = df[\"text_raw\"].str.len().fillna(0) < 3\n",
    "df.loc[mask_short, \"text_raw\"] = df.loc[mask_short, \"text_raw\"] + \" \" + df.loc[mask_short, \"body\"].astype(str)\n",
    "\n",
    "# 截断 & 清洗\n",
    "df[\"text_raw\"] = df[\"text_raw\"].astype(str).str.slice(0, MAX_LEN)\n",
    "df[\"text_clean\"] = df[\"text_raw\"].map(clean_text)\n",
    "df = df[df[\"text_clean\"].str.len() >= MIN_LEN].copy()\n",
    "\n",
    "# 解析时间（兼容 ISO 或 数字时间戳）\n",
    "def parse_created_any(s):\n",
    "    ts = pd.to_datetime(s, errors=\"coerce\", utc=True)  # 先试 ISO\n",
    "    if ts.isna().mean() > 0.5:  # 多数 NaT，尝试数字\n",
    "        c = pd.to_numeric(s, errors=\"coerce\")\n",
    "        if c.notna().any():\n",
    "            unit = \"ms\" if (c.dropna().median() > 10**12) else \"s\"\n",
    "            ts = pd.to_datetime(c, unit=unit, errors=\"coerce\", utc=True)\n",
    "    return ts\n",
    "\n",
    "if \"created_time_utc\" not in df.columns:\n",
    "    if \"created_utc\" in df.columns:\n",
    "        df[\"created_time_utc\"] = parse_created_any(df[\"created_utc\"])\n",
    "    elif \"created\" in df.columns:\n",
    "        df[\"created_time_utc\"] = parse_created_any(df[\"created\"])\n",
    "    else:\n",
    "        df[\"created_time_utc\"] = pd.NaT\n",
    "\n",
    "# year_month\n",
    "if df[\"created_time_utc\"].notna().any():\n",
    "    df[\"year_month\"] = pd.to_datetime(df[\"created_time_utc\"]).dt.strftime(\"%Y-%m\")\n",
    "elif \"source_file\" in df.columns:\n",
    "    df[\"year_month\"] = df[\"source_file\"].str.extract(r'((20\\d{2})[-_](\\d{2}))')[0]\n",
    "else:\n",
    "    df[\"year_month\"] = None\n",
    "\n",
    "# 去重\n",
    "before = len(df)\n",
    "if \"id\" in df.columns:\n",
    "    df = df.sort_values([\"id\",\"score\"] if \"score\" in df.columns else [\"id\"]) \\\n",
    "           .drop_duplicates(\"id\")\n",
    "else:\n",
    "    keys = [k for k in [\"title\",\"created_time_utc\",\"subreddit\"] if k in df.columns]\n",
    "    df = df.drop_duplicates(subset=keys) if keys else df.drop_duplicates()\n",
    "after = len(df)\n",
    "\n",
    "# 保存\n",
    "base = os.path.splitext(os.path.basename(INPUT_CSV))[0]\n",
    "out_path = os.path.join(OUT_DIR, f\"{base}_clean.csv\")\n",
    "df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"✅ 清洗完成\")\n",
    "print(\" - 输入:\", os.path.abspath(INPUT_CSV))\n",
    "print(\" - 输出:\", os.path.abspath(out_path))\n",
    "print(f\" - 记录数: {after}（去重前 {before}）\")\n",
    "print(\" - 关键列存在：\", [c for c in [\"text_raw\",\"text_clean\",\"created_time_utc\",\"year_month\"] if c in df.columns])\n",
    "df.head(3)\n",
    "\n"
   ],
   "id": "6ce7df6601f2518",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 清洗完成\n",
      " - 输入: C:\\Users\\Jimmy\\Desktop\\760\\data\\reddit\\processed\\reddit_eth_standard_2025-09-24.csv\n",
      " - 输出: C:\\Users\\Jimmy\\Desktop\\760\\cleaned\\reddit_eth_standard_2025-09-24_clean.csv\n",
      " - 记录数: 34（去重前 34）\n",
      " - 关键列存在： ['text_raw', 'text_clean', 'created_time_utc', 'year_month']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "        id          author       subreddit                created_utc  \\\n",
       "0  1noxjbk   AutoModerator       ethtrader  2025-09-24T00:00:42+00:00   \n",
       "1  1noxje4   AutoModerator  CryptoCurrency  2025-09-24T00:00:46+00:00   \n",
       "2  1nozvws  LavishlyRitzyy  CryptoCurrency  2025-09-24T01:52:58+00:00   \n",
       "\n",
       "               created           created_time_utc  \\\n",
       "0  2025-09-24 00:00:42  2025-09-24T00:00:42+00:00   \n",
       "1  2025-09-24 00:00:46  2025-09-24T00:00:46+00:00   \n",
       "2  2025-09-24 01:52:58  2025-09-24T01:52:58+00:00   \n",
       "\n",
       "                                               title  \\\n",
       "0  Daily General Discussion - September 24, 2025 ...   \n",
       "1  Daily Crypto Discussion - September 24, 2025 (...   \n",
       "2  Morgan Stanley To Start Trading Bitcoin And Cr...   \n",
       "\n",
       "                                            selftext  body  \\\n",
       "0  Welcome to the Daily General Discussion thread...   NaN   \n",
       "1  **Welcome to the Daily Crypto Discussion threa...   NaN   \n",
       "2                                                NaN   NaN   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.reddit.com/r/ethtrader/comments/1n...   \n",
       "1  https://www.reddit.com/r/CryptoCurrency/commen...   \n",
       "2  https://blockchainreporter.net/morgan-stanley-...   \n",
       "\n",
       "                                           permalink  score  upvote_ratio  \\\n",
       "0  https://www.reddit.com/r/ethtrader/comments/1n...      7          0.89   \n",
       "1  https://www.reddit.com/r/CryptoCurrency/commen...     22          0.83   \n",
       "2  https://www.reddit.com/r/CryptoCurrency/commen...     70          0.94   \n",
       "\n",
       "   num_comments  num_crossposts  over_18  is_self  \\\n",
       "0            48               0    False     True   \n",
       "1           390               0    False     True   \n",
       "2             4               0    False    False   \n",
       "\n",
       "                                            text_raw  \\\n",
       "0  Daily General Discussion - September 24, 2025 ...   \n",
       "1  Daily Crypto Discussion - September 24, 2025 (...   \n",
       "2  Morgan Stanley To Start Trading Bitcoin And Cr...   \n",
       "\n",
       "                                          text_clean year_month  \n",
       "0  daily general discussion september 24 2025 utc...    2025-09  \n",
       "1  daily crypto discussion september 24 2025 gmt ...    2025-09  \n",
       "2  morgan stanley to start trading bitcoin and cr...    2025-09  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>created</th>\n",
       "      <th>created_time_utc</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>body</th>\n",
       "      <th>url</th>\n",
       "      <th>permalink</th>\n",
       "      <th>score</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>num_crossposts</th>\n",
       "      <th>over_18</th>\n",
       "      <th>is_self</th>\n",
       "      <th>text_raw</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>year_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1noxjbk</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>ethtrader</td>\n",
       "      <td>2025-09-24T00:00:42+00:00</td>\n",
       "      <td>2025-09-24 00:00:42</td>\n",
       "      <td>2025-09-24T00:00:42+00:00</td>\n",
       "      <td>Daily General Discussion - September 24, 2025 ...</td>\n",
       "      <td>Welcome to the Daily General Discussion thread...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reddit.com/r/ethtrader/comments/1n...</td>\n",
       "      <td>https://www.reddit.com/r/ethtrader/comments/1n...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.89</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Daily General Discussion - September 24, 2025 ...</td>\n",
       "      <td>daily general discussion september 24 2025 utc...</td>\n",
       "      <td>2025-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1noxje4</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>CryptoCurrency</td>\n",
       "      <td>2025-09-24T00:00:46+00:00</td>\n",
       "      <td>2025-09-24 00:00:46</td>\n",
       "      <td>2025-09-24T00:00:46+00:00</td>\n",
       "      <td>Daily Crypto Discussion - September 24, 2025 (...</td>\n",
       "      <td>**Welcome to the Daily Crypto Discussion threa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reddit.com/r/CryptoCurrency/commen...</td>\n",
       "      <td>https://www.reddit.com/r/CryptoCurrency/commen...</td>\n",
       "      <td>22</td>\n",
       "      <td>0.83</td>\n",
       "      <td>390</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Daily Crypto Discussion - September 24, 2025 (...</td>\n",
       "      <td>daily crypto discussion september 24 2025 gmt ...</td>\n",
       "      <td>2025-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1nozvws</td>\n",
       "      <td>LavishlyRitzyy</td>\n",
       "      <td>CryptoCurrency</td>\n",
       "      <td>2025-09-24T01:52:58+00:00</td>\n",
       "      <td>2025-09-24 01:52:58</td>\n",
       "      <td>2025-09-24T01:52:58+00:00</td>\n",
       "      <td>Morgan Stanley To Start Trading Bitcoin And Cr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://blockchainreporter.net/morgan-stanley-...</td>\n",
       "      <td>https://www.reddit.com/r/CryptoCurrency/commen...</td>\n",
       "      <td>70</td>\n",
       "      <td>0.94</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Morgan Stanley To Start Trading Bitcoin And Cr...</td>\n",
       "      <td>morgan stanley to start trading bitcoin and cr...</td>\n",
       "      <td>2025-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T08:17:26.844189Z",
     "start_time": "2025-09-25T08:17:25.642312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Cell 2: 对 cleaned 文件做 VADER 打分，并写入主表 posts_scores_{D}.csv ===\n",
    "import os, pandas as pd, datetime as dt\n",
    "from datetime import timezone\n",
    "\n",
    "# 输入（上一格输出）\n",
    "D = (dt.datetime.now(timezone.utc) - dt.timedelta(days=1)).date()\n",
    "clean_in = f\"cleaned/reddit_eth_standard_{D}_clean.csv\"\n",
    "if not os.path.exists(clean_in):\n",
    "    # 若你用的是“最新文件兜底”，名字可能不是昨天日期；找 cleaned 目录最新的 _clean.csv\n",
    "    import glob\n",
    "    cand = sorted(glob.glob(\"cleaned/*_clean.csv\"))\n",
    "    if not cand:\n",
    "        raise FileNotFoundError(\"找不到 cleaned 文件\")\n",
    "    clean_in = cand[-1]\n",
    "\n",
    "# 读取\n",
    "df = pd.read_csv(clean_in)\n",
    "\n",
    "# 选主表基列\n",
    "base_cols = [c for c in [\n",
    "    \"id\",\"subreddit\",\"created_time_utc\",\"title\",\"selftext\",\"body\",\n",
    "    \"text_clean\",\"score\",\"num_comments\",\"upvote_ratio\",\"permalink\",\"url\"\n",
    "] if c in df.columns]\n",
    "dfb = df[base_cols].copy()\n",
    "\n",
    "# VADER\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "try:\n",
    "    _ = nltk.data.find(\"sentiment/vader_lexicon.zip\")\n",
    "except LookupError:\n",
    "    nltk.download(\"vader_lexicon\")\n",
    "\n",
    "text_col = \"text_clean\" if \"text_clean\" in dfb.columns else None\n",
    "if text_col is None:\n",
    "    def _join(*xs): return \" \".join([str(x) for x in xs if pd.notna(x)])\n",
    "    dfb[\"__text_tmp__\"] = [_join(t, s, b) for t, s, b in zip(\n",
    "        dfb.get(\"title\",\"\"), dfb.get(\"selftext\",\"\"), dfb.get(\"body\",\"\")\n",
    "    )]\n",
    "    text_col = \"__text_tmp__\"\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "dfb[\"vader\"] = dfb[text_col].map(lambda t: sia.polarity_scores(str(t))[\"compound\"])\n",
    "\n",
    "# 主表保存（后续可继续在这个文件追加 s1…s5 列）\n",
    "os.makedirs(\"data/reddit/scored\", exist_ok=True)\n",
    "master_p = f\"data/reddit/scored/posts_scores_{D}.csv\"\n",
    "tmp_p = master_p + \".tmp\"\n",
    "dfb.to_csv(tmp_p, index=False, encoding=\"utf-8-sig\")\n",
    "os.replace(tmp_p, master_p)\n",
    "\n",
    "print(\"✅ VADER 完成 & 主表已保存：\", os.path.abspath(master_p))\n",
    "print(\" - 列：\", list(dfb.columns))\n",
    "print(\" - 行：\", len(dfb))\n",
    "dfb[[\"id\",\"vader\",\"title\"]].head(3)\n"
   ],
   "id": "b5699c9b1712f6d0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Jimmy\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ VADER 完成 & 主表已保存： C:\\Users\\Jimmy\\Desktop\\760\\data\\reddit\\scored\\posts_scores_2025-09-24.csv\n",
      " - 列： ['id', 'subreddit', 'created_time_utc', 'title', 'selftext', 'body', 'text_clean', 'score', 'num_comments', 'upvote_ratio', 'permalink', 'url', 'vader']\n",
      " - 行： 34\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "        id   vader                                              title\n",
       "0  1noxjbk  0.9509  Daily General Discussion - September 24, 2025 ...\n",
       "1  1noxje4  0.9801  Daily Crypto Discussion - September 24, 2025 (...\n",
       "2  1nozvws  0.0000  Morgan Stanley To Start Trading Bitcoin And Cr..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>vader</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1noxjbk</td>\n",
       "      <td>0.9509</td>\n",
       "      <td>Daily General Discussion - September 24, 2025 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1noxje4</td>\n",
       "      <td>0.9801</td>\n",
       "      <td>Daily Crypto Discussion - September 24, 2025 (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1nozvws</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Morgan Stanley To Start Trading Bitcoin And Cr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T08:24:42.002057Z",
     "start_time": "2025-09-25T08:22:58.691975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# Part 3: 本地 LM Studio 小模型打分 -> 在主表追加列 s1\n",
    "# =========================\n",
    "# 依赖：pip install openai pandas numpy tqdm nest_asyncio\n",
    "\n",
    "import os, re, glob, time, asyncio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from tqdm.auto import tqdm\n",
    "from openai import AsyncOpenAI\n",
    "import datetime as dt\n",
    "from datetime import timezone\n",
    "\n",
    "# -------- LM Studio 基本配置 --------\n",
    "LMSTUDIO_BASE_URL = \"http://127.0.0.1:1234/v1\"\n",
    "LMSTUDIO_API_KEY  = \"lm-studio\"\n",
    "MODEL_NAME        = \"meta-llama-3.1-8b-instruct\"   # LM Studio 右侧 Info 的 API identifier\n",
    "OUT_COL           = \"s1\"                           # 本模型输出列名（你的第一个小模型）\n",
    "\n",
    "# -------- 主表路径（含 vader 列）--------\n",
    "D = (dt.datetime.now(timezone.utc) - dt.timedelta(days=1)).date()\n",
    "INPUT_CSV  = f\"data/reddit/scored/posts_scores_{D}.csv\"  # 主表（上一步已写入 vader）\n",
    "OUTPUT_CSV = INPUT_CSV                                   # 就地写回（原子写）\n",
    "\n",
    "if not os.path.exists(INPUT_CSV):\n",
    "    # 兜底：找 scored 目录下最新的 posts_scores_*.csv\n",
    "    cands = sorted(glob.glob(\"data/reddit/scored/posts_scores_*.csv\"), key=os.path.getmtime)\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(\"找不到主表 posts_scores_{D}.csv，请先完成 VADER 步骤。\")\n",
    "    INPUT_CSV = OUTPUT_CSV = cands[-1]\n",
    "\n",
    "TEXT_COL   = \"text_clean\"    # 优先用清洗文本\n",
    "VADER_COL  = \"vader\"         # 可用于挑选“难例”再打分（可选）\n",
    "\n",
    "# -------- 提速与控制参数 --------\n",
    "MAX_TEXT_LEN = 256      # 截断避免上下文过长\n",
    "VADER_EDGE   = None     # 仅对 |vader|<阈值 的样本打分；设为 None 则全量打分\n",
    "CONCURRENCY  = 8        # 并发数\n",
    "RETRY        = 3\n",
    "\n",
    "# ——提示词（注意大括号需转义成双大括号）——\n",
    "PROMPT = (\n",
    "    \"Classify the sentiment as exactly one token from {{POS, NEU, NEG}}.\\n\"\n",
    "    \"Text:\\n{text}\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "# 仅允许 POS/NEU/NEG（llama.cpp grammar）\n",
    "GRAMMAR = r'root ::= \"POS\" | \"NEU\" | \"NEG\"'\n",
    "\n",
    "def map_label_to_score(label: str) -> float:\n",
    "    s = (label or \"\").strip().upper()\n",
    "    first = re.split(r\"\\s+\", s)[0] if s else \"\"\n",
    "    if first in {\"POS\",\"NEU\",\"NEG\"}:\n",
    "        return 1.0 if first==\"POS\" else (0.0 if first==\"NEU\" else -1.0)\n",
    "    if \"POS\" in s: return 1.0\n",
    "    if \"NEG\" in s: return -1.0\n",
    "    if \"NEU\" in s: return 0.0\n",
    "    return 0.0\n",
    "\n",
    "# -------- 读取主表 & 选择待打样本 --------\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_CSV, low_memory=False)\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(INPUT_CSV, encoding=\"latin-1\", low_memory=False)\n",
    "\n",
    "# 文本列兜底：若没有 text_clean，则用 title+selftext+body 合成\n",
    "if TEXT_COL not in df.columns:\n",
    "    def _join(*xs): return \" \".join([str(x) for x in xs if pd.notna(x)])\n",
    "    df[\"__text_tmp__\"] = [_join(t, s, b) for t, s, b in zip(\n",
    "        df.get(\"title\",\"\"), df.get(\"selftext\",\"\"), df.get(\"body\",\"\")\n",
    "    )]\n",
    "    TEXT_COL = \"__text_tmp__\"\n",
    "\n",
    "# 确保待写列存在（新列先置 NaN）\n",
    "if OUT_COL not in df.columns:\n",
    "    df[OUT_COL] = np.nan\n",
    "\n",
    "mask = df[OUT_COL].isna()\n",
    "if VADER_EDGE is not None and VADER_COL in df.columns:\n",
    "    mask &= df[VADER_COL].abs() < VADER_EDGE  # 仅给模糊样本补打分\n",
    "\n",
    "todo = df[mask].copy()\n",
    "if todo.empty:\n",
    "    print(f\"没有需要新打分的样本（{OUT_COL} 已存在或被阈值过滤）。文件：{INPUT_CSV}\")\n",
    "else:\n",
    "    # 文本去重以减少请求\n",
    "    todo[\"__txt\"] = todo[TEXT_COL].astype(str).str.slice(0, MAX_TEXT_LEN)\n",
    "    groups = todo.groupby(\"__txt\").indices\n",
    "    unique_texts = list(groups.keys())\n",
    "    print(f\"主表: {os.path.basename(INPUT_CSV)}\")\n",
    "    print(f\"待打唯一文本数: {len(unique_texts)}（原样本 {len(todo)} / 全量 {len(df)}）\")\n",
    "\n",
    "    aclient = AsyncOpenAI(base_url=LMSTUDIO_BASE_URL, api_key=LMSTUDIO_API_KEY)\n",
    "\n",
    "    async def classify_text(t: str) -> float:\n",
    "        msg = PROMPT.format(text=t)\n",
    "        for attempt in range(RETRY):\n",
    "            try:\n",
    "                resp = await aclient.chat.completions.create(\n",
    "                    model=MODEL_NAME,\n",
    "                    messages=[{\"role\":\"user\",\"content\": msg}],\n",
    "                    temperature=0,\n",
    "                    max_tokens=1,\n",
    "                    top_p=1,\n",
    "                    stop=[\"\\n\"],\n",
    "                    extra_body={\"grammar\": GRAMMAR}\n",
    "                )\n",
    "                return map_label_to_score(resp.choices[0].message.content)\n",
    "            except Exception:\n",
    "                if attempt == RETRY - 1:\n",
    "                    return np.nan\n",
    "                await asyncio.sleep(0.6 * (attempt + 1))\n",
    "\n",
    "    async def run_all(texts, concurrency=8):\n",
    "        sem = asyncio.Semaphore(concurrency)\n",
    "        results = {}\n",
    "\n",
    "        async def bound_task(t):\n",
    "            async with sem:\n",
    "                score = await classify_text(t)\n",
    "            return t, score\n",
    "\n",
    "        tasks = [asyncio.create_task(bound_task(t)) for t in texts]\n",
    "        with tqdm(total=len(tasks), desc=f\"{MODEL_NAME} → {OUT_COL}\", unit=\"req\") as pbar:\n",
    "            for fut in asyncio.as_completed(tasks):\n",
    "                t, sc = await fut\n",
    "                results[t] = sc\n",
    "                pbar.update(1)\n",
    "        return results\n",
    "\n",
    "    loop = asyncio.get_event_loop()\n",
    "    scores_map = loop.run_until_complete(run_all(unique_texts, CONCURRENCY))\n",
    "\n",
    "    # 回填 & 原子落盘\n",
    "    for t, idxs in groups.items():\n",
    "        df.loc[idxs, OUT_COL] = scores_map.get(t, np.nan)\n",
    "\n",
    "    tmp_out = OUTPUT_CSV + \".tmp\"\n",
    "    df.to_csv(tmp_out, index=False, encoding=\"utf-8-sig\")\n",
    "    os.replace(tmp_out, OUTPUT_CSV)\n",
    "\n",
    "    print(f\"✅ 已写回列 {OUT_COL} -> {OUTPUT_CSV}\")\n"
   ],
   "id": "1980a9e8c299e765",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主表: posts_scores_2025-09-24.csv\n",
      "待打唯一文本数: 32（原样本 34 / 全量 34）\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "meta-llama-3.1-8b-instruct → s1:   0%|          | 0/32 [00:00<?, ?req/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "91e207fa45904834a529de2bd4b88bc5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已写回列 s1 -> data/reddit/scored/posts_scores_2025-09-24.csv\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T08:37:47.881005Z",
     "start_time": "2025-09-25T08:37:37.251909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === s2: Gemma-2-9B 在主表补齐列 s2（更稳健，带失败重试与兜底）===\n",
    "import os, re, glob, asyncio, numpy as np, pandas as pd\n",
    "import nest_asyncio; nest_asyncio.apply()\n",
    "from tqdm.auto import tqdm\n",
    "from openai import AsyncOpenAI\n",
    "import datetime as dt\n",
    "from datetime import timezone\n",
    "\n",
    "LMSTUDIO_BASE_URL = \"http://127.0.0.1:1234/v1\"\n",
    "LMSTUDIO_API_KEY  = \"lm-studio\"\n",
    "MODEL_NAME        = \"google/gemma-2-9b\"\n",
    "OUT_COL           = \"s2\"\n",
    "\n",
    "# ——配置：全量打分、降低并发、加大重试——\n",
    "VADER_EDGE = None          # 不过滤\n",
    "DEDUP_UNIQUE_TEXTS = True  # 文本去重（失败会影响同文本多行；更快）\n",
    "MAX_TEXT_LEN = 256\n",
    "CONCURRENCY = 4            # ↓更稳（按需调高）\n",
    "RETRY = 5                  # ↑更稳\n",
    "FALLBACK_TO_VADER_SIGN = True  # 最终仍失败时，用 vader 的符号兜底\n",
    "\n",
    "# ——路径——\n",
    "D = (dt.datetime.now(timezone.utc) - dt.timedelta(days=1)).date()\n",
    "INPUT_CSV = f\"data/reddit/scored/posts_scores_{D}.csv\"\n",
    "if not os.path.exists(INPUT_CSV):\n",
    "    cands = sorted(glob.glob(\"data/reddit/scored/posts_scores_*.csv\"), key=os.path.getmtime)\n",
    "    if not cands: raise FileNotFoundError(\"找不到主表 posts_scores_*.csv\")\n",
    "    INPUT_CSV = cands[-1]\n",
    "OUTPUT_CSV = INPUT_CSV\n",
    "\n",
    "df = pd.read_csv(INPUT_CSV, low_memory=False)\n",
    "\n",
    "# 文本列兜底\n",
    "TEXT_COL = \"text_clean\"\n",
    "if TEXT_COL not in df.columns:\n",
    "    def _join(*xs): return \" \".join([str(x) for x in xs if pd.notna(x)])\n",
    "    df[\"__text_tmp__\"] = [_join(t, s, b) for t, s, b in zip(\n",
    "        df.get(\"title\",\"\"), df.get(\"selftext\",\"\"), df.get(\"body\",\"\")\n",
    "    )]\n",
    "    TEXT_COL = \"__text_tmp__\"\n",
    "\n",
    "# 准备要打分的行\n",
    "if OUT_COL not in df.columns: df[OUT_COL] = np.nan\n",
    "mask = df[OUT_COL].isna()\n",
    "if (VADER_EDGE is not None) and (\"vader\" in df.columns):\n",
    "    mask &= df[\"vader\"].abs() < VADER_EDGE\n",
    "\n",
    "todo = df.loc[mask].copy()\n",
    "if todo.empty:\n",
    "    print(f\"没有需要打分的样本（{OUT_COL} 已存在或被阈值过滤）。文件：{INPUT_CSV}\")\n",
    "else:\n",
    "    todo[\"__txt\"] = todo[TEXT_COL].astype(str).str.slice(0, MAX_TEXT_LEN)\n",
    "\n",
    "    if DEDUP_UNIQUE_TEXTS:\n",
    "        groups = todo.groupby(\"__txt\").indices\n",
    "        work_items = list(groups.keys())  # 唯一文本\n",
    "        def write_back(scores_map):\n",
    "            for t, idxs in groups.items():\n",
    "                df.loc[idxs, OUT_COL] = scores_map.get(t, np.nan)\n",
    "    else:\n",
    "        work_items = list(todo.index)     # 行索引\n",
    "        def write_back(scores_map):\n",
    "            for idx, sc in scores_map.items():\n",
    "                df.loc[idx, OUT_COL] = sc\n",
    "\n",
    "    print(f\"主表: {os.path.basename(INPUT_CSV)} | 待打: {len(work_items)}（去重={DEDUP_UNIQUE_TEXTS}）\")\n",
    "\n",
    "    PROMPT = (\n",
    "        \"Classify the sentiment as exactly one token from {{POS, NEU, NEG}}.\\n\"\n",
    "        \"Text:\\n{text}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    GRAMMAR = r'root ::= \"POS\" | \"NEU\" | \"NEG\"'\n",
    "\n",
    "    def map_label_to_score(label: str) -> float:\n",
    "        s = (label or \"\").strip().upper()\n",
    "        first = re.split(r\"\\s+\", s)[0] if s else \"\"\n",
    "        if first in {\"POS\",\"NEU\",\"NEG\"}:\n",
    "            return 1.0 if first==\"POS\" else (0.0 if first==\"NEU\" else -1.0)\n",
    "        if \"POS\" in s: return 1.0\n",
    "        if \"NEG\" in s: return -1.0\n",
    "        if \"NEU\" in s: return 0.0\n",
    "        return 0.0\n",
    "\n",
    "    aclient = AsyncOpenAI(base_url=LMSTUDIO_BASE_URL, api_key=LMSTUDIO_API_KEY)\n",
    "\n",
    "    async def ask(text) -> float:\n",
    "        msg = PROMPT.format(text=text)\n",
    "        for attempt in range(RETRY):\n",
    "            try:\n",
    "                r = await aclient.chat.completions.create(\n",
    "                    model=MODEL_NAME,\n",
    "                    messages=[{\"role\":\"user\",\"content\": msg}],\n",
    "                    temperature=0,\n",
    "                    max_tokens=1,\n",
    "                    top_p=1,\n",
    "                    stop=[\"\\n\"],\n",
    "                    extra_body={\"grammar\": GRAMMAR}\n",
    "                )\n",
    "                return map_label_to_score(r.choices[0].message.content)\n",
    "            except Exception:\n",
    "                # 按指数退避\n",
    "                await asyncio.sleep(0.7 * (attempt + 1))\n",
    "                if attempt == RETRY - 1:\n",
    "                    return np.nan\n",
    "\n",
    "    async def run_batch(items, desc):\n",
    "        sem = asyncio.Semaphore(CONCURRENCY)\n",
    "        results = {}\n",
    "        async def one(item):\n",
    "            text = item if DEDUP_UNIQUE_TEXTS else todo.at[item, \"__txt\"]\n",
    "            async with sem:\n",
    "                sc = await ask(text)\n",
    "            return item, sc\n",
    "        tasks = [asyncio.create_task(one(x)) for x in items]\n",
    "        with tqdm(total=len(tasks), desc=desc, unit=\"req\") as pbar:\n",
    "            for fut in asyncio.as_completed(tasks):\n",
    "                k, sc = await fut\n",
    "                results[k] = sc\n",
    "                pbar.update(1)\n",
    "        return results\n",
    "\n",
    "    loop = asyncio.get_event_loop()\n",
    "    scores = loop.run_until_complete(run_batch(work_items, f\"{MODEL_NAME} → {OUT_COL}\"))\n",
    "\n",
    "    # 二次只重打失败的（NaN 的）项\n",
    "    failed = [k for k, v in scores.items() if (v is None) or (isinstance(v, float) and np.isnan(v))]\n",
    "    if failed:\n",
    "        print(f\"⚠️ 首轮失败 {len(failed)} 条，降并发重试…\")\n",
    "        CONCURRENCY = max(2, CONCURRENCY // 2)\n",
    "        retry_scores = loop.run_until_complete(run_batch(failed, f\"retry {MODEL_NAME} → {OUT_COL}\"))\n",
    "        scores.update(retry_scores)\n",
    "\n",
    "    # 写回\n",
    "    write_back(scores)\n",
    "\n",
    "    # 最终兜底：仍 NaN 的用 vader 符号或 0\n",
    "    still_nan = df[OUT_COL].isna().sum()\n",
    "    if still_nan and FALLBACK_TO_VADER_SIGN:\n",
    "        print(f\"⚠️ 仍有 {still_nan} 条 NaN，用 vader 符号兜底。\")\n",
    "        sign = np.sign(df.get(\"vader\", 0.0).fillna(0.0))\n",
    "        df.loc[df[OUT_COL].isna(), OUT_COL] = sign.replace(0, 0.0)\n",
    "\n",
    "    tmp = OUTPUT_CSV + \".tmp\"\n",
    "    df.to_csv(tmp, index=False, encoding=\"utf-8-sig\")\n",
    "    os.replace(tmp, OUTPUT_CSV)\n",
    "    print(f\"✅ 已写回列 {OUT_COL} -> {OUTPUT_CSV}\")\n"
   ],
   "id": "d6c7e83f662467d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主表: posts_scores_2025-09-24.csv | 待打: 32（去重=True）\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "google/gemma-2-9b → s2:   0%|          | 0/32 [00:00<?, ?req/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6d6fbc76890d41258864d6ec6c44b8d5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已写回列 s2 -> data/reddit/scored/posts_scores_2025-09-24.csv\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T08:59:47.280935Z",
     "start_time": "2025-09-25T08:59:42.249328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# s3：相对路径读取/写回 data/reddit/scored/posts_scores_{D}.csv（全量打分）\n",
    "# =========================\n",
    "# pip install openai pandas numpy tqdm nest_asyncio\n",
    "\n",
    "import os, re, glob, asyncio, numpy as np, pandas as pd\n",
    "import nest_asyncio; nest_asyncio.apply()\n",
    "from tqdm.auto import tqdm\n",
    "from openai import AsyncOpenAI\n",
    "import datetime as dt\n",
    "\n",
    "# ---- 路径（相对项目根目录）----\n",
    "D = (dt.datetime.utcnow().date() - dt.timedelta(days=1))\n",
    "MASTER_DIR = \"data/reddit/scored\"\n",
    "PREF = os.path.join(MASTER_DIR, f\"posts_scores_{D}.csv\")\n",
    "\n",
    "if os.path.exists(PREF):\n",
    "    MASTER_PATH = PREF\n",
    "else:\n",
    "    cands = sorted(glob.glob(os.path.join(MASTER_DIR, \"posts_scores_*.csv\")), key=os.path.getmtime)\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(\"未找到主表：data/reddit/scored/posts_scores_*.csv，请先跑 VADER 初始化主表。\")\n",
    "    MASTER_PATH = cands[-1]\n",
    "\n",
    "# ---- LM Studio 配置 ----\n",
    "LMSTUDIO_BASE_URL = \"http://127.0.0.1:1234/v1\"\n",
    "LMSTUDIO_API_KEY  = \"lm-studio\"\n",
    "MODEL_NAME        = \"qwen2.5-7b-instruct-1m\"   # ← 改成 LM Studio 右侧的 API identifier\n",
    "OUT_COL           = \"s3\"\n",
    "\n",
    "# ---- 打分参数（全量）----\n",
    "TEXT_COL     = \"text_clean\"\n",
    "MAX_TEXT_LEN = 256\n",
    "CONCURRENCY  = 8\n",
    "RETRY        = 3\n",
    "\n",
    "PROMPT = (\n",
    "    \"Classify the sentiment as exactly one token from {{POS, NEU, NEG}}.\\n\"\n",
    "    \"Text:\\n{text}\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "GRAMMAR = r'root ::= \"POS\" | \"NEU\" | \"NEG\"'\n",
    "\n",
    "def map_label_to_score(label: str) -> float:\n",
    "    s = (label or \"\").strip().upper()\n",
    "    first = re.split(r\"\\s+\", s)[0] if s else \"\"\n",
    "    if first in {\"POS\",\"NEU\",\"NEG\"}:\n",
    "        return 1.0 if first==\"POS\" else (0.0 if first==\"NEU\" else -1.0)\n",
    "    if \"POS\" in s: return 1.0\n",
    "    if \"NEG\" in s: return -1.0\n",
    "    if \"NEU\" in s: return 0.0\n",
    "    return 0.0\n",
    "\n",
    "# ---- 读取主表 & 选择待打样本（全量，补缺）----\n",
    "df = pd.read_csv(MASTER_PATH, low_memory=False)\n",
    "\n",
    "if TEXT_COL not in df.columns:\n",
    "    def _join(*xs): return \" \".join([str(x) for x in xs if pd.notna(x)])\n",
    "    df[\"__text_tmp__\"] = [_join(t, s, b) for t, s, b in zip(\n",
    "        df.get(\"title\",\"\"), df.get(\"selftext\",\"\"), df.get(\"body\",\"\")\n",
    "    )]\n",
    "    TEXT_COL = \"__text_tmp__\"\n",
    "\n",
    "if OUT_COL not in df.columns:\n",
    "    df[OUT_COL] = np.nan\n",
    "\n",
    "mask = df[OUT_COL].isna()       # 只补缺\n",
    "todo = df.loc[mask].copy()\n",
    "\n",
    "if todo.empty:\n",
    "    print(f\"没有需要打分的样本（{OUT_COL} 已存在且无缺失）。文件：{MASTER_PATH}\")\n",
    "else:\n",
    "    todo[\"__txt\"] = todo[TEXT_COL].astype(str).str.slice(0, MAX_TEXT_LEN)\n",
    "    groups = todo.groupby(\"__txt\").indices   # 文本去重提速\n",
    "    unique_texts = list(groups.keys())\n",
    "    print(f\"目标文件: {MASTER_PATH}\")\n",
    "    print(f\"待打唯一文本数: {len(unique_texts)}（原样本 {len(todo)} / 全量 {len(df)}）\")\n",
    "\n",
    "    aclient = AsyncOpenAI(base_url=LMSTUDIO_BASE_URL, api_key=LMSTUDIO_API_KEY)\n",
    "\n",
    "    async def classify_text(t: str) -> float:\n",
    "        msg = PROMPT.format(text=t)\n",
    "        for attempt in range(RETRY):\n",
    "            try:\n",
    "                r = await aclient.chat.completions.create(\n",
    "                    model=MODEL_NAME,\n",
    "                    messages=[{\"role\":\"user\",\"content\": msg}],\n",
    "                    temperature=0,\n",
    "                    max_tokens=1,\n",
    "                    top_p=1,\n",
    "                    stop=[\"\\n\"],\n",
    "                    extra_body={\"grammar\": GRAMMAR}\n",
    "                )\n",
    "                return map_label_to_score(r.choices[0].message.content)\n",
    "            except Exception:\n",
    "                if attempt == RETRY - 1:\n",
    "                    return np.nan\n",
    "                await asyncio.sleep(0.6 * (attempt + 1))\n",
    "\n",
    "    async def run_all(texts, concurrency=8):\n",
    "        sem = asyncio.Semaphore(concurrency)\n",
    "        results = {}\n",
    "        async def bound_task(t):\n",
    "            async with sem:\n",
    "                sc = await classify_text(t)\n",
    "            return t, sc\n",
    "        tasks = [asyncio.create_task(bound_task(t)) for t in texts]\n",
    "        with tqdm(total=len(tasks), desc=f\"{MODEL_NAME} → {OUT_COL}\", unit=\"req\") as pbar:\n",
    "            for fut in asyncio.as_completed(tasks):\n",
    "                t, sc = await fut\n",
    "                results[t] = sc\n",
    "                pbar.update(1)\n",
    "        return results\n",
    "\n",
    "    loop = asyncio.get_event_loop()\n",
    "    scores_map = loop.run_until_complete(run_all(unique_texts, CONCURRENCY))\n",
    "\n",
    "    # 回填 & 原子落盘（同一文件）\n",
    "    for t, idxs in groups.items():\n",
    "        df.loc[idxs, OUT_COL] = scores_map.get(t, np.nan)\n",
    "\n",
    "    tmp_out = MASTER_PATH + \".tmp\"\n",
    "    df.to_csv(tmp_out, index=False, encoding=\"utf-8-sig\")\n",
    "    os.replace(tmp_out, MASTER_PATH)\n",
    "\n",
    "    print(f\"✅ 已写回列 {OUT_COL} -> {MASTER_PATH}\")\n"
   ],
   "id": "9e9cea083fece5ec",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jimmy\\AppData\\Local\\Temp\\ipykernel_15604\\3966319938.py:13: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  D = (dt.datetime.utcnow().date() - dt.timedelta(days=1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目标文件: data/reddit/scored\\posts_scores_2025-09-24.csv\n",
      "待打唯一文本数: 32（原样本 34 / 全量 34）\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "qwen2.5-7b-instruct-1m → s3:   0%|          | 0/32 [00:00<?, ?req/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2adbfa854d424c9c934f54350767c6e6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已写回列 s3 -> data/reddit/scored\\posts_scores_2025-09-24.csv\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T09:02:26.701386Z",
     "start_time": "2025-09-25T09:02:21.874613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# s4：LM Studio 本地小模型打分（全量补缺）→ 追加列 s4\n",
    "# =========================\n",
    "# 依赖：pip install openai pandas numpy tqdm nest_asyncio\n",
    "\n",
    "import os, re, glob, asyncio, numpy as np, pandas as pd\n",
    "import nest_asyncio; nest_asyncio.apply()\n",
    "from tqdm.auto import tqdm\n",
    "from openai import AsyncOpenAI\n",
    "import datetime as dt\n",
    "\n",
    "# ---- 路径（相对项目根）----\n",
    "D = (dt.datetime.utcnow().date() - dt.timedelta(days=1))\n",
    "MASTER_DIR  = \"data/reddit/scored\"\n",
    "PREF        = os.path.join(MASTER_DIR, f\"posts_scores_{D}.csv\")\n",
    "MASTER_PATH = PREF if os.path.exists(PREF) else sorted(\n",
    "    glob.glob(os.path.join(MASTER_DIR, \"posts_scores_*.csv\")),\n",
    "    key=os.path.getmtime\n",
    ")[-1]\n",
    "\n",
    "# ---- LM Studio 配置（改这里的 MODEL_NAME）----\n",
    "LMSTUDIO_BASE_URL = \"http://127.0.0.1:1234/v1\"\n",
    "LMSTUDIO_API_KEY  = \"lm-studio\"\n",
    "MODEL_NAME        = \"mistralai/mistral-7b-instruct-v0.3\"   # ← 改成 LM Studio 右侧 API identifier\n",
    "OUT_COL           = \"s4\"\n",
    "\n",
    "# ---- 打分参数 ----\n",
    "TEXT_COL     = \"text_clean\"\n",
    "MAX_TEXT_LEN = 256\n",
    "CONCURRENCY  = 8\n",
    "RETRY        = 3\n",
    "DEDUP_UNIQUE_TEXTS = True   # True：同文只打一次（快）；False：逐行必打\n",
    "\n",
    "PROMPT = (\n",
    "    \"Classify the sentiment as exactly one token from {{POS, NEU, NEG}}.\\n\"\n",
    "    \"Text:\\n{text}\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "GRAMMAR = r'root ::= \"POS\" | \"NEU\" | \"NEG\"'\n",
    "\n",
    "def map_label_to_score(label: str) -> float:\n",
    "    s = (label or \"\").strip().upper()\n",
    "    first = re.split(r\"\\s+\", s)[0] if s else \"\"\n",
    "    if first in {\"POS\",\"NEU\",\"NEG\"}:\n",
    "        return 1.0 if first==\"POS\" else (0.0 if first==\"NEU\" else -1.0)\n",
    "    if \"POS\" in s: return 1.0\n",
    "    if \"NEG\" in s: return -1.0\n",
    "    if \"NEU\" in s: return 0.0\n",
    "    return 0.0\n",
    "\n",
    "# ---- 读取主表 & 选取待打样本（全量补缺）----\n",
    "df = pd.read_csv(MASTER_PATH, low_memory=False)\n",
    "\n",
    "if TEXT_COL not in df.columns:\n",
    "    def _join(*xs): return \" \".join([str(x) for x in xs if pd.notna(x)])\n",
    "    df[\"__text_tmp__\"] = [_join(t, s, b) for t, s, b in zip(\n",
    "        df.get(\"title\",\"\"), df.get(\"selftext\",\"\"), df.get(\"body\",\"\")\n",
    "    )]\n",
    "    TEXT_COL = \"__text_tmp__\"\n",
    "\n",
    "if OUT_COL not in df.columns:\n",
    "    df[OUT_COL] = np.nan\n",
    "\n",
    "todo = df.loc[df[OUT_COL].isna()].copy()\n",
    "if todo.empty:\n",
    "    print(f\"没有需要打分的样本（{OUT_COL} 已存在且无缺失）。文件：{MASTER_PATH}\")\n",
    "else:\n",
    "    if DEDUP_UNIQUE_TEXTS:\n",
    "        todo[\"__txt\"] = todo[TEXT_COL].astype(str).str.slice(0, MAX_TEXT_LEN)\n",
    "        groups = todo.groupby(\"__txt\").indices\n",
    "        work_items = list(groups.keys())          # 唯一文本集合\n",
    "        def write_back(scores_map):\n",
    "            for t, idxs in groups.items():\n",
    "                df.loc[idxs, OUT_COL] = scores_map.get(t, np.nan)\n",
    "    else:\n",
    "        todo[\"__txt\"] = todo[TEXT_COL].astype(str).str.slice(0, MAX_TEXT_LEN)\n",
    "        work_items = list(todo.index)             # 行索引集合\n",
    "        def write_back(scores_map):\n",
    "            for idx, sc in scores_map.items():\n",
    "                df.loc[idx, OUT_COL] = sc\n",
    "\n",
    "    print(f\"目标文件: {MASTER_PATH}\")\n",
    "    print(f\"准备打分: {len(work_items)}（去重={DEDUP_UNIQUE_TEXTS}）\")\n",
    "\n",
    "    aclient = AsyncOpenAI(base_url=LMSTUDIO_BASE_URL, api_key=LMSTUDIO_API_KEY)\n",
    "\n",
    "    async def classify_text(text: str) -> float:\n",
    "        msg = PROMPT.format(text=text)\n",
    "        for attempt in range(RETRY):\n",
    "            try:\n",
    "                r = await aclient.chat.completions.create(\n",
    "                    model=MODEL_NAME,\n",
    "                    messages=[{\"role\":\"user\",\"content\": msg}],\n",
    "                    temperature=0,\n",
    "                    max_tokens=1,\n",
    "                    top_p=1,\n",
    "                    stop=[\"\\n\"],\n",
    "                    extra_body={\"grammar\": GRAMMAR}\n",
    "                )\n",
    "                return map_label_to_score(r.choices[0].message.content)\n",
    "            except Exception:\n",
    "                if attempt == RETRY - 1:\n",
    "                    return np.nan\n",
    "                await asyncio.sleep(0.6 * (attempt + 1))\n",
    "\n",
    "    async def run_all(items):\n",
    "        sem = asyncio.Semaphore(CONCURRENCY)\n",
    "        results = {}\n",
    "        async def run_one(item):\n",
    "            text = item if DEDUP_UNIQUE_TEXTS else todo.at[item, \"__txt\"]\n",
    "            async with sem:\n",
    "                sc = await classify_text(text)\n",
    "            return item, sc\n",
    "        tasks = [asyncio.create_task(run_one(x)) for x in items]\n",
    "        with tqdm(total=len(tasks), desc=f\"{MODEL_NAME} → {OUT_COL}\", unit=\"req\") as pbar:\n",
    "            for fut in asyncio.as_completed(tasks):\n",
    "                k, sc = await fut\n",
    "                results[k] = sc\n",
    "                pbar.update(1)\n",
    "        return results\n",
    "\n",
    "    loop = asyncio.get_event_loop()\n",
    "    scores_map = loop.run_until_complete(run_all(work_items))\n",
    "\n",
    "    # 回填 & 原子落盘\n",
    "    write_back(scores_map)\n",
    "    tmp_out = MASTER_PATH + \".tmp\"\n",
    "    df.to_csv(tmp_out, index=False, encoding=\"utf-8-sig\")\n",
    "    os.replace(tmp_out, MASTER_PATH)\n",
    "\n",
    "    print(f\"✅ 已写回列 {OUT_COL} -> {MASTER_PATH}\")\n"
   ],
   "id": "c3f6f9da352e532d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jimmy\\AppData\\Local\\Temp\\ipykernel_15604\\1081153019.py:13: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  D = (dt.datetime.utcnow().date() - dt.timedelta(days=1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目标文件: data/reddit/scored\\posts_scores_2025-09-24.csv\n",
      "准备打分: 32（去重=True）\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mistralai/mistral-7b-instruct-v0.3 → s4:   0%|          | 0/32 [00:00<?, ?req/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5bbf4b2c833d49a9a6908c834142d8ce"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已写回列 s4 -> data/reddit/scored\\posts_scores_2025-09-24.csv\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T09:05:46.795182Z",
     "start_time": "2025-09-25T09:05:42.126908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# s5：LM Studio 本地小模型打分（全量补缺）→ 追加列 s5\n",
    "# =========================\n",
    "# 依赖：pip install openai pandas numpy tqdm nest_asyncio\n",
    "\n",
    "import os, re, glob, asyncio, numpy as np, pandas as pd\n",
    "import nest_asyncio; nest_asyncio.apply()\n",
    "from tqdm.auto import tqdm\n",
    "from openai import AsyncOpenAI\n",
    "import datetime as dt\n",
    "\n",
    "# ---- 路径（相对项目根）----\n",
    "D = (dt.datetime.utcnow().date() - dt.timedelta(days=1))\n",
    "MASTER_DIR  = \"data/reddit/scored\"\n",
    "PREF        = os.path.join(MASTER_DIR, f\"posts_scores_{D}.csv\")\n",
    "MASTER_PATH = PREF if os.path.exists(PREF) else sorted(\n",
    "    glob.glob(os.path.join(MASTER_DIR, \"posts_scores_*.csv\")),\n",
    "    key=os.path.getmtime\n",
    ")[-1]\n",
    "\n",
    "# ---- LM Studio 配置（改这里的 MODEL_NAME）----\n",
    "LMSTUDIO_BASE_URL = \"http://127.0.0.1:1234/v1\"\n",
    "LMSTUDIO_API_KEY  = \"lm-studio\"\n",
    "MODEL_NAME        = \"nous-hermes-2-mistral-7b-dpo\"   # ← 改成 LM Studio 右侧 API identifier\n",
    "OUT_COL           = \"s5\"\n",
    "\n",
    "# ---- 打分参数 ----\n",
    "TEXT_COL     = \"text_clean\"\n",
    "MAX_TEXT_LEN = 256\n",
    "CONCURRENCY  = 8\n",
    "RETRY        = 3\n",
    "DEDUP_UNIQUE_TEXTS = True  # True: 同文只打一次；False: 逐行必打\n",
    "\n",
    "PROMPT = (\n",
    "    \"Classify the sentiment as exactly one token from {{POS, NEU, NEG}}.\\n\"\n",
    "    \"Text:\\n{text}\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "GRAMMAR = r'root ::= \"POS\" | \"NEU\" | \"NEG\"'\n",
    "\n",
    "def map_label_to_score(label: str) -> float:\n",
    "    s = (label or \"\").strip().upper()\n",
    "    first = re.split(r\"\\s+\", s)[0] if s else \"\"\n",
    "    if first in {\"POS\",\"NEU\",\"NEG\"}:\n",
    "        return 1.0 if first==\"POS\" else (0.0 if first==\"NEU\" else -1.0)\n",
    "    if \"POS\" in s: return 1.0\n",
    "    if \"NEG\" in s: return -1.0\n",
    "    if \"NEU\" in s: return 0.0\n",
    "    return 0.0\n",
    "\n",
    "# ---- 读取主表 & 选择待打样本（全量补缺）----\n",
    "df = pd.read_csv(MASTER_PATH, low_memory=False)\n",
    "\n",
    "if TEXT_COL not in df.columns:\n",
    "    def _join(*xs): return \" \".join([str(x) for x in xs if pd.notna(x)])\n",
    "    df[\"__text_tmp__\"] = [_join(t, s, b) for t, s, b in zip(\n",
    "        df.get(\"title\",\"\"), df.get(\"selftext\",\"\"), df.get(\"body\",\"\")\n",
    "    )]\n",
    "    TEXT_COL = \"__text_tmp__\"\n",
    "\n",
    "if OUT_COL not in df.columns:\n",
    "    df[OUT_COL] = np.nan\n",
    "\n",
    "todo = df.loc[df[OUT_COL].isna()].copy()\n",
    "if todo.empty:\n",
    "    print(f\"没有需要打分的样本（{OUT_COL} 已存在且无缺失）。文件：{MASTER_PATH}\")\n",
    "else:\n",
    "    if DEDUP_UNIQUE_TEXTS:\n",
    "        todo[\"__txt\"] = todo[TEXT_COL].astype(str).str.slice(0, MAX_TEXT_LEN)\n",
    "        groups = todo.groupby(\"__txt\").indices\n",
    "        work_items = list(groups.keys())\n",
    "        def write_back(scores_map):\n",
    "            for t, idxs in groups.items():\n",
    "                df.loc[idxs, OUT_COL] = scores_map.get(t, np.nan)\n",
    "    else:\n",
    "        todo[\"__txt\"] = todo[TEXT_COL].astype(str).str.slice(0, MAX_TEXT_LEN)\n",
    "        work_items = list(todo.index)\n",
    "        def write_back(scores_map):\n",
    "            for idx, sc in scores_map.items():\n",
    "                df.loc[idx, OUT_COL] = sc\n",
    "\n",
    "    print(f\"目标文件: {MASTER_PATH}\")\n",
    "    print(f\"准备打分: {len(work_items)}（去重={DEDUP_UNIQUE_TEXTS}）\")\n",
    "\n",
    "    aclient = AsyncOpenAI(base_url=LMSTUDIO_BASE_URL, api_key=LMSTUDIO_API_KEY)\n",
    "\n",
    "    async def classify_text(text: str) -> float:\n",
    "        msg = PROMPT.format(text=text)\n",
    "        for attempt in range(RETRY):\n",
    "            try:\n",
    "                r = await aclient.chat.completions.create(\n",
    "                    model=MODEL_NAME,\n",
    "                    messages=[{\"role\":\"user\",\"content\": msg}],\n",
    "                    temperature=0,\n",
    "                    max_tokens=1,\n",
    "                    top_p=1,\n",
    "                    stop=[\"\\n\"],\n",
    "                    extra_body={\"grammar\": GRAMMAR}\n",
    "                )\n",
    "                return map_label_to_score(r.choices[0].message.content)\n",
    "            except Exception:\n",
    "                if attempt == RETRY - 1:\n",
    "                    return np.nan\n",
    "                await asyncio.sleep(0.6 * (attempt + 1))\n",
    "\n",
    "    async def run_all(items):\n",
    "        sem = asyncio.Semaphore(CONCURRENCY)\n",
    "        results = {}\n",
    "        async def run_one(item):\n",
    "            text = item if DEDUP_UNIQUE_TEXTS else todo.at[item, \"__txt\"]\n",
    "            async with sem:\n",
    "                sc = await classify_text(text)\n",
    "            return item, sc\n",
    "        tasks = [asyncio.create_task(run_one(x)) for x in items]\n",
    "        with tqdm(total=len(tasks), desc=f\"{MODEL_NAME} → {OUT_COL}\", unit=\"req\") as pbar:\n",
    "            for fut in asyncio.as_completed(tasks):\n",
    "                k, sc = await fut\n",
    "                results[k] = sc\n",
    "                pbar.update(1)\n",
    "        return results\n",
    "\n",
    "    loop = asyncio.get_event_loop()\n",
    "    scores_map = loop.run_until_complete(run_all(work_items))\n",
    "\n",
    "    # 回填 & 原子落盘\n",
    "    write_back(scores_map)\n",
    "    tmp_out = MASTER_PATH + \".tmp\"\n",
    "    df.to_csv(tmp_out, index=False, encoding=\"utf-8-sig\")\n",
    "    os.replace(tmp_out, MASTER_PATH)\n",
    "\n",
    "    print(f\"✅ 已写回列 {OUT_COL} -> {MASTER_PATH}\")\n"
   ],
   "id": "963dca94bbe997dc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jimmy\\AppData\\Local\\Temp\\ipykernel_15604\\2155423926.py:13: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  D = (dt.datetime.utcnow().date() - dt.timedelta(days=1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目标文件: data/reddit/scored\\posts_scores_2025-09-24.csv\n",
      "准备打分: 32（去重=True）\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nous-hermes-2-mistral-7b-dpo → s5:   0%|          | 0/32 [00:00<?, ?req/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8b832b21f2144063860e82b3fa02be7b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已写回列 s5 -> data/reddit/scored\\posts_scores_2025-09-24.csv\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T09:13:30.188996Z",
     "start_time": "2025-09-25T09:13:30.114813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# Weighted daily -> 1min forward-fill (vader + s1..s5)\n",
    "# 从 data/reddit/scored 聚合到分钟级；每天定值，展开成 1min\n",
    "# =========================\n",
    "# pip install pandas numpy\n",
    "\n",
    "import os, glob, math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---- 路径 ----\n",
    "SCORED_DIR = \"data/reddit/scored\"\n",
    "OUT_DIR    = \"data/reddit/weighted\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ---- 读取所有 posts_scores_*.csv ----\n",
    "paths = sorted(glob.glob(os.path.join(SCORED_DIR, \"posts_scores_*.csv\")))\n",
    "if not paths:\n",
    "    raise FileNotFoundError(\"data/reddit/scored/ 下未找到 posts_scores_*.csv，请先生成主表。\")\n",
    "\n",
    "dfs = []\n",
    "for p in paths:\n",
    "    try:\n",
    "        dfp = pd.read_csv(p, low_memory=False)\n",
    "    except UnicodeDecodeError:\n",
    "        dfp = pd.read_csv(p, encoding=\"latin-1\", low_memory=False)\n",
    "    dfp[\"__source\"] = os.path.basename(p)\n",
    "    dfs.append(dfp)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# ---- 情绪列：兼容两种命名（s1..s5 或 sent_s1..sent_s5），统一成 s1..s5 ----\n",
    "sent_cols = []\n",
    "if \"vader\" in df.columns:\n",
    "    sent_cols.append(\"vader\")\n",
    "\n",
    "# 优先 s1..s5；若不存在则映射 sent_s1..sent_s5 -> s1..s5\n",
    "for k in [\"s1\",\"s2\",\"s3\",\"s4\",\"s5\"]:\n",
    "    if k in df.columns:\n",
    "        sent_cols.append(k)\n",
    "    elif f\"sent_{k}\" in df.columns:\n",
    "        df[k] = pd.to_numeric(df[f\"sent_{k}\"], errors=\"coerce\")\n",
    "        sent_cols.append(k)\n",
    "\n",
    "if not sent_cols:\n",
    "    raise ValueError(\"没有找到情绪列（期望 vader + s1..s5 中至少一列）。\")\n",
    "\n",
    "# ---- 时间列（优先 created_time_utc，其次 created_utc 秒）----\n",
    "if \"created_time_utc\" in df.columns:\n",
    "    t = pd.to_datetime(df[\"created_time_utc\"], errors=\"coerce\", utc=True)\n",
    "elif \"created_utc\" in df.columns:\n",
    "    t = pd.to_datetime(df[\"created_utc\"], unit=\"s\", errors=\"coerce\", utc=True)\n",
    "else:\n",
    "    raise ValueError(\"需要 'created_time_utc' 或 'created_utc' 列。\")\n",
    "\n",
    "df[\"created_time_utc\"] = t\n",
    "df = df.dropna(subset=[\"created_time_utc\"])\n",
    "df = df[df[\"created_time_utc\"] >= \"2005-01-01\"]\n",
    "\n",
    "# ---- 去重（若有 id）----\n",
    "if \"id\" in df.columns:\n",
    "    df = df.sort_values(\"created_time_utc\").drop_duplicates(\"id\", keep=\"last\")\n",
    "\n",
    "# ---- 权重：log(1+score) + 0.5*log(1+num_comments) ----\n",
    "score = pd.to_numeric(df.get(\"score\", 0), errors=\"coerce\").fillna(0).clip(lower=0)\n",
    "if \"num_comments\" in df.columns:\n",
    "    numc = pd.to_numeric(df[\"num_comments\"], errors=\"coerce\").fillna(0).clip(lower=0)\n",
    "    weight = score.apply(math.log1p) + 0.5 * numc.apply(math.log1p)\n",
    "else:\n",
    "    weight = score.apply(math.log1p)\n",
    "df[\"__w\"] = weight\n",
    "\n",
    "# ---- 情绪列转数值并裁剪到 [-1,1] ----\n",
    "for c in sent_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\").clip(-1, 1)\n",
    "\n",
    "# ---- 日聚合（UTC 天）：加权平均；若当日总权重=0，则退化为简单均值 ----\n",
    "df[\"date_utc\"] = df[\"created_time_utc\"].dt.date\n",
    "\n",
    "def wavg(series: pd.Series, w: pd.Series) -> float:\n",
    "    s = series.astype(float)\n",
    "    w = w.astype(float)\n",
    "    den = np.nansum(w)\n",
    "    return float(np.nansum(s * w) / den) if den > 0 else float(np.nanmean(s))\n",
    "\n",
    "daily_rows = []\n",
    "for d, g in df.groupby(\"date_utc\"):\n",
    "    row = {\"date_utc\": d}\n",
    "    for c in sent_cols:\n",
    "        row[c] = wavg(g[c], g[\"__w\"])\n",
    "    daily_rows.append(row)\n",
    "\n",
    "daily = pd.DataFrame(daily_rows).sort_values(\"date_utc\").reset_index(drop=True)\n",
    "\n",
    "# ---- 扩展到每分钟并前向填充（merge_asof，更稳）----\n",
    "# 1) 日度时间戳设为当天 00:00 UTC\n",
    "daily_ff = daily.copy()\n",
    "daily_ff[\"ts_day\"] = pd.to_datetime(daily_ff[\"date_utc\"]).dt.tz_localize(\"UTC\")\n",
    "daily_ff = daily_ff.sort_values(\"ts_day\").reset_index(drop=True)\n",
    "\n",
    "# 2) 生成分钟索引（UTC）\n",
    "start = daily_ff[\"ts_day\"].min()\n",
    "end   = daily_ff[\"ts_day\"].max() + pd.Timedelta(days=1) - pd.Timedelta(minutes=1)\n",
    "minute_df = pd.DataFrame({\"ts\": pd.date_range(start=start, end=end, freq=\"min\", tz=\"UTC\")})\n",
    "\n",
    "# 3) asof 向后合并：每分钟取“最近且不晚于它的日度 00:00”值\n",
    "joined = pd.merge_asof(\n",
    "    minute_df.sort_values(\"ts\"),\n",
    "    daily_ff[[\"ts_day\"] + sent_cols].sort_values(\"ts_day\"),\n",
    "    left_on=\"ts\", right_on=\"ts_day\",\n",
    "    direction=\"backward\"\n",
    ")\n",
    "\n",
    "# 4) 清理列，只保留 ts + 情绪列\n",
    "minute_df = joined.drop(columns=[\"ts_day\"])\n",
    "minute_df = minute_df[[\"ts\"] + sent_cols]\n",
    "\n",
    "# ---- 保存 ----\n",
    "daily_out  = os.path.join(OUT_DIR, \"sentiment_daily_vader_s1_s5.csv\")\n",
    "minute_out = os.path.join(OUT_DIR, \"sentiment_1min_vader_s1_s5.csv\")\n",
    "\n",
    "daily_out_df = daily.rename(columns={\"date_utc\": \"ts\"})\n",
    "daily_out_df[\"ts\"] = pd.to_datetime(daily_out_df[\"ts\"]).dt.tz_localize(\"UTC\")\n",
    "\n",
    "daily_out_df.to_csv(daily_out, index=False, encoding=\"utf-8\")\n",
    "minute_df.to_csv(minute_out, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"✅ Done.\")\n",
    "print(f\"Days   : {daily_out_df['ts'].min().date()} ~ {daily_out_df['ts'].max().date()}\")\n",
    "print(f\"Daily  : {daily_out}\")\n",
    "print(f\"1-min  : {minute_out}\")\n",
    "print(\"\\nPreview (daily):\")\n",
    "display(daily_out_df.head(3))\n",
    "print(\"\\nPreview (1-min):\")\n",
    "display(minute_df.head(3))\n",
    "\n"
   ],
   "id": "c2e0b49cef057f33",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done.\n",
      "Days   : 2025-09-24 ~ 2025-09-24\n",
      "Daily  : data/reddit/weighted\\sentiment_daily_vader_s1_s5.csv\n",
      "1-min  : data/reddit/weighted\\sentiment_1min_vader_s1_s5.csv\n",
      "\n",
      "Preview (daily):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                         ts   vader       s1        s2        s3        s4  \\\n",
       "0 2025-09-24 00:00:00+00:00  0.4048 -0.23979  0.035424 -0.152483 -0.232451   \n",
       "\n",
       "         s5  \n",
       "0 -0.167206  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>vader</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-09-24 00:00:00+00:00</td>\n",
       "      <td>0.4048</td>\n",
       "      <td>-0.23979</td>\n",
       "      <td>0.035424</td>\n",
       "      <td>-0.152483</td>\n",
       "      <td>-0.232451</td>\n",
       "      <td>-0.167206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preview (1-min):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                         ts   vader       s1        s2        s3        s4  \\\n",
       "0 2025-09-24 00:00:00+00:00  0.4048 -0.23979  0.035424 -0.152483 -0.232451   \n",
       "1 2025-09-24 00:01:00+00:00  0.4048 -0.23979  0.035424 -0.152483 -0.232451   \n",
       "2 2025-09-24 00:02:00+00:00  0.4048 -0.23979  0.035424 -0.152483 -0.232451   \n",
       "\n",
       "         s5  \n",
       "0 -0.167206  \n",
       "1 -0.167206  \n",
       "2 -0.167206  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>vader</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-09-24 00:00:00+00:00</td>\n",
       "      <td>0.4048</td>\n",
       "      <td>-0.23979</td>\n",
       "      <td>0.035424</td>\n",
       "      <td>-0.152483</td>\n",
       "      <td>-0.232451</td>\n",
       "      <td>-0.167206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-09-24 00:01:00+00:00</td>\n",
       "      <td>0.4048</td>\n",
       "      <td>-0.23979</td>\n",
       "      <td>0.035424</td>\n",
       "      <td>-0.152483</td>\n",
       "      <td>-0.232451</td>\n",
       "      <td>-0.167206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-09-24 00:02:00+00:00</td>\n",
       "      <td>0.4048</td>\n",
       "      <td>-0.23979</td>\n",
       "      <td>0.035424</td>\n",
       "      <td>-0.152483</td>\n",
       "      <td>-0.232451</td>\n",
       "      <td>-0.167206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

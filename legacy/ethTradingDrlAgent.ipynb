{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbf2c92b",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning for Ethereum Trading\n",
    "## 15-Day State Space Methodology Implementation\n",
    "\n",
    "This notebook implements a comprehensive Deep Reinforcement Learning (DRL) approach for Ethereum trading based on academic research methodology. The implementation follows \"Reinforcement Learning Pair Trading: A Dynamic Scaling Approach\" with significant enhancements for cryptocurrency markets.\n",
    "\n",
    "### üìã **Methodology Overview**\n",
    "\n",
    "**State Space (15D)**: Comprehensive feature set including:\n",
    "- Core Features: Position, Z-score, Normalized Zone, Price Momentum, Z-score Momentum, Position Change\n",
    "- Technical Indicators: MACD (3 components), RSI, Bollinger Bands (3 components), OBV\n",
    "- Sentiment Data: Social Media Sentiment Score (Reddit)\n",
    "\n",
    "**Action Space**: Continuous [-1, 1] with maximum position shift constraint (0.1 per minute)\n",
    "\n",
    "**Reward Function**: Multi-component hybrid signal:\n",
    "- Primary: Profit-and-Loss (PnL) with normalization\n",
    "- Risk-Adjusted: Differential Sharpe Ratio\n",
    "- Penalties: Transaction costs, Drawdown penalties, Holding penalties\n",
    "- Incentives: Activity rewards\n",
    "\n",
    "**Training Protocol**: \n",
    "- Rolling window training (6-12 months)\n",
    "- Temporal data splits (70% train, 15% validation, 15% test)\n",
    "- Combinatorial Purged Cross-Validation (CPCV)\n",
    "- Episode length: 28 days (40,320 minutes)\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è **Notebook Structure**\n",
    "1. **Imports & Configuration** - Dependencies and global parameters\n",
    "2. **Data Loading & Preprocessing** - ETH data with technical indicators\n",
    "3. **Feature Engineering** - 15D state space construction\n",
    "4. **Environment Definition** - Custom trading environment\n",
    "5. **Model Architecture** - A2C/TD3 implementations\n",
    "6. **Training Pipeline** - Rolling window training\n",
    "7. **Hyperparameter Optimization** - Bulk configuration testing\n",
    "8. **Evaluation & Analysis** - Performance metrics and visualization\n",
    "9. **Sentiment Integration** - Placeholder for Reddit sentiment data\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ **Key Features**\n",
    "- ‚úÖ Complete 15D state space as per methodology\n",
    "- ‚úÖ Multi-component reward function optimization\n",
    "- ‚úÖ Rolling window training protocol\n",
    "- ‚úÖ Comprehensive hyperparameter testing (1000 configs)\n",
    "- ‚úÖ Advanced risk management\n",
    "- ‚úÖ Temporal data splitting\n",
    "- üîÑ Sentiment data integration (placeholder)\n",
    "\n",
    "---\n",
    "\n",
    "*Implementation Status: Core methodology complete, ready for sentiment data integration*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a3296c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/choemanseung/4th year/760/760-ethereum-trading/venv/lib/python3.12/site-packages (2.8.0)\n",
      "Requirement already satisfied: numpy in /Users/choemanseung/4th year/760/760-ethereum-trading/venv/lib/python3.12/site-packages (2.3.2)\n",
      "Requirement already satisfied: pandas in /Users/choemanseung/4th year/760/760-ethereum-trading/venv/lib/python3.12/site-packages (2.3.2)\n",
      "Requirement already satisfied: matplotlib in /Users/choemanseung/4th year/760/760-ethereum-trading/venv/lib/python3.12/site-packages (3.10.6)\n",
      "Requirement already satisfied: seaborn in /Users/choemanseung/4th year/760/760-ethereum-trading/venv/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: plotly in /Users/choemanseung/4th year/760/760-ethereum-trading/venv/lib/python3.12/site-packages (6.3.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/choemanseung/4th year/760/760-ethereum-trading/venv/lib/python3.12/site-packages (1.7.1)\n",
      "Requirement already satisfied: stable-baselines3 in /Users/choemanseung/4th year/760/760-ethereum-trading/venv/lib/python3.12/site-packages (2.7.0)\n",
      "Requirement already satisfied: gymnasium in /Users/choemanseung/4th year/760/760-ethereum-trading/venv/lib/python3.12/site-packages (1.2.0)\n",
      "Requirement already satisfied: pyarrow in /Users/choemanseung/4th year/760/760-ethereum-trading/venv/lib/python3.12/site-packages (21.0.0)\n",
      "Requirement already satisfied: ta in /Users/choemanseung/4th year/760/760-ethereum-trading/venv/lib/python3.12/site-packages (0.11.0)\n",
      "Requirement already satisfied: filelock in /Users/choemanseung/4th year/760/760-ethereum-trading/venv/lib/python3.12/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/choemanseung/4th year/760/760-ethereum-trading/venv/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /Users/choemanseung/4th year/760/760-ethereum-trading/venv/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/choemanseung/4th year/760/760-ethereum-trading/venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/choemanseung/4th year/760/760-ethereum-trading/venv/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/choemanseung/4th year/760/760-ethereum-trading/venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/choemanseung/4th year/760/760-ethereum-trading/venv/lib/python3.12/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/choemanseung/4th year/760/760-ethereum-trading/venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/choemanseung/4th year/760/760-ethereum-trading/venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/choemanseung/4th year/760/760-ethereum-trading/venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/choemanseung/4th year/760/760-ethereum-trading/venv/lib/python3.12/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/choemanseung/4th year/760/760-ethereum-trading/venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/choemanseung/4th year/760/760-ethereum-trading/venv/lib/python3.12/site-packages (from matplotlib) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/choemanseung/4th year/760/760-ethereum-trading/venv/lib/python3.12/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/choemanseung/4th year/760/760-ethereum-trading/venv/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/choemanseung/4th year/760/760-ethereum-trading/venv/lib/python3.12/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/choemanseung/4th year/760/760-ethereum-trading/venv/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /Users/choemanseung/4th year/760/760-ethereum-trading/venv/lib/python3.12/site-packages (from plotly) (2.3.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Users/choemanseung/4th year/760/760-ethereum-trading/venv/lib/python3.12/site-packages (from scikit-learn) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/choemanseung/4th year/760/760-ethereum-trading/venv/lib/python3.12/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/choemanseung/4th year/760/760-ethereum-trading/venv/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: cloudpickle in /Users/choemanseung/4th year/760/760-ethereum-trading/venv/lib/python3.12/site-packages (from stable-baselines3) (3.1.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/choemanseung/4th year/760/760-ethereum-trading/venv/lib/python3.12/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: six>=1.5 in /Users/choemanseung/4th year/760/760-ethereum-trading/venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/choemanseung/4th year/760/760-ethereum-trading/venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/choemanseung/4th year/760/760-ethereum-trading/venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch numpy pandas matplotlib seaborn plotly scikit-learn stable-baselines3 gymnasium pyarrow ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a245068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports loaded successfully\n",
      "üé≤ Random seed set to: 42\n",
      "üíª Available CPU cores: 12\n",
      "üêç Python version: 3.12.5 (v3.12.5:ff3bc82f7c9, Aug  7 2024, 05:32:06) [Clang 13.0.0 (clang-1300.0.29.30)]\n",
      "üìä NumPy version: 2.3.2\n",
      "üêº Pandas version: 2.3.2\n",
      "üèãÔ∏è Gymnasium version: 1.2.0\n",
      "üß† Stable-Baselines3 available\n"
     ]
    }
   ],
   "source": [
    "# ===================== IMPORTS & DEPENDENCIES =====================\n",
    "# Core libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple, Optional, Any, Union\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning & Deep Reinforcement Learning\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import A2C, TD3\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold, BaseCallback\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "# Data Processing & Technical Analysis\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "import ta  # Technical Analysis library\n",
    "\n",
    "# Visualization & Analysis\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Statistical Analysis\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "# import statsmodels.api as sm\n",
    "\n",
    "# Parallel Processing\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import joblib\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "set_random_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (15, 8)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All imports loaded successfully\")\n",
    "print(f\"üé≤ Random seed set to: {RANDOM_SEED}\")\n",
    "print(f\"üíª Available CPU cores: {cpu_count()}\")\n",
    "print(f\"üêç Python version: {sys.version}\")\n",
    "print(f\"üìä NumPy version: {np.__version__}\")\n",
    "print(f\"üêº Pandas version: {pd.__version__}\")\n",
    "print(f\"üèãÔ∏è Gymnasium version: {gym.__version__}\")\n",
    "print(f\"üß† Stable-Baselines3 available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aa797e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Configuration loaded successfully!\n",
      "üìä State Space Dimensions: 15D\n",
      "   - Core Features: 6D (Position, Z-score, Zone, Price Momentum, Z-score Momentum, Position Change)\n",
      "   - Technical Indicators: 8D (MACD√ó3, RSI√ó1, BB√ó3, OBV√ó1)\n",
      "   - Sentiment Data: 1D (Reddit Sentiment)\n",
      "üí∞ Trading Configuration:\n",
      "   - Initial Capital: $10,000\n",
      "   - Episode Length: 10,080 minutes (28 days)\n",
      "   - Max Position Shift: 0.1 per minute\n",
      "üéØ Reward Function: Multi-component hybrid (6 components)\n",
      "üìÖ Training Protocol: Rolling window (6 months)\n",
      "üíª Parallel Processing: 8 cores\n",
      "üîÑ Sentiment Integration: Enabled (placeholder)\n",
      "üìÅ Output Directory: ./processed_data_15d\n"
     ]
    }
   ],
   "source": [
    "# ===================== CONFIGURATION & PARAMETERS =====================\n",
    "\n",
    "# ==================== DATA CONFIGURATION ====================\n",
    "# Path to preprocessed ETH data with technical indicators\n",
    "DATA_PATH = \"../ETHUSDT_1m_with_indicators.parquet\"\n",
    "TIMESTAMP_COL = \"ts\"                    # Timestamp column name\n",
    "PRICE_COL = \"close\"                     # Primary price column\n",
    "OUTPUT_DIR = \"./processed_data_15d\"     # Output directory for processed data\n",
    "MODEL_DIR = \"./models\"                  # Directory for saved models\n",
    "CONFIG_FILE = \"./drl_training_configs.json\"  # Hyperparameter configurations\n",
    "\n",
    "# ==================== STATE SPACE CONFIGURATION (15D) ====================\n",
    "# Core Features (6 dimensions)\n",
    "MA_PERIOD = 60                          # Moving average period for Z-score baseline\n",
    "Z_SCORE_WINDOW = 120                    # Rolling window for Z-score calculation (2 hours)\n",
    "LOOKBACK_WINDOW = 120                   # State representation lookback (2 hours minimum)\n",
    "\n",
    "# Zone thresholds for normalized zone calculation\n",
    "OPEN_THRESHOLD = 2.0                    # Z-score threshold to open positions (¬±2.0)\n",
    "CLOSE_THRESHOLD = 0.5                   # Z-score threshold to close positions (¬±0.5)\n",
    "\n",
    "# Technical Indicators (8 dimensions)\n",
    "# MACD: 3 components (MACD, Signal, Histogram)\n",
    "# RSI: 1 component\n",
    "# Bollinger Bands: 3 components (Middle, Upper, Lower)\n",
    "# OBV: 1 component\n",
    "\n",
    "# Sentiment Data (1 dimension)\n",
    "SENTIMENT_WEIGHT = 0.15                 # Weight for sentiment in Fear & Greed Index\n",
    "\n",
    "# ==================== ACTION SPACE CONFIGURATION ====================\n",
    "# Continuous action space [-1, 1]\n",
    "MAX_POSITION_SHIFT = 0.1                # Maximum position change per minute (as per methodology)\n",
    "ACTION_NOISE_STD = 0.1                  # Action noise for exploration\n",
    "\n",
    "# ==================== REWARD FUNCTION CONFIGURATION ====================\n",
    "# Multi-component reward system weights\n",
    "REWARD_COMPONENTS = {\n",
    "    # Primary Reward: Profit-and-Loss\n",
    "    'pnl_scale': 100.0,                 # PnL reward scaling factor\n",
    "    'pnl_normalization': 'nav',         # Normalization method ('nav', 'portfolio', 'none')\n",
    "    \n",
    "    # Risk-Adjusted Return (Differential Sharpe Ratio)\n",
    "    'sharpe_weight': 0.2,               # Weight for Sharpe ratio component\n",
    "    'sharpe_window': 1440,              # Window for Sharpe calculation (24 hours)\n",
    "    \n",
    "    # Transaction Costs Penalty\n",
    "    'transaction_penalty': 1.0,         # Base transaction cost multiplier\n",
    "    'fee_rate': 0.001,                  # Trading fee rate (0.1%)\n",
    "    'slippage': 0.0005,                 # Market impact slippage (0.05%)\n",
    "    \n",
    "    # Drawdown Penalty\n",
    "    'drawdown_threshold': 0.10,         # Drawdown threshold (10%)\n",
    "    'drawdown_penalty': 50.0,           # Large negative reward for exceeding threshold\n",
    "    \n",
    "    # Holding Reward/Penalty\n",
    "    'holding_penalty': 0.001,           # Penalty for prolonged inactivity\n",
    "    'holding_reward': 0.0005,           # Bonus for profitable positions\n",
    "    'max_hold_periods': 1440,           # Maximum hold duration (24 hours)\n",
    "    \n",
    "    # Activity Incentives\n",
    "    'activity_reward': 0.1,             # Reward for taking action\n",
    "    'inactivity_penalty': 0.005,        # Escalating penalty for inactivity\n",
    "\n",
    "    # Sentiment Reward Feedback\n",
    "    'enable_sentiment_reward': True,   # Optional toggle\n",
    "    'sentiment_reward_weight': 0.0,     # Tunable multiplier\n",
    "\n",
    "}\n",
    "\n",
    "# ==================== TRADING PARAMETERS ====================\n",
    "INITIAL_CAPITAL = 10000.0               # Initial portfolio value\n",
    "EPISODE_LENGTH = 1440 * 7                  # Episode length in minutes (28 days)\n",
    "POSITION_LIMITS = (-1.0, 1.0)          # Position limits\n",
    "\n",
    "# ==================== TRAINING PROTOCOL CONFIGURATION ====================\n",
    "# Data Splitting Strategy (Strict Chronological Order)\n",
    "TRAIN_RATIO = 0.70                      # Training set: 70% of data (~2.6 years)\n",
    "VALIDATION_RATIO = 0.15                 # Validation set: 15% of data (~7 months)  \n",
    "TEST_RATIO = 0.15                       # Test set: 15% of data (~7 months)\n",
    "\n",
    "# Rolling Window Training\n",
    "ROLLING_WINDOW_MONTHS = 6              # Rolling window size (6-12 months)\n",
    "EVALUATION_PERIOD_MONTHS = 1           # Evaluation period (1 month)\n",
    "ROLLING_STEP_MONTHS = 1                 # Step size for rolling (1 month)\n",
    "\n",
    "# Combinatorial Purged Cross-Validation (CPCV)\n",
    "N_PURGED_SEGMENTS = 10                  # Number of segments for CPCV\n",
    "PURGE_PERIODS = 1440                    # Purge periods (24 hours)\n",
    "EMBARGO_PERIODS = 720                   # Embargo periods (12 hours)\n",
    "\n",
    "# Episode and State Design\n",
    "STATE_LOOKBACK_HOURS = 2                # State lookback (minimum for Z-score)\n",
    "\n",
    "# ==================== MODEL HYPERPARAMETERS ====================\n",
    "# A2C Default Parameters\n",
    "A2C_PARAMS = {\n",
    "    'learning_rate': 3e-4,\n",
    "    'n_steps': 2048,\n",
    "    'gamma': 0.99,\n",
    "    'gae_lambda': 0.95,\n",
    "    'ent_coef': 0.01,\n",
    "    'vf_coef': 0.5,\n",
    "    'max_grad_norm': 0.5,\n",
    "    'normalize_advantage': True,\n",
    "    'use_rms_prop': True,\n",
    "    'use_sde': False,\n",
    "}\n",
    "\n",
    "# TD3 Default Parameters  \n",
    "TD3_PARAMS = {\n",
    "    'learning_rate': 3e-4,\n",
    "    'buffer_size': 1000000,\n",
    "    'learning_starts': 10000,\n",
    "    'batch_size': 256,\n",
    "    'tau': 0.005,\n",
    "    'gamma': 0.99,\n",
    "    'train_freq': 1,\n",
    "    'gradient_steps': 1,\n",
    "    'noise_std': 0.1,\n",
    "    'target_noise': 0.2,\n",
    "    'noise_clip': 0.5,\n",
    "    'policy_delay': 2,\n",
    "}\n",
    "\n",
    "# Training Configuration\n",
    "TRAINING_CONFIG = {\n",
    "    'total_timesteps': 200000,           # Default training timesteps\n",
    "    'eval_freq': 10000,                  # Evaluation frequency\n",
    "    'n_eval_episodes': 10,               # Number of evaluation episodes\n",
    "    'eval_log_path': './logs',           # Evaluation log path\n",
    "    'save_freq': 25000,                  # Model save frequency\n",
    "    'verbose': 1,                        # Verbosity level\n",
    "}\n",
    "\n",
    "# ==================== BULK TESTING CONFIGURATION ====================\n",
    "BULK_CONFIG = {\n",
    "    'max_parallel_jobs': min(cpu_count() - 1, 8),  # Parallel jobs for bulk testing\n",
    "    'config_batch_size': 10,             # Batch size for configuration testing\n",
    "    'early_stopping_patience': 3,        # Early stopping patience\n",
    "    'performance_metric': 'sharpe_ratio', # Primary metric for optimization\n",
    "    'min_evaluation_episodes': 5,        # Minimum episodes for evaluation\n",
    "    'results_file': 'bulk_results.json', # Results output file\n",
    "}\n",
    "\n",
    "# ==================== FEATURE ENGINEERING CONFIGURATION ====================\n",
    "FEATURE_CONFIG = {\n",
    "    'normalization_method': 'robust',    # Normalization method ('standard', 'minmax', 'robust')\n",
    "    'feature_selection': True,           # Enable feature selection\n",
    "    'correlation_threshold': 0.95,       # Correlation threshold for feature removal\n",
    "    'variance_threshold': 0.01,          # Variance threshold for feature removal\n",
    "    'pca_components': None,              # PCA components (None = disabled)\n",
    "}\n",
    "\n",
    "# ==================== SENTIMENT INTEGRATION CONFIGURATION ====================\n",
    "SENTIMENT_CONFIG = {\n",
    "    'enabled': True,                     # Enable sentiment features (placeholder)\n",
    "    'data_path': '../sentiment_1min_vader_s1_s5.csv',  # Path to sentiment data\n",
    "    'aggregation_method': 'weighted_mean', # Aggregation method for sentiment scores\n",
    "    'time_window': 60,                   # Time window for sentiment aggregation (minutes)\n",
    "    'smoothing_factor': 0.1,             # Exponential smoothing factor\n",
    "    'sentiment_features': [              # Sentiment feature columns (placeholder)\n",
    "        'reddit_sentiment_score',\n",
    "        'reddit_compound_score', \n",
    "        'reddit_post_volume',\n",
    "        'reddit_comment_sentiment'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs('./logs', exist_ok=True)\n",
    "\n",
    "print(\"üîß Configuration loaded successfully!\")\n",
    "print(f\"üìä State Space Dimensions: 15D\")\n",
    "print(f\"   - Core Features: 6D (Position, Z-score, Zone, Price Momentum, Z-score Momentum, Position Change)\")\n",
    "print(f\"   - Technical Indicators: 8D (MACD√ó3, RSI√ó1, BB√ó3, OBV√ó1)\")\n",
    "print(f\"   - Sentiment Data: 1D (Reddit Sentiment)\")\n",
    "print(f\"üí∞ Trading Configuration:\")\n",
    "print(f\"   - Initial Capital: ${INITIAL_CAPITAL:,.0f}\")\n",
    "print(f\"   - Episode Length: {EPISODE_LENGTH:,} minutes (28 days)\")\n",
    "print(f\"   - Max Position Shift: {MAX_POSITION_SHIFT} per minute\")\n",
    "print(f\"üéØ Reward Function: Multi-component hybrid (6 components)\")\n",
    "print(f\"üìÖ Training Protocol: Rolling window ({ROLLING_WINDOW_MONTHS} months)\")\n",
    "print(f\"üíª Parallel Processing: {BULK_CONFIG['max_parallel_jobs']} cores\")\n",
    "print(f\"üîÑ Sentiment Integration: {'Enabled' if SENTIMENT_CONFIG['enabled'] else 'Disabled'} (placeholder)\")\n",
    "print(f\"üìÅ Output Directory: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adc95978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting data loading and feature engineering...\n",
      "üìÅ Loading data from: ../ETHUSDT_1m_with_indicators.parquet\n",
      "   üîß Found timestamp in index, converting to column\n",
      "   ‚úÖ Loaded 1,883,407 rows of data\n",
      "   üìä Columns: ['ts', 'open', 'high', 'low', 'close', 'volume', 'number_of_trades', 'symbol', 'RSI', 'BB_mid', 'BB_high', 'BB_low', 'EMA_12', 'EMA_26', 'MACD', 'MACD_signal', 'MACD_diff', 'ATR']\n",
      "   üìÖ Date range: 2022-01-01 00:33:00+00:00 to 2025-07-31 23:59:00+00:00\n",
      "\n",
      "üîß Feature Engineering Pipeline (15D State Space):\n",
      "   üî¨ Calculating core features (6D)...\n",
      "      ‚úÖ Z-score range: [-10.32, 10.36]\n",
      "      ‚úÖ Zone distribution: {0.0: 569097, -0.5: 568471, 0.5: 566294, -1.0: 92863, 1.0: 86682}\n",
      "      ‚úÖ Price momentum range: [-0.0959, 0.0635]\n",
      "   üìà Calculating technical indicators (8D)...\n",
      "      üìä Full OHLCV data available - calculating comprehensive indicators\n",
      "      ‚úÖ Technical indicators calculated and normalized\n",
      "      üìä Indicators: 8 components\n",
      "   üîÑ Integrating sentiment data (1D)...\n",
      "      ‚úÖ Sentiment merged ‚Äì coverage: 100.0%\n",
      "\n",
      "‚úÖ All 15D features successfully calculated!\n",
      "\n",
      "üìä Feature Engineering Summary:\n",
      "   üìà Raw data: 1,883,407 rows\n",
      "   üîß Processed data: 1,883,407 rows\n",
      "   üìã Features: 13 dimensions\n",
      "   üéØ State space: 15D (as per methodology)\n",
      "   üíæ Memory usage: 533.5 MB\n",
      "   üíæ Processed data saved: ./processed_data_15d/processed_eth_data_15d.parquet\n",
      "\n",
      "üìã Sample of 15D Feature Data:\n",
      "          z_score  zone_norm  price_momentum  z_score_momentum  macd_norm  \\\n",
      "1883402 -0.100069        0.0        0.000898          0.345595  -0.305405   \n",
      "1883403  0.109267        0.0        0.000541          0.209336  -0.237294   \n",
      "1883404 -0.046648        0.0       -0.000438         -0.155915  -0.197820   \n",
      "1883405 -0.477679        0.0       -0.001238         -0.431031  -0.212083   \n",
      "1883406 -0.060336        0.0        0.001050          0.417343  -0.180113   \n",
      "\n",
      "         macd_signal_norm  macd_diff_norm  rsi_norm  bb_mid_norm  \\\n",
      "1883402         -0.471293        0.455547 -0.033035     0.851316   \n",
      "1883403         -0.428076        0.550241  0.046707     0.851349   \n",
      "1883404         -0.385111        0.546953 -0.021522     0.851528   \n",
      "1883405         -0.353772        0.396004 -0.202000     0.851491   \n",
      "1883406         -0.321904        0.402852 -0.034044     0.851574   \n",
      "\n",
      "         bb_high_norm  bb_low_norm  obv_norm  sentiment_score  \n",
      "1883402      0.850257     0.852098 -0.729490         0.084228  \n",
      "1883403      0.850375     0.852046 -0.729366         0.084228  \n",
      "1883404      0.850650     0.852129 -0.729553         0.084228  \n",
      "1883405      0.850625     0.852080 -0.729614         0.084228  \n",
      "1883406      0.850773     0.852097 -0.728610         0.084228  \n",
      "\n",
      "üéØ Data ready for 15D state space training!\n",
      "üìä Dataset shape: (1883407, 31)\n",
      "üìÖ Ready for temporal data splitting...\n"
     ]
    }
   ],
   "source": [
    "# ===================== DATA LOADING & PREPROCESSING =====================\n",
    "\n",
    "def load_eth_data(data_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and preprocess ETH/USDT 1-minute data with technical indicators.\n",
    "    \n",
    "    Args:\n",
    "        data_path: Path to the ETH data file\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned and preprocessed data\n",
    "    \"\"\"\n",
    "    print(f\"üìÅ Loading data from: {data_path}\")\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"Data file not found: {data_path}\")\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_parquet(data_path)\n",
    "    \n",
    "    # Handle timestamp column and reset index if needed\n",
    "    if df.index.name == TIMESTAMP_COL or 'ts' in str(df.index.name) or isinstance(df.index, pd.DatetimeIndex):\n",
    "        print(f\"   üîß Found timestamp in index, converting to column\")\n",
    "        df = df.reset_index()  # Convert index to column (don't drop it!)\n",
    "    \n",
    "    # Clean column names\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    # Check for timestamp column\n",
    "    if TIMESTAMP_COL not in df.columns:\n",
    "        # Try to find timestamp column\n",
    "        possible_ts_cols = ['timestamp', 'ts', 'time', 'date', 'datetime']\n",
    "        for col in possible_ts_cols:\n",
    "            if col in df.columns:\n",
    "                print(f\"   üîß Found timestamp column: '{col}', renaming to '{TIMESTAMP_COL}'\")\n",
    "                df = df.rename(columns={col: TIMESTAMP_COL})\n",
    "                break\n",
    "        else:\n",
    "            raise ValueError(f\"Could not find timestamp column. Available columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Basic preprocessing\n",
    "    df = df.dropna()  # Remove any NaN values\n",
    "    df = df.reset_index(drop=True)  # Reset row index\n",
    "    \n",
    "    # Ensure required columns are present\n",
    "    required_cols = [PRICE_COL, 'volume']\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Required column '{col}' not found in data\")\n",
    "    \n",
    "    # Sort by timestamp and clean price data\n",
    "    df = df.sort_values(TIMESTAMP_COL).reset_index(drop=True)\n",
    "    df[PRICE_COL] = pd.to_numeric(df[PRICE_COL], errors='coerce')\n",
    "    df = df.dropna(subset=[PRICE_COL])\n",
    "    df = df[df[PRICE_COL] > 0]\n",
    "    \n",
    "    print(f\"   ‚úÖ Loaded {len(df):,} rows of data\")\n",
    "    print(f\"   üìä Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Display date range if possible\n",
    "    if TIMESTAMP_COL in df.columns:\n",
    "        if df[TIMESTAMP_COL].dtype == 'int64':\n",
    "            # Convert Unix timestamp to datetime for display\n",
    "            start_date = pd.to_datetime(df[TIMESTAMP_COL].min(), unit='s')\n",
    "            end_date = pd.to_datetime(df[TIMESTAMP_COL].max(), unit='s')\n",
    "            print(f\"   üìÖ Date range: {start_date} to {end_date}\")\n",
    "        elif pd.api.types.is_datetime64_any_dtype(df[TIMESTAMP_COL]):\n",
    "            print(f\"   üìÖ Date range: {df[TIMESTAMP_COL].min()} to {df[TIMESTAMP_COL].max()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_core_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate core features (6 dimensions) as per methodology.\n",
    "    Features 1 & 6 (position, position_change) will be added by the environment.\n",
    "    \n",
    "    Core Features:\n",
    "    2. Z-score: Standardized mean-reversion signal\n",
    "    3. Normalized Zone: Discrete trading signal\n",
    "    4. Price Momentum: Last-minute price return\n",
    "    5. Z-score Momentum: One-minute change in Z-score\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with price data\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with core features added\n",
    "    \"\"\"\n",
    "    print(\"   üî¨ Calculating core features (6D)...\")\n",
    "    \n",
    "    # Calculate pseudo-spread and Z-score\n",
    "    # Step 1: Moving average (baseline for pseudo-spread)\n",
    "    df['ma_baseline'] = df[PRICE_COL].rolling(window=MA_PERIOD, min_periods=1).mean()\n",
    "    \n",
    "    # Step 2: Spread (price deviation from MA) \n",
    "    df['spread'] = df[PRICE_COL] - df['ma_baseline']\n",
    "    \n",
    "    # Step 3: Z-score (standardized spread over rolling window)\n",
    "    spread_mean = df['spread'].rolling(window=Z_SCORE_WINDOW, min_periods=1).mean()\n",
    "    spread_std = df['spread'].rolling(window=Z_SCORE_WINDOW, min_periods=1).std()\n",
    "    df['z_score'] = (df['spread'] - spread_mean) / (spread_std + 1e-8)\n",
    "    df['z_score'] = df['z_score'].fillna(0)  # Fill NaN with neutral value\n",
    "    \n",
    "    # Feature 3: Normalized Zone\n",
    "    def calculate_trading_zone(z_score):\n",
    "        if z_score > OPEN_THRESHOLD:\n",
    "            return 1.0  # Strong sell signal\n",
    "        elif z_score > CLOSE_THRESHOLD:\n",
    "            return 0.5  # Weak sell signal\n",
    "        elif z_score >= -CLOSE_THRESHOLD:\n",
    "            return 0.0  # Neutral (close positions)\n",
    "        elif z_score >= -OPEN_THRESHOLD:\n",
    "            return -0.5  # Weak buy signal\n",
    "        else:\n",
    "            return -1.0  # Strong buy signal\n",
    "    \n",
    "    df['zone_norm'] = df['z_score'].apply(calculate_trading_zone)\n",
    "    \n",
    "    # Feature 4: Price Momentum (last-minute price return)\n",
    "    df['price_momentum'] = df[PRICE_COL].pct_change(1).fillna(0)\n",
    "    df['price_momentum'] = df['price_momentum'].clip(-0.1, 0.1)  # Clip for numerical stability\n",
    "    \n",
    "    # Feature 5: Z-score Momentum (one-minute change in Z-score)\n",
    "    df['z_score_momentum'] = df['z_score'].diff(1).fillna(0)\n",
    "    df['z_score_momentum'] = df['z_score_momentum'].clip(-2.0, 2.0)  # Clip extreme values\n",
    "    \n",
    "    # Clean up intermediate columns\n",
    "    df = df.drop(['ma_baseline', 'spread'], axis=1)\n",
    "    \n",
    "    print(f\"      ‚úÖ Z-score range: [{df['z_score'].min():.2f}, {df['z_score'].max():.2f}]\")\n",
    "    print(f\"      ‚úÖ Zone distribution: {df['zone_norm'].value_counts().to_dict()}\")\n",
    "    print(f\"      ‚úÖ Price momentum range: [{df['price_momentum'].min():.4f}, {df['price_momentum'].max():.4f}]\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_technical_indicators(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate technical indicators (8 dimensions) as per methodology.\n",
    "    \n",
    "    Technical Indicators:\n",
    "    7. MACD Line (normalized)\n",
    "    8. MACD Signal (normalized) \n",
    "    9. MACD Histogram (normalized)\n",
    "    10. RSI (normalized)\n",
    "    11. Bollinger Bands Mid (normalized)\n",
    "    12. Bollinger Bands High (normalized)\n",
    "    13. Bollinger Bands Low (normalized)\n",
    "    14. OBV (normalized)\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with OHLCV data\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with technical indicators added\n",
    "    \"\"\"\n",
    "    print(\"   üìà Calculating technical indicators (8D)...\")\n",
    "    \n",
    "    # Check for required columns\n",
    "    required_cols = ['open', 'high', 'low', 'close', 'volume']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"      ‚ö†Ô∏è Missing columns for technical indicators: {missing_cols}\")\n",
    "        print(\"      üìä Available columns:\", list(df.columns))\n",
    "        \n",
    "        # Use existing indicator columns if available\n",
    "        if all(col in df.columns for col in ['MACD', 'MACD_signal', 'MACD_diff']):\n",
    "            df['macd_norm'] = df['MACD']\n",
    "            df['macd_signal_norm'] = df['MACD_signal']\n",
    "            df['macd_diff_norm'] = df['MACD_diff']\n",
    "            print(\"      ‚úÖ Using existing MACD indicators\")\n",
    "        else:\n",
    "            # Calculate MACD manually using close prices\n",
    "            ema12 = df['close'].ewm(span=12).mean()\n",
    "            ema26 = df['close'].ewm(span=26).mean()\n",
    "            df['macd_norm'] = ema12 - ema26\n",
    "            df['macd_signal_norm'] = df['macd_norm'].ewm(span=9).mean()\n",
    "            df['macd_diff_norm'] = df['macd_norm'] - df['macd_signal_norm']\n",
    "            print(\"      ‚úÖ Calculated MACD from close prices\")\n",
    "        \n",
    "        # RSI calculation\n",
    "        if 'RSI' in df.columns:\n",
    "            df['rsi_norm'] = df['RSI'] / 100.0  # Normalize to [0,1]\n",
    "            print(\"      ‚úÖ Using existing RSI indicator\")\n",
    "        else:\n",
    "            # Calculate RSI manually\n",
    "            delta = df['close'].diff()\n",
    "            gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "            loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "            rs = gain / (loss + 1e-8)  # Add small epsilon to avoid division by zero\n",
    "            df['rsi_norm'] = (1 - (1 / (1 + rs)))\n",
    "            print(\"      ‚úÖ Calculated RSI from close prices\")\n",
    "        \n",
    "        # Bollinger Bands\n",
    "        if all(col in df.columns for col in ['BB_mid', 'BB_high', 'BB_low']):\n",
    "            df['bb_mid_norm'] = df['BB_mid']\n",
    "            df['bb_high_norm'] = df['BB_high'] \n",
    "            df['bb_low_norm'] = df['BB_low']\n",
    "            print(\"      ‚úÖ Using existing Bollinger Bands\")\n",
    "        else:\n",
    "            # Calculate Bollinger Bands\n",
    "            bb_period = 20\n",
    "            bb_std = 2\n",
    "            df['bb_mid_norm'] = df['close'].rolling(window=bb_period).mean()\n",
    "            bb_std_val = df['close'].rolling(window=bb_period).std()\n",
    "            df['bb_high_norm'] = df['bb_mid_norm'] + (bb_std_val * bb_std)\n",
    "            df['bb_low_norm'] = df['bb_mid_norm'] - (bb_std_val * bb_std)\n",
    "            print(\"      ‚úÖ Calculated Bollinger Bands from close prices\")\n",
    "        \n",
    "        # OBV calculation\n",
    "        if 'volume' in df.columns:\n",
    "            # Calculate OBV efficiently\n",
    "            price_change = df['close'].diff()\n",
    "            volume_direction = np.where(price_change > 0, df['volume'], \n",
    "                                      np.where(price_change < 0, -df['volume'], 0))\n",
    "            df['obv_norm'] = volume_direction.cumsum()\n",
    "            print(\"      ‚úÖ Calculated OBV from volume data\")\n",
    "        else:\n",
    "            # Create dummy OBV if no volume data\n",
    "            df['obv_norm'] = 0\n",
    "            print(\"      ‚ö†Ô∏è No volume data - using dummy OBV\")\n",
    "            \n",
    "    else:\n",
    "        # Calculate all indicators using ta library\n",
    "        print(\"      üìä Full OHLCV data available - calculating comprehensive indicators\")\n",
    "        \n",
    "        try:\n",
    "            # 1-3. MACD (3 components)\n",
    "            macd = ta.trend.MACD(df['close'])\n",
    "            df['macd_norm'] = macd.macd()\n",
    "            df['macd_signal_norm'] = macd.macd_signal()\n",
    "            df['macd_diff_norm'] = macd.macd_diff()\n",
    "            \n",
    "            # 4. RSI\n",
    "            df['rsi_norm'] = ta.momentum.RSIIndicator(df['close']).rsi() / 100.0  # Normalize to [0,1]\n",
    "            \n",
    "            # 5-7. Bollinger Bands (3 components)\n",
    "            bb = ta.volatility.BollingerBands(df['close'])\n",
    "            df['bb_mid_norm'] = bb.bollinger_mavg()\n",
    "            df['bb_high_norm'] = bb.bollinger_hband()\n",
    "            df['bb_low_norm'] = bb.bollinger_lband()\n",
    "            \n",
    "            # 8. OBV\n",
    "            df['obv_norm'] = ta.volume.OnBalanceVolumeIndicator(df['close'], df['volume']).on_balance_volume()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      ‚ö†Ô∏è Error calculating technical indicators with ta library: {e}\")\n",
    "            # Create placeholder features if calculation fails\n",
    "            for indicator in ['macd_norm', 'macd_signal_norm', 'macd_diff_norm', 'rsi_norm', \n",
    "                             'bb_mid_norm', 'bb_high_norm', 'bb_low_norm', 'obv_norm']:\n",
    "                df[indicator] = 0.0\n",
    "    \n",
    "    # Normalize all technical indicators\n",
    "    tech_indicators = ['macd_norm', 'macd_signal_norm', 'macd_diff_norm', 'rsi_norm', \n",
    "                      'bb_mid_norm', 'bb_high_norm', 'bb_low_norm', 'obv_norm']\n",
    "    \n",
    "    for indicator in tech_indicators:\n",
    "        if indicator in df.columns:\n",
    "            # Robust normalization (clip outliers, then normalize)\n",
    "            q99 = df[indicator].quantile(0.99)\n",
    "            q01 = df[indicator].quantile(0.01)\n",
    "            df[indicator] = df[indicator].clip(q01, q99)\n",
    "            \n",
    "            # Min-max normalization to [-1, 1] range\n",
    "            min_val = df[indicator].min()\n",
    "            max_val = df[indicator].max()\n",
    "            if max_val > min_val:\n",
    "                df[indicator] = 2 * (df[indicator] - min_val) / (max_val - min_val) - 1\n",
    "            else:\n",
    "                df[indicator] = 0\n",
    "            \n",
    "            df[indicator] = df[indicator].fillna(0)\n",
    "    \n",
    "    print(f\"      ‚úÖ Technical indicators calculated and normalized\")\n",
    "    print(f\"      üìä Indicators: {len(tech_indicators)} components\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def integrate_sentiment_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Integrate minute-level sentiment data into the feature set.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with price and technical features\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with sentiment_score feature added\n",
    "    \"\"\"\n",
    "    print(\"   üîÑ Integrating sentiment data (1D)...\")\n",
    "\n",
    "    if not SENTIMENT_CONFIG.get(\"enabled\", False):\n",
    "        df[\"sentiment_score\"] = 0.0\n",
    "        print(\"      ‚ö†Ô∏è Sentiment disabled ‚Äì using neutral value 0.0\")\n",
    "        return df\n",
    "\n",
    "    data_path = SENTIMENT_CONFIG.get(\"data_path\")\n",
    "    if not data_path or not os.path.exists(data_path):\n",
    "        print(\"      ‚ö†Ô∏è Sentiment file missing ‚Äì using 0.0\")\n",
    "        df[\"sentiment_score\"] = 0.0\n",
    "        return df\n",
    "\n",
    "    try:\n",
    "        sentiment_df = pd.read_csv(data_path)\n",
    "    except Exception as exc:\n",
    "        print(f\"      ‚ö†Ô∏è Failed to load sentiment data: {exc} - defaulting to 0.0\")\n",
    "        df['sentiment_score'] = 0.0\n",
    "        return df\n",
    "\n",
    "    if TIMESTAMP_COL not in sentiment_df.columns:\n",
    "        raise ValueError(f\"Sentiment file must include '{TIMESTAMP_COL}' column\")\n",
    "\n",
    "    # Process sentiment data\n",
    "    sentiment_df[TIMESTAMP_COL] = pd.to_datetime(sentiment_df[TIMESTAMP_COL])\n",
    "    sentiment_cols = [c for c in sentiment_df.columns if c != TIMESTAMP_COL]\n",
    "    \n",
    "    if not sentiment_cols:\n",
    "        print(\"      ‚ö†Ô∏è No sentiment columns found - defaulting to 0.0\")\n",
    "        df['sentiment_score'] = 0.0\n",
    "        return df\n",
    "    \n",
    "    # Aggregate to minute level and compute a single sentiment score\n",
    "    sentiment_df = (\n",
    "        sentiment_df.groupby(TIMESTAMP_COL)[sentiment_cols]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "    sentiment_df[\"sentiment_score\"] = sentiment_df[sentiment_cols].mean(axis=1)\n",
    "\n",
    "    # Handle timestamp compatibility for merging\n",
    "    df_ts_dtype = df[TIMESTAMP_COL].dtype\n",
    "    \n",
    "    # Check if timestamp is numeric (Unix timestamp)\n",
    "    if pd.api.types.is_numeric_dtype(df_ts_dtype):\n",
    "        # Convert datetime to Unix timestamp to match df format\n",
    "        sentiment_df[TIMESTAMP_COL] = (\n",
    "            sentiment_df[TIMESTAMP_COL].view(\"int64\") // 10**9\n",
    "        ).astype(df_ts_dtype)\n",
    "    else:\n",
    "        # Both are datetime types - ensure they're compatible\n",
    "        df[TIMESTAMP_COL] = pd.to_datetime(df[TIMESTAMP_COL])\n",
    "        sentiment_df[TIMESTAMP_COL] = pd.to_datetime(sentiment_df[TIMESTAMP_COL])\n",
    "        \n",
    "        # Remove timezone info to make them compatible\n",
    "        if hasattr(df[TIMESTAMP_COL].dtype, 'tz') and df[TIMESTAMP_COL].dtype.tz is not None:\n",
    "            df[TIMESTAMP_COL] = df[TIMESTAMP_COL].dt.tz_localize(None)\n",
    "        if hasattr(sentiment_df[TIMESTAMP_COL].dtype, 'tz') and sentiment_df[TIMESTAMP_COL].dtype.tz is not None:\n",
    "            sentiment_df[TIMESTAMP_COL] = sentiment_df[TIMESTAMP_COL].dt.tz_localize(None)\n",
    "\n",
    "    # Merge sentiment data\n",
    "    df = df.merge(\n",
    "        sentiment_df[[TIMESTAMP_COL, \"sentiment_score\"]],\n",
    "        on=TIMESTAMP_COL,\n",
    "        how=\"left\",\n",
    "    )\n",
    "    coverage = df[\"sentiment_score\"].notna().mean()\n",
    "    df[\"sentiment_score\"] = df[\"sentiment_score\"].fillna(0.0)\n",
    "\n",
    "    print(f\"      ‚úÖ Sentiment merged ‚Äì coverage: {coverage:.1%}\")\n",
    "    return df\n",
    "\n",
    "# ===================== MAIN EXECUTION =====================\n",
    "\n",
    "def run_feature_engineering_pipeline():\n",
    "    \"\"\"Main execution pipeline for data loading and feature engineering.\"\"\"\n",
    "    \n",
    "    print(\"üöÄ Starting data loading and feature engineering...\")\n",
    "    \n",
    "    # Load raw data\n",
    "    df_raw = load_eth_data(DATA_PATH)\n",
    "    \n",
    "    print(\"\\nüîß Feature Engineering Pipeline (15D State Space):\")\n",
    "    df_processed = df_raw.copy()\n",
    "    \n",
    "    # Core features (6D) - 4 calculated here, 2 added by environment\n",
    "    df_processed = calculate_core_features(df_processed)\n",
    "    \n",
    "    # Technical indicators (8D) \n",
    "    df_processed = calculate_technical_indicators(df_processed)\n",
    "    \n",
    "    # Sentiment data (1D)\n",
    "    df_processed = integrate_sentiment_data(df_processed)\n",
    "    \n",
    "    # Define final feature columns for 15D state space\n",
    "    FEATURE_COLUMNS = [\n",
    "        # Core features (6D) - 4 calculated, 2 will be added by environment\n",
    "        'z_score',           # Feature 2: Z-score signal\n",
    "        'zone_norm',         # Feature 3: Normalized zone\n",
    "        'price_momentum',    # Feature 4: Price momentum\n",
    "        'z_score_momentum',  # Feature 5: Z-score momentum\n",
    "        # Features 1 & 6 (position, position_change) added by environment\n",
    "        \n",
    "        # Technical indicators (8D)\n",
    "        'macd_norm',         # Feature 7: MACD line\n",
    "        'macd_signal_norm',  # Feature 8: MACD signal\n",
    "        'macd_diff_norm',    # Feature 9: MACD histogram\n",
    "        'rsi_norm',          # Feature 10: RSI\n",
    "        'bb_mid_norm',       # Feature 11: Bollinger middle\n",
    "        'bb_high_norm',      # Feature 12: Bollinger upper\n",
    "        'bb_low_norm',       # Feature 13: Bollinger lower\n",
    "        'obv_norm',          # Feature 14: OBV\n",
    "        \n",
    "        # Sentiment data (1D)\n",
    "        'sentiment_score'    # Feature 15: Reddit sentiment\n",
    "    ]\n",
    "    \n",
    "    # Verify all features are present\n",
    "    missing_features = [col for col in FEATURE_COLUMNS if col not in df_processed.columns]\n",
    "    if missing_features:\n",
    "        print(f\"\\n‚ùå Missing features: {missing_features}\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ All 15D features successfully calculated!\")\n",
    "    \n",
    "    print(f\"\\nüìä Feature Engineering Summary:\")\n",
    "    print(f\"   üìà Raw data: {len(df_raw):,} rows\")\n",
    "    print(f\"   üîß Processed data: {len(df_processed):,} rows\") \n",
    "    print(f\"   üìã Features: {len(FEATURE_COLUMNS)} dimensions\")\n",
    "    print(f\"   üéØ State space: 15D (as per methodology)\")\n",
    "    print(f\"   üíæ Memory usage: {df_processed.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    # Save processed data\n",
    "    processed_data_path = os.path.join(OUTPUT_DIR, 'processed_eth_data_15d.parquet')\n",
    "    df_processed.to_parquet(processed_data_path, index=False)\n",
    "    print(f\"   üíæ Processed data saved: {processed_data_path}\")\n",
    "    \n",
    "    # Display sample of processed features\n",
    "    print(f\"\\nüìã Sample of 15D Feature Data:\")\n",
    "    sample_features = df_processed[FEATURE_COLUMNS].tail()\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', None)\n",
    "    print(sample_features)\n",
    "    pd.reset_option('display.max_columns')\n",
    "    pd.reset_option('display.width')\n",
    "    \n",
    "    print(f\"\\nüéØ Data ready for 15D state space training!\")\n",
    "    print(f\"üìä Dataset shape: {df_processed.shape}\")\n",
    "    print(f\"üìÖ Ready for temporal data splitting...\")\n",
    "    \n",
    "    return df_processed, FEATURE_COLUMNS\n",
    "\n",
    "# Execute the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    df_processed, feature_columns = run_feature_engineering_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc581bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Applying Temporal Data Splitting Protocol:\n",
      "üîÑ Creating temporal data splits (70/15/15)...\n",
      "   ‚úÖ Training:   1,318,384 rows (70.0%)\n",
      "   ‚úÖ Validation: 282,511 rows (15.0%)\n",
      "   ‚úÖ Test:       282,512 rows (15.0%)\n",
      "   üìÖ Training period:   2022-01-01 to 2024-07-04\n",
      "   üìÖ Validation period: 2024-07-04 to 2025-01-16\n",
      "   üìÖ Test period:       2025-01-16 to 2025-07-31\n",
      "   üíæ Data splits saved to: ./processed_data_15d\n",
      "\n",
      "‚úÖ Temporal data splitting complete!\n",
      "üéØ Ready for rolling window training implementation\n",
      "üìã Next: Custom trading environment with 15D state space\n"
     ]
    }
   ],
   "source": [
    "# ===================== TEMPORAL DATA SPLITTING =====================\n",
    "\n",
    "def create_temporal_data_splits(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Create temporal data splits following methodology:\n",
    "    - 70% Training data (strict chronological order)\n",
    "    - 15% Validation data  \n",
    "    - 15% Test data\n",
    "    \n",
    "    Args:\n",
    "        df: Processed DataFrame with features\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing train, validation, test splits and metadata\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Creating temporal data splits (70/15/15)...\")\n",
    "    \n",
    "    total_rows = len(df)\n",
    "    \n",
    "    # Calculate split indices (strict chronological order)\n",
    "    train_end_idx = int(total_rows * TRAIN_RATIO)\n",
    "    val_end_idx = int(total_rows * (TRAIN_RATIO + VALIDATION_RATIO))\n",
    "    \n",
    "    # Create splits\n",
    "    train_df = df.iloc[:train_end_idx].copy().reset_index(drop=True)\n",
    "    val_df = df.iloc[train_end_idx:val_end_idx].copy().reset_index(drop=True)\n",
    "    test_df = df.iloc[val_end_idx:].copy().reset_index(drop=True)\n",
    "    \n",
    "    # Calculate date ranges for metadata\n",
    "    if TIMESTAMP_COL in df.columns:\n",
    "        train_start = pd.to_datetime(train_df[TIMESTAMP_COL].iloc[0], unit='s')\n",
    "        train_end = pd.to_datetime(train_df[TIMESTAMP_COL].iloc[-1], unit='s')\n",
    "        val_start = pd.to_datetime(val_df[TIMESTAMP_COL].iloc[0], unit='s')\n",
    "        val_end = pd.to_datetime(val_df[TIMESTAMP_COL].iloc[-1], unit='s')\n",
    "        test_start = pd.to_datetime(test_df[TIMESTAMP_COL].iloc[0], unit='s')\n",
    "        test_end = pd.to_datetime(test_df[TIMESTAMP_COL].iloc[-1], unit='s')\n",
    "    else:\n",
    "        train_start = train_end = val_start = val_end = test_start = test_end = None\n",
    "    \n",
    "    # Create metadata\n",
    "    split_info = {\n",
    "        'total_rows': total_rows,\n",
    "        'train_rows': len(train_df),\n",
    "        'val_rows': len(val_df),\n",
    "        'test_rows': len(test_df),\n",
    "        'train_ratio': len(train_df) / total_rows,\n",
    "        'val_ratio': len(val_df) / total_rows,\n",
    "        'test_ratio': len(test_df) / total_rows,\n",
    "        'train_start_date': train_start.isoformat() if train_start else None,\n",
    "        'train_end_date': train_end.isoformat() if train_end else None,\n",
    "        'val_start_date': val_start.isoformat() if val_start else None,\n",
    "        'val_end_date': val_end.isoformat() if val_end else None,\n",
    "        'test_start_date': test_start.isoformat() if test_start else None,\n",
    "        'test_end_date': test_end.isoformat() if test_end else None,\n",
    "        'methodology_compliance': True,\n",
    "        'chronological_order': True\n",
    "    }\n",
    "    \n",
    "    print(f\"   ‚úÖ Training:   {len(train_df):,} rows ({len(train_df)/total_rows:.1%})\")\n",
    "    print(f\"   ‚úÖ Validation: {len(val_df):,} rows ({len(val_df)/total_rows:.1%})\")\n",
    "    print(f\"   ‚úÖ Test:       {len(test_df):,} rows ({len(test_df)/total_rows:.1%})\")\n",
    "    \n",
    "    if train_start:\n",
    "        print(f\"   üìÖ Training period:   {train_start.strftime('%Y-%m-%d')} to {train_end.strftime('%Y-%m-%d')}\")\n",
    "        print(f\"   üìÖ Validation period: {val_start.strftime('%Y-%m-%d')} to {val_end.strftime('%Y-%m-%d')}\")\n",
    "        print(f\"   üìÖ Test period:       {test_start.strftime('%Y-%m-%d')} to {test_end.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    # Save splits to files\n",
    "    train_path = os.path.join(OUTPUT_DIR, 'train_data_15d.parquet')\n",
    "    val_path = os.path.join(OUTPUT_DIR, 'val_data_15d.parquet')\n",
    "    test_path = os.path.join(OUTPUT_DIR, 'test_data_15d.parquet')\n",
    "    split_info_path = os.path.join(OUTPUT_DIR, 'temporal_split_info.json')\n",
    "    \n",
    "    train_df.to_parquet(train_path, index=False)\n",
    "    val_df.to_parquet(val_path, index=False)\n",
    "    test_df.to_parquet(test_path, index=False)\n",
    "    \n",
    "    with open(split_info_path, 'w') as f:\n",
    "        json.dump(split_info, f, indent=2)\n",
    "    \n",
    "    print(f\"   üíæ Data splits saved to: {OUTPUT_DIR}\")\n",
    "    \n",
    "    return {\n",
    "        'train': train_df,\n",
    "        'validation': val_df,\n",
    "        'test': test_df,\n",
    "        'split_info': split_info\n",
    "    }\n",
    "\n",
    "# Apply temporal data splitting\n",
    "print(\"\\nüìä Applying Temporal Data Splitting Protocol:\")\n",
    "data_splits = create_temporal_data_splits(df_processed)\n",
    "\n",
    "# Extract data splits for easy access\n",
    "train_data = data_splits['train']\n",
    "val_data = data_splits['validation'] \n",
    "test_data = data_splits['test']\n",
    "split_metadata = data_splits['split_info']\n",
    "\n",
    "print(f\"\\n‚úÖ Temporal data splitting complete!\")\n",
    "print(f\"üéØ Ready for rolling window training implementation\")\n",
    "print(f\"üìã Next: Custom trading environment with 15D state space\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35cbb3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ EthereumTradingEnvironment class defined\n",
      "üéØ Features: 15D state space + multi-component reward function\n",
      "üìã Next: Model architecture and training pipeline\n"
     ]
    }
   ],
   "source": [
    "# ===================== CUSTOM TRADING ENVIRONMENT (15D STATE SPACE) =====================\n",
    "\n",
    "class EthereumTradingEnvironment(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Ethereum Trading Environment implementing 15D state space methodology.\n",
    "    \n",
    "    Features:\n",
    "    - 15-dimensional state space (6 core + 8 technical + 1 sentiment)\n",
    "    - Multi-component reward function (6 components)\n",
    "    - Continuous action space [-1, 1] with position shift constraints\n",
    "    - Rolling window training support\n",
    "    - Advanced risk management\n",
    "    \n",
    "    State Space (15D):\n",
    "    1. Position: Current portfolio exposure [-1, 1]\n",
    "    2. Z-score: Standardized mean-reversion signal  \n",
    "    3. Normalized Zone: Discrete trading signal [-1, 1]\n",
    "    4. Price Momentum: Last-minute price return\n",
    "    5. Z-score Momentum: Change in Z-score \n",
    "    6. Position Change: Change in portfolio exposure\n",
    "    7-9. MACD: Line, Signal, Histogram (normalized)\n",
    "    10. RSI: Relative Strength Index (normalized)\n",
    "    11-13. Bollinger Bands: Mid, High, Low (normalized)\n",
    "    14. OBV: On-Balance Volume (normalized)\n",
    "    15. Sentiment: Reddit sentiment score (normalized)\n",
    "    \"\"\"\n",
    "    \n",
    "    metadata = {'render.modes': []}\n",
    "    \n",
    "    def __init__(self, \n",
    "                 data: pd.DataFrame,\n",
    "                 feature_columns: List[str],\n",
    "                 reward_config: Dict[str, float] = None,\n",
    "                 episode_length: int = EPISODE_LENGTH,\n",
    "                 lookback_window: int = LOOKBACK_WINDOW,\n",
    "                 initial_capital: float = INITIAL_CAPITAL,\n",
    "                 max_position_shift: float = MAX_POSITION_SHIFT,\n",
    "                 random_start: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize Ethereum Trading Environment.\n",
    "        \n",
    "        Args:\n",
    "            data: DataFrame with processed features\n",
    "            feature_columns: List of feature column names (should be 13, position+position_change added dynamically)\n",
    "            reward_config: Reward function configuration\n",
    "            episode_length: Length of trading episodes (minutes)\n",
    "            lookback_window: Lookback window for state representation\n",
    "            initial_capital: Initial portfolio value\n",
    "            max_position_shift: Maximum position change per step\n",
    "            random_start: Whether to randomize episode start positions\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store data and configuration\n",
    "        self.data = data.copy()\n",
    "        self.feature_columns = feature_columns\n",
    "        self.reward_config = reward_config or REWARD_COMPONENTS\n",
    "        self.episode_length = episode_length\n",
    "        self.lookback_window = lookback_window\n",
    "        self.initial_capital = initial_capital\n",
    "        self.max_position_shift = max_position_shift\n",
    "        self.random_start = random_start\n",
    "        \n",
    "        # Cache price and feature arrays for faster access\n",
    "        self.prices = self.data[PRICE_COL].values\n",
    "        self.timestamps = self.data[TIMESTAMP_COL].values if TIMESTAMP_COL in self.data.columns else None\n",
    "        \n",
    "        # Create feature matrix (13D - position and position_change added dynamically)\n",
    "        self.features = self.data[feature_columns].values\n",
    "        \n",
    "        # Define action and observation spaces\n",
    "        # Action space: Continuous [-1, 1] for target position\n",
    "        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)\n",
    "        \n",
    "        # Observation space: 15D state space\n",
    "        # [position, z_score, zone_norm, price_momentum, z_score_momentum, position_change, \n",
    "        #  macd_norm, macd_signal_norm, macd_diff_norm, rsi_norm, bb_mid_norm, bb_high_norm, bb_low_norm, obv_norm, sentiment_score]\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([-1.0, -5.0, -1.0, -0.1, -2.0, -0.2, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0], dtype=np.float32),\n",
    "            high=np.array([1.0, 5.0, 1.0, 0.1, 2.0, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], dtype=np.float32),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Episode bounds\n",
    "        self.min_start_idx = lookback_window  # Need lookback for state construction\n",
    "        self.max_start_idx = len(self.data) - episode_length - 1\n",
    "        \n",
    "        # Initialize episode state variables\n",
    "        self.reset()\n",
    "        \n",
    "        print(f\"‚úÖ EthereumTradingEnvironment initialized\")\n",
    "        print(f\"   üìä Data length: {len(self.data):,} rows\")\n",
    "        print(f\"   üéØ State space: 15D (6 core + 8 technical + 1 sentiment)\")\n",
    "        print(f\"   üéÆ Action space: Continuous [-1, 1]\")\n",
    "        print(f\"   üìè Episode length: {episode_length} minutes\")\n",
    "        print(f\"   üîô Lookback window: {lookback_window} minutes\")\n",
    "        print(f\"   üí∞ Initial capital: ${initial_capital:,.0f}\")\n",
    "        \n",
    "    def _get_state(self, current_idx: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Construct 15D state representation.\n",
    "        \n",
    "        Args:\n",
    "            current_idx: Current time index\n",
    "            \n",
    "        Returns:\n",
    "            15D state vector\n",
    "        \"\"\"\n",
    "        # Get current features (13D: core + technical + sentiment)\n",
    "        current_features = self.features[current_idx].copy()\n",
    "        \n",
    "        # Add position and position_change to create 15D state\n",
    "        state = np.zeros(15, dtype=np.float32)\n",
    "        \n",
    "        # 1. Position\n",
    "        state[0] = self.position\n",
    "        \n",
    "        # 2-5. Core features (z_score, zone_norm, price_momentum, z_score_momentum)\n",
    "        state[1:5] = current_features[:4]\n",
    "        \n",
    "        # 6. Position change\n",
    "        state[5] = self.position_change\n",
    "        \n",
    "        # 7-14. Technical indicators (8D)\n",
    "        state[6:14] = current_features[4:12]\n",
    "        \n",
    "        # 15. Sentiment score\n",
    "        state[14] = current_features[12]\n",
    "        \n",
    "        # Ensure state is within bounds\n",
    "        state = np.clip(state, self.observation_space.low, self.observation_space.high)\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def _calculate_multi_component_reward(self, action: float, current_idx: int) -> Tuple[float, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Calculate multi-component reward as per methodology.\n",
    "        \n",
    "        Components:\n",
    "        1. Primary: Profit-and-Loss (PnL)\n",
    "        2. Risk-Adjusted: Differential Sharpe Ratio  \n",
    "        3. Transaction Costs Penalty\n",
    "        4. Drawdown Penalty\n",
    "        5. Holding Reward/Penalty\n",
    "        6. Activity Incentives\n",
    "        \n",
    "        Args:\n",
    "            action: Target position [-1, 1]\n",
    "            current_idx: Current time index\n",
    "            \n",
    "        Returns:\n",
    "            Total reward and component breakdown\n",
    "        \"\"\"\n",
    "        # Apply position shift constraint\n",
    "        target_position = np.clip(action, -1.0, 1.0)\n",
    "        position_change = target_position - self.position\n",
    "        \n",
    "        # Enforce maximum position shift per minute\n",
    "        if abs(position_change) > self.max_position_shift:\n",
    "            position_change = np.sign(position_change) * self.max_position_shift\n",
    "            target_position = self.position + position_change\n",
    "        \n",
    "        # Calculate price change for PnL\n",
    "        if current_idx + 1 < len(self.prices):\n",
    "            price_change = (self.prices[current_idx + 1] - self.prices[current_idx]) / self.prices[current_idx]\n",
    "        else:\n",
    "            price_change = 0.0\n",
    "        \n",
    "        # Component 1: Primary Reward - Profit-and-Loss\n",
    "        position_return = self.position * price_change\n",
    "        portfolio_change = position_return * self.portfolio_value\n",
    "        \n",
    "        if self.reward_config['pnl_normalization'] == 'nav':\n",
    "            pnl_reward = (portfolio_change / self.initial_capital) * self.reward_config['pnl_scale']\n",
    "        else:\n",
    "            pnl_reward = portfolio_change * self.reward_config['pnl_scale']\n",
    "        \n",
    "        # Component 2: Risk-Adjusted Return (Simplified Sharpe)\n",
    "        if len(self.return_history) > 1:\n",
    "            returns_array = np.array(self.return_history[-self.reward_config['sharpe_window']:])\n",
    "            if len(returns_array) > 1 and np.std(returns_array) > 0:\n",
    "                sharpe_ratio = np.mean(returns_array) / np.std(returns_array)\n",
    "                sharpe_reward = sharpe_ratio * self.reward_config['sharpe_weight']\n",
    "            else:\n",
    "                sharpe_reward = 0.0\n",
    "        else:\n",
    "            sharpe_reward = 0.0\n",
    "        \n",
    "        # Component 3: Transaction Costs Penalty\n",
    "        trade_cost = abs(position_change) * (self.reward_config['fee_rate'] + self.reward_config['slippage'])\n",
    "        transaction_penalty = -trade_cost * self.portfolio_value * self.reward_config['transaction_penalty']\n",
    "        \n",
    "        # Component 4: Drawdown Penalty\n",
    "        current_nav = self.portfolio_value / self.initial_capital\n",
    "        if current_nav < (self.peak_nav * (1 - self.reward_config['drawdown_threshold'])):\n",
    "            drawdown_penalty = -self.reward_config['drawdown_penalty']\n",
    "        else:\n",
    "            drawdown_penalty = 0.0\n",
    "        \n",
    "        # Component 5: Holding Reward/Penalty\n",
    "        if abs(self.position) > 0.1:  # In position\n",
    "            if position_return > 0:\n",
    "                holding_reward = self.reward_config['holding_reward']\n",
    "            else:\n",
    "                holding_reward = -self.reward_config['holding_penalty']\n",
    "        else:   \n",
    "            holding_reward = 0.0\n",
    "        \n",
    "        # Add penalty for holding too long\n",
    "        if self.steps_in_position > self.reward_config['max_hold_periods']:\n",
    "            holding_reward -= self.reward_config['holding_penalty'] * 2\n",
    "        \n",
    "        # Component 6: Activity Incentives\n",
    "        if abs(position_change) > 0.01:  # Taking action\n",
    "            activity_reward = self.reward_config['activity_reward']\n",
    "            self.inactive_steps = 0\n",
    "        else:\n",
    "            self.inactive_steps += 1\n",
    "            # Escalating penalty for inactivity\n",
    "            activity_reward = -self.reward_config['inactivity_penalty'] * (1 + self.inactive_steps * 0.1)\n",
    "\n",
    "        \n",
    "        # Optional Component 7: Sentiment Alignment\n",
    "        sentiment_reward = 0.0\n",
    "        if self.reward_config.get('enable_sentiment_reward', False):\n",
    "            sentiment_score = float(self.features[current_idx, -1])\n",
    "            weight = self.reward_config.get('sentiment_reward_weight', 0.0)\n",
    "            normalized_position = target_position / self.max_position_shift\n",
    "            sentiment_reward = sentiment_score * weight * normalized_position\n",
    "            \n",
    "        # Total reward\n",
    "        total_reward = (pnl_reward + sharpe_reward + transaction_penalty + \n",
    "                       drawdown_penalty + holding_reward + activity_reward + sentiment_reward)\n",
    "        \n",
    "        # Update position and portfolio\n",
    "        self.position = target_position\n",
    "        self.position_change = position_change\n",
    "        self.portfolio_value += portfolio_change - abs(trade_cost * self.portfolio_value)\n",
    "        \n",
    "        # Update tracking variables\n",
    "        self.return_history.append(position_return)\n",
    "        if len(self.return_history) > self.reward_config['sharpe_window']:\n",
    "            self.return_history.pop(0)\n",
    "        \n",
    "        current_nav = self.portfolio_value / self.initial_capital\n",
    "        if current_nav > self.peak_nav:\n",
    "            self.peak_nav = current_nav\n",
    "        \n",
    "        if abs(self.position) > 0.1:\n",
    "            self.steps_in_position += 1\n",
    "        else:\n",
    "            self.steps_in_position = 0\n",
    "        \n",
    "        # Component breakdown for analysis\n",
    "        reward_breakdown = {\n",
    "            'pnl_reward': pnl_reward,\n",
    "            'sharpe_reward': sharpe_reward,\n",
    "            'transaction_penalty': transaction_penalty,\n",
    "            'drawdown_penalty': drawdown_penalty,\n",
    "            'holding_reward': holding_reward,\n",
    "            'activity_reward': activity_reward,\n",
    "            'total_reward': total_reward,\n",
    "            'portfolio_value': self.portfolio_value,\n",
    "            'position': self.position,\n",
    "            'nav': current_nav\n",
    "        }\n",
    "        \n",
    "        return total_reward, reward_breakdown\n",
    "    \n",
    "    def reset(self, seed: Optional[int] = None, options: Optional[Dict] = None) -> Tuple[np.ndarray, Dict]:\n",
    "        \"\"\"Reset environment for new episode.\"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # Randomize episode start if enabled\n",
    "        if self.random_start:\n",
    "            self.current_idx = self.np_random.integers(self.min_start_idx, self.max_start_idx)\n",
    "        else:\n",
    "            self.current_idx = self.min_start_idx\n",
    "        \n",
    "        # Initialize episode state\n",
    "        self.step_count = 0\n",
    "        self.position = 0.0\n",
    "        self.position_change = 0.0\n",
    "        self.portfolio_value = self.initial_capital\n",
    "        self.peak_nav = 1.0\n",
    "        self.steps_in_position = 0\n",
    "        self.inactive_steps = 0\n",
    "        self.return_history = []\n",
    "        \n",
    "        # Get initial state\n",
    "        initial_state = self._get_state(self.current_idx)\n",
    "        \n",
    "        \n",
    "        info = {\n",
    "            'portfolio_value': self.portfolio_value,\n",
    "            'position': self.position,\n",
    "            'nav': self.portfolio_value / self.initial_capital,\n",
    "            'episode_start_idx': self.current_idx\n",
    "        }\n",
    "        \n",
    "        return initial_state, info\n",
    "    \n",
    "    def step(self, action: Union[np.ndarray, float]) -> Tuple[np.ndarray, float, bool, bool, Dict]:\n",
    "        \"\"\"Execute one trading step.\"\"\"\n",
    "        \n",
    "        # Handle action input\n",
    "        if isinstance(action, np.ndarray):\n",
    "            action_value = float(action[0])\n",
    "        else:\n",
    "            action_value = float(action)\n",
    "        \n",
    "        # Calculate reward and update state\n",
    "        reward, reward_breakdown = self._calculate_multi_component_reward(action_value, self.current_idx)\n",
    "        \n",
    "        # Move to next time step\n",
    "        self.current_idx += 1\n",
    "        self.step_count += 1\n",
    "        \n",
    "        # Check termination conditions\n",
    "        terminated = (self.step_count >= self.episode_length or \n",
    "                     self.current_idx >= len(self.data) - 1 or\n",
    "                     self.portfolio_value <= 0.1 * self.initial_capital)  # Stop loss at 90% loss\n",
    "        \n",
    "        truncated = False\n",
    "        \n",
    "        # Get next state\n",
    "        if not terminated:\n",
    "            next_state = self._get_state(self.current_idx)\n",
    "        else:\n",
    "            next_state = self._get_state(self.current_idx - 1)  # Use last valid state\n",
    "\n",
    "        current_features = self.features[self.current_idx - 1] if self.current_idx > 0 else self.features[self.current_idx]\n",
    "        sentiment_val = current_features[-1]\n",
    "\n",
    "        # Prepare info dictionary\n",
    "        info = {\n",
    "            **reward_breakdown,\n",
    "            'step_count': self.step_count,\n",
    "            'current_idx': self.current_idx,\n",
    "            'episode_progress': self.step_count / self.episode_length,\n",
    "            'price': float(self.prices[self.current_idx - 1]) if self.current_idx > 0 else float(self.prices[self.current_idx]),\n",
    "            'sentiment': sentiment_val\n",
    "        }\n",
    "        \n",
    "        return next_state, reward, terminated, truncated, info\n",
    "\n",
    "print(\"‚úÖ EthereumTradingEnvironment class defined\")\n",
    "print(\"üéØ Features: 15D state space + multi-component reward function\")\n",
    "print(\"üìã Next: Model architecture and training pipeline\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0cae80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Initializing Model Training Pipeline:\n",
      "Using device: cpu\n",
      "‚úÖ ModelTrainer initialized\n",
      "   üìä Training data: 1,318,384 rows\n",
      "   üìä Validation data: 282,511 rows\n",
      "   üìä Test data: 282,512 rows\n",
      "   üéØ Features: 13 dimensions\n",
      "‚úÖ Model training pipeline ready!\n",
      "üìã Next: Bulk hyperparameter configuration testing\n"
     ]
    }
   ],
   "source": [
    "# ===================== MODEL ARCHITECTURE & TRAINING PIPELINE =====================\n",
    "\n",
    "class ModelTrainer:\n",
    "    \"\"\"\n",
    "    Model trainer implementing rolling window training protocol as per methodology.\n",
    "    \n",
    "    Features:\n",
    "    - Rolling window training (6-12 months)\n",
    "    - A2C and TD3 model support\n",
    "    - Hyperparameter configuration loading\n",
    "    - Performance tracking and evaluation\n",
    "    - Model persistence and versioning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 train_data: pd.DataFrame,\n",
    "                 val_data: pd.DataFrame,\n",
    "                 test_data: pd.DataFrame,\n",
    "                 feature_columns: List[str]):\n",
    "        \"\"\"\n",
    "        Initialize ModelTrainer.\n",
    "        \n",
    "        Args:\n",
    "            train_data: Training dataset\n",
    "            val_data: Validation dataset  \n",
    "            test_data: Test dataset\n",
    "            feature_columns: List of feature columns\n",
    "        \"\"\"\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.test_data = test_data\n",
    "        self.feature_columns = feature_columns\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = \"cuda\"\n",
    "        else:\n",
    "            self.device = \"cpu\"\n",
    "\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        \n",
    "        print(f\"‚úÖ ModelTrainer initialized\")\n",
    "        print(f\"   üìä Training data: {len(train_data):,} rows\")\n",
    "        print(f\"   üìä Validation data: {len(val_data):,} rows\")\n",
    "        print(f\"   üìä Test data: {len(test_data):,} rows\")\n",
    "        print(f\"   üéØ Features: {len(feature_columns)} dimensions\")\n",
    "    \n",
    "    def create_environment(self, \n",
    "                          data: pd.DataFrame, \n",
    "                          reward_config: Dict[str, float] = None,\n",
    "                          random_start: bool = True) -> DummyVecEnv:\n",
    "        \"\"\"Create vectorized environment for training/evaluation.\"\"\"\n",
    "        \n",
    "        def make_env():\n",
    "            env = EthereumTradingEnvironment(\n",
    "                data=data,\n",
    "                feature_columns=self.feature_columns,\n",
    "                reward_config=reward_config,\n",
    "                random_start=random_start\n",
    "            )\n",
    "            return Monitor(env)\n",
    "        \n",
    "        return DummyVecEnv([make_env])\n",
    "    \n",
    "    def train_a2c_model(self, \n",
    "                       config: Dict[str, Any],\n",
    "                       reward_config: Dict[str, float] = None,\n",
    "                       train_data: pd.DataFrame = None,\n",
    "                       val_data: pd.DataFrame = None) -> Tuple[A2C, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Train A2C model with given configuration and optional reward override.\n",
    "        \n",
    "        Args:\n",
    "            config: Model configuration dictionary\n",
    "            train_data: Training data (optional, uses self.train_data if None)\n",
    "            val_data: Validation data (optional, uses self.val_data if None)\n",
    "            \n",
    "        Returns:\n",
    "            Trained model and performance metrics\n",
    "        \"\"\"\n",
    "        train_data = train_data or self.train_data\n",
    "        val_data = val_data or self.val_data\n",
    "\n",
    "        reward_config = reward_config or config.get('reward_components')\n",
    "        \n",
    "        print(f\"üöÄ Training A2C model: {config.get('config_id', 'default')}\")\n",
    "        \n",
    "        # Create environments\n",
    "        train_env = self.create_environment(train_data, reward_config)\n",
    "        val_env = self.create_environment(val_data, reward_config, random_start=False)\n",
    "\n",
    "        \n",
    "        # Extract model parameters\n",
    "        model_params = config.get('model_params', A2C_PARAMS)\n",
    "        training_params = config.get('training', TRAINING_CONFIG)\n",
    "\n",
    "\n",
    "        \n",
    "        # Create A2C model\n",
    "        model = A2C(\n",
    "            policy=\"MlpPolicy\",\n",
    "            env=train_env,\n",
    "            device=self.device,\n",
    "            learning_rate=model_params.get('learning_rate', A2C_PARAMS['learning_rate']),\n",
    "            n_steps=model_params.get('n_steps', A2C_PARAMS['n_steps']),\n",
    "            gamma=model_params.get('gamma', A2C_PARAMS['gamma']),\n",
    "            gae_lambda=model_params.get('gae_lambda', A2C_PARAMS['gae_lambda']),\n",
    "            ent_coef=model_params.get('ent_coef', A2C_PARAMS['ent_coef']),\n",
    "            vf_coef=model_params.get('vf_coef', A2C_PARAMS['vf_coef']),\n",
    "            max_grad_norm=model_params.get('max_grad_norm', A2C_PARAMS['max_grad_norm']),\n",
    "            verbose=0,\n",
    "            seed=RANDOM_SEED\n",
    "        )\n",
    "        \n",
    "        # Set up evaluation callback\n",
    "        eval_callback = EvalCallback(\n",
    "            val_env,\n",
    "            best_model_save_path=None,  # We'll save manually\n",
    "            log_path=None,\n",
    "            eval_freq=training_params.get('eval_freq', TRAINING_CONFIG['eval_freq']),\n",
    "            n_eval_episodes=training_params.get('n_eval_episodes', TRAINING_CONFIG['n_eval_episodes']),\n",
    "            deterministic=True,\n",
    "            render=False,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        total_timesteps = training_params.get('total_timesteps', TRAINING_CONFIG['total_timesteps'])\n",
    "        start_time = time.time()\n",
    "        \n",
    "        model.learn(\n",
    "            total_timesteps=total_timesteps,\n",
    "            callback=eval_callback,\n",
    "            tb_log_name=f\"a2c_{config.get('config_id', 'default')}\"\n",
    "        )\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Evaluate model performance\n",
    "        mean_reward = self.evaluate_model(model, val_env, n_episodes=5)\n",
    "        \n",
    "        performance_metrics = {\n",
    "            'mean_reward': mean_reward,\n",
    "            'training_time': training_time,\n",
    "            'total_timesteps': total_timesteps,\n",
    "            'config_id': config.get('config_id', 'default')\n",
    "        }\n",
    "        \n",
    "        print(f\"   ‚úÖ Training complete: {training_time:.1f}s, Mean reward: {mean_reward:.4f}\")\n",
    "        \n",
    "        return model, performance_metrics\n",
    "    \n",
    "    def train_td3_model(self, \n",
    "                       config: Dict[str, Any],\n",
    "                       reward_config: Dict[str, float] = None,\n",
    "                       train_data: pd.DataFrame = None,\n",
    "                       val_data: pd.DataFrame = None) -> Tuple[TD3, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Train TD3 model with given configuration.\n",
    "        \n",
    "        Args:\n",
    "            config: Model configuration dictionary\n",
    "            train_data: Training data (optional)\n",
    "            val_data: Validation data (optional)\n",
    "            \n",
    "        Returns:\n",
    "            Trained model and performance metrics\n",
    "        \"\"\"\n",
    "        train_data = train_data or self.train_data\n",
    "        val_data = val_data or self.val_data\n",
    "\n",
    "        reward_config = reward_config or config.get('reward_components')\n",
    "        \n",
    "        print(f\"üöÄ Training TD3 model: {config.get('config_id', 'default')}\")\n",
    "        \n",
    "        # Create environments\n",
    "        train_env = self.create_environment(train_data, reward_config)\n",
    "        val_env = self.create_environment(val_data, reward_config, random_start=False)\n",
    "        \n",
    "        # Extract model parameters\n",
    "        model_params = config.get('model_params', TD3_PARAMS)\n",
    "        training_params = config.get('training', TRAINING_CONFIG)\n",
    "        \n",
    "        # Create action noise\n",
    "        action_noise = NormalActionNoise(\n",
    "            mean=np.zeros(1), \n",
    "            sigma=model_params.get('noise_std', TD3_PARAMS['noise_std']) * np.ones(1)\n",
    "        )\n",
    "        \n",
    "        # Create TD3 model\n",
    "        model = TD3(\n",
    "            policy=\"MlpPolicy\",\n",
    "            env=train_env,\n",
    "            device=self.device,\n",
    "            learning_rate=model_params.get('learning_rate', TD3_PARAMS['learning_rate']),\n",
    "            buffer_size=model_params.get('buffer_size', TD3_PARAMS['buffer_size']),\n",
    "            learning_starts=model_params.get('learning_starts', TD3_PARAMS['learning_starts']),\n",
    "            batch_size=model_params.get('batch_size', TD3_PARAMS['batch_size']),\n",
    "            tau=model_params.get('tau', TD3_PARAMS['tau']),\n",
    "            gamma=model_params.get('gamma', TD3_PARAMS['gamma']),\n",
    "            train_freq=model_params.get('train_freq', TD3_PARAMS['train_freq']),\n",
    "            gradient_steps=model_params.get('gradient_steps', TD3_PARAMS['gradient_steps']),\n",
    "            action_noise=action_noise,\n",
    "            target_policy_noise=model_params.get('target_noise', TD3_PARAMS['target_noise']),\n",
    "            target_noise_clip=model_params.get('noise_clip', TD3_PARAMS['noise_clip']),\n",
    "            policy_delay=model_params.get('policy_delay', TD3_PARAMS['policy_delay']),\n",
    "            verbose=0,\n",
    "            seed=RANDOM_SEED\n",
    "        )\n",
    "        \n",
    "        # Set up evaluation callback\n",
    "        eval_callback = EvalCallback(\n",
    "            val_env,\n",
    "            best_model_save_path=None,\n",
    "            log_path=None,\n",
    "            eval_freq=training_params.get('eval_freq', TRAINING_CONFIG['eval_freq']),\n",
    "            n_eval_episodes=training_params.get('n_eval_episodes', TRAINING_CONFIG['n_eval_episodes']),\n",
    "            deterministic=True,\n",
    "            render=False,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        total_timesteps = training_params.get('total_timesteps', TRAINING_CONFIG['total_timesteps'])\n",
    "        start_time = time.time()\n",
    "        \n",
    "        model.learn(\n",
    "            total_timesteps=total_timesteps,\n",
    "            callback=eval_callback,\n",
    "            tb_log_name=f\"td3_{config.get('config_id', 'default')}\"\n",
    "        )\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Evaluate model performance\n",
    "        mean_reward = self.evaluate_model(model, val_env, n_episodes=5)\n",
    "        \n",
    "        performance_metrics = {\n",
    "            'mean_reward': mean_reward,\n",
    "            'training_time': training_time,\n",
    "            'total_timesteps': total_timesteps,\n",
    "            'config_id': config.get('config_id', 'default')\n",
    "        }\n",
    "        \n",
    "        print(f\"   ‚úÖ Training complete: {training_time:.1f}s, Mean reward: {mean_reward:.4f}\")\n",
    "        \n",
    "        return model, performance_metrics\n",
    "    \n",
    "    def evaluate_model(self, model, env, n_episodes: int = 10) -> float:\n",
    "        \"\"\"Evaluate model performance.\"\"\"\n",
    "        episode_rewards = []\n",
    "        \n",
    "        for _ in range(n_episodes):\n",
    "            obs = env.reset()\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "                obs, reward, done, info = env.step(action)\n",
    "                episode_reward += reward[0]\n",
    "            \n",
    "            episode_rewards.append(episode_reward)\n",
    "        \n",
    "        return np.mean(episode_rewards)\n",
    "\n",
    "def rolling_window_training(trainer: ModelTrainer,\n",
    "                           config: Dict[str, Any],\n",
    "                           rolling_months: int = ROLLING_WINDOW_MONTHS,\n",
    "                           eval_months: int = EVALUATION_PERIOD_MONTHS) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Implement rolling window training as per methodology.\n",
    "    \n",
    "    Args:\n",
    "        trainer: ModelTrainer instance\n",
    "        config: Model configuration\n",
    "        rolling_months: Rolling window size in months\n",
    "        eval_months: Evaluation period in months\n",
    "        \n",
    "    Returns:\n",
    "        List of performance results for each window\n",
    "    \"\"\"\n",
    "    print(f\"üîÑ Starting rolling window training\")\n",
    "    print(f\"   üìÖ Rolling window: {rolling_months} months\")\n",
    "    print(f\"   üìä Evaluation period: {eval_months} months\")\n",
    "    \n",
    "    # Calculate time windows (simplified - using row indices)\n",
    "    total_rows = len(trainer.train_data)\n",
    "    minutes_per_month = 30 * 24 * 60  # Approximate\n",
    "    \n",
    "    rolling_window_size = rolling_months * minutes_per_month\n",
    "    eval_window_size = eval_months * minutes_per_month\n",
    "    \n",
    "    results = []\n",
    "    window_start = 0\n",
    "    \n",
    "    while window_start + rolling_window_size + eval_window_size <= total_rows:\n",
    "        # Define current windows\n",
    "        train_window_end = window_start + rolling_window_size\n",
    "        eval_window_end = train_window_end + eval_window_size\n",
    "        \n",
    "        # Extract data for current window\n",
    "        current_train_data = trainer.train_data.iloc[window_start:train_window_end].copy()\n",
    "        current_eval_data = trainer.train_data.iloc[train_window_end:eval_window_end].copy()\n",
    "        \n",
    "        print(f\"   üìä Window {len(results)+1}: Training [{window_start}:{train_window_end}], Eval [{train_window_end}:{eval_window_end}]\")\n",
    "        \n",
    "        # Train model for current window\n",
    "        if config['algorithm'] == 'A2C':\n",
    "            model, metrics = trainer.train_a2c_model(config, current_train_data, current_eval_data)\n",
    "        elif config['algorithm'] == 'TD3':\n",
    "            model, metrics = trainer.train_td3_model(config, current_train_data, current_eval_data)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported algorithm: {config['algorithm']}\")\n",
    "        \n",
    "        # Add window information to metrics\n",
    "        metrics.update({\n",
    "            'window_index': len(results) + 1,\n",
    "            'train_start_idx': window_start,\n",
    "            'train_end_idx': train_window_end,\n",
    "            'eval_start_idx': train_window_end,\n",
    "            'eval_end_idx': eval_window_end,\n",
    "            'train_rows': len(current_train_data),\n",
    "            'eval_rows': len(current_eval_data)\n",
    "        })\n",
    "        \n",
    "        results.append(metrics)\n",
    "        \n",
    "        # Move to next window\n",
    "        window_start += eval_window_size  # Step by evaluation period\n",
    "        \n",
    "        # Save model for this window\n",
    "        model_path = os.path.join(MODEL_DIR, f\"{config['config_id']}_window_{len(results)}\")\n",
    "        model.save(model_path)\n",
    "        \n",
    "        print(f\"      ‚úÖ Window {len(results)} complete, model saved to: {model_path}\")\n",
    "    \n",
    "    print(f\"üéâ Rolling window training complete! {len(results)} windows processed\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Initialize model trainer\n",
    "print(\"\\nüß† Initializing Model Training Pipeline:\")\n",
    "trainer = ModelTrainer(train_data, val_data, test_data, feature_columns)\n",
    "\n",
    "print(\"‚úÖ Model training pipeline ready!\")\n",
    "print(\"üìã Next: Bulk hyperparameter configuration testing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4db660a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¨ Initializing Bulk Configuration Testing Pipeline:\n",
      "   ‚úÖ Loaded 202 configurations\n",
      "   üìä Algorithms: ['A2C']\n",
      "‚úÖ BulkConfigurationTester initialized\n",
      "   üìÅ Config file: ./drl_training_configs.json\n",
      "   üî¢ Configurations loaded: 202\n",
      "‚úÖ Bulk configuration testing pipeline ready!\n",
      "üîç Ready to test 1000 hyperparameter combinations\n",
      "üìã Next: Evaluation and visualization sections\n"
     ]
    }
   ],
   "source": [
    "# ===================== BULK HYPERPARAMETER CONFIGURATION TESTING =====================\n",
    "\n",
    "class BulkConfigurationTester:\n",
    "    \"\"\"\n",
    "    Bulk hyperparameter configuration testing for reward function optimization.\n",
    "    \n",
    "    Features:\n",
    "    - Load and test 1000 generated configurations\n",
    "    - Parallel processing for efficiency\n",
    "    - Performance tracking and ranking\n",
    "    - Statistical analysis of results\n",
    "    - Best configuration identification\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, trainer: ModelTrainer, config_file: str = CONFIG_FILE):\n",
    "        \"\"\"\n",
    "        Initialize BulkConfigurationTester.\n",
    "        \n",
    "        Args:\n",
    "            trainer: ModelTrainer instance\n",
    "            config_file: Path to configuration file\n",
    "        \"\"\"\n",
    "        self.trainer = trainer\n",
    "        self.config_file = config_file\n",
    "        self.configurations = []\n",
    "        self.results = []\n",
    "        \n",
    "        # Load configurations\n",
    "        self.load_configurations()\n",
    "        \n",
    "        print(f\"‚úÖ BulkConfigurationTester initialized\")\n",
    "        print(f\"   üìÅ Config file: {config_file}\")\n",
    "        print(f\"   üî¢ Configurations loaded: {len(self.configurations)}\")\n",
    "    \n",
    "    def load_configurations(self):\n",
    "        \"\"\"Load hyperparameter configurations from JSON file.\"\"\"\n",
    "        try:\n",
    "            with open(self.config_file, 'r') as f:\n",
    "                config_data = json.load(f)\n",
    "                self.configurations = config_data['configurations']\n",
    "                self.metadata = config_data['metadata']\n",
    "                \n",
    "            print(f\"   ‚úÖ Loaded {len(self.configurations)} configurations\")\n",
    "            print(f\"   üìä Algorithms: {self.metadata.get('algorithms', ['A2C', 'TD3'])}\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"   ‚ö†Ô∏è Configuration file not found: {self.config_file}\")\n",
    "            print(f\"   üîÑ Generating configurations using script...\")\n",
    "            \n",
    "            # Run the configuration generator\n",
    "            os.system(f\"cd {os.path.dirname(self.config_file) or '.'} && python generate_drl_config.py\")\n",
    "            \n",
    "            # Try loading again\n",
    "            with open(self.config_file, 'r') as f:\n",
    "                config_data = json.load(f)\n",
    "                self.configurations = config_data['configurations']\n",
    "                self.metadata = config_data['metadata']\n",
    "                \n",
    "            print(f\"   ‚úÖ Generated and loaded {len(self.configurations)} configurations\")\n",
    "    \n",
    "    def test_single_configuration(self, config: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Test a single configuration.\n",
    "        \n",
    "        Args:\n",
    "            config: Configuration dictionary\n",
    "            \n",
    "        Returns:\n",
    "            Results dictionary with performance metrics\n",
    "        \"\"\"\n",
    "        try:\n",
    "            config_id = config.get('config_id', 'unknown')\n",
    "            algorithm = config.get('algorithm', 'A2C')\n",
    "            \n",
    "            # Train model with configuration\n",
    "            if algorithm == 'A2C':\n",
    "                model, metrics = self.trainer.train_a2c_model(config)\n",
    "            elif algorithm == 'TD3':\n",
    "                model, metrics = self.trainer.train_td3_model(config)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported algorithm: {algorithm}\")\n",
    "            \n",
    "            # Test on holdout test set\n",
    "            test_env = self.trainer.create_environment(\n",
    "                self.trainer.test_data, \n",
    "                config.get('reward_components'),\n",
    "                random_start=False\n",
    "            )\n",
    "            \n",
    "            test_reward = self.trainer.evaluate_model(model, test_env, n_episodes=3)\n",
    "            \n",
    "            # Calculate comprehensive performance metrics\n",
    "            results = {\n",
    "                'config_id': config_id,\n",
    "                'algorithm': algorithm,\n",
    "                'validation_reward': metrics['mean_reward'],\n",
    "                'test_reward': test_reward,\n",
    "                'training_time': metrics['training_time'],\n",
    "                'total_timesteps': metrics['total_timesteps'],\n",
    "                'reward_components': config.get('reward_components', {}),\n",
    "                'model_params': config.get('model_params', {}),\n",
    "                'training_params': config.get('training', {}),\n",
    "                'risk_management': config.get('risk_management', {}),\n",
    "                'performance_score': test_reward,  # Primary metric for ranking\n",
    "                'status': 'success'\n",
    "            }\n",
    "            \n",
    "            print(f\"   ‚úÖ {config_id}: Val={metrics['mean_reward']:.4f}, Test={test_reward:.4f}\")\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå {config.get('config_id', 'unknown')}: Error - {str(e)}\")\n",
    "            return {\n",
    "                'config_id': config.get('config_id', 'unknown'),\n",
    "                'algorithm': config.get('algorithm', 'unknown'),\n",
    "                'status': 'failed',\n",
    "                'error': str(e),\n",
    "                'performance_score': float('-inf')\n",
    "            }\n",
    "    \n",
    "    def test_configuration_batch(self, configs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Test a batch of configurations.\"\"\"\n",
    "        batch_results = []\n",
    "        \n",
    "        for i, config in enumerate(configs):\n",
    "            print(f\"   üîÑ Testing config {i+1}/{len(configs)}: {config.get('config_id', 'unknown')}\")\n",
    "            result = self.test_single_configuration(config)\n",
    "            batch_results.append(result)\n",
    "        \n",
    "        return batch_results\n",
    "    \n",
    "    def run_bulk_testing(self, \n",
    "                        max_configs: int = None,\n",
    "                        parallel: bool = True,\n",
    "                        batch_size: int = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Run bulk configuration testing.\n",
    "        \n",
    "        Args:\n",
    "            max_configs: Maximum number of configurations to test (None = all)\n",
    "            parallel: Whether to use parallel processing\n",
    "            batch_size: Batch size for processing\n",
    "            \n",
    "        Returns:\n",
    "            List of results\n",
    "        \"\"\"\n",
    "        # Determine configurations to test\n",
    "        configs_to_test = self.configurations[:max_configs] if max_configs else self.configurations\n",
    "        batch_size = batch_size or BULK_CONFIG['config_batch_size']\n",
    "        \n",
    "        print(f\"üöÄ Starting bulk configuration testing\")\n",
    "        print(f\"   üî¢ Total configurations: {len(configs_to_test)}\")\n",
    "        print(f\"   ‚ö° Parallel processing: {parallel}\")\n",
    "        print(f\"   üì¶ Batch size: {batch_size}\")\n",
    "        print(f\"   üíª Max parallel jobs: {BULK_CONFIG['max_parallel_jobs']}\")\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        if parallel and len(configs_to_test) > 1:\n",
    "            # Parallel processing using batches\n",
    "            config_batches = [configs_to_test[i:i+batch_size] \n",
    "                            for i in range(0, len(configs_to_test), batch_size)]\n",
    "            \n",
    "            print(f\"   üìä Processing {len(config_batches)} batches...\")\n",
    "            \n",
    "            # Process batches sequentially (parallel within each batch would be complex)\n",
    "            for batch_idx, batch_configs in enumerate(config_batches):\n",
    "                print(f\"\\n   üì¶ Batch {batch_idx+1}/{len(config_batches)}: {len(batch_configs)} configs\")\n",
    "                \n",
    "                batch_results = self.test_configuration_batch(batch_configs)\n",
    "                all_results.extend(batch_results)\n",
    "                \n",
    "                # Save intermediate results\n",
    "                self.save_intermediate_results(all_results, batch_idx)\n",
    "                \n",
    "                print(f\"      ‚úÖ Batch {batch_idx+1} complete\")\n",
    "        \n",
    "        else:\n",
    "            # Sequential processing\n",
    "            print(\"   üìä Processing configurations sequentially...\")\n",
    "            all_results = self.test_configuration_batch(configs_to_test)\n",
    "        \n",
    "        # Store results\n",
    "        self.results = all_results\n",
    "        \n",
    "        # Analyze and summarize results\n",
    "        self.analyze_results()\n",
    "        \n",
    "        print(f\"\\nüéâ Bulk testing complete!\")\n",
    "        print(f\"   ‚úÖ Successful configs: {len([r for r in all_results if r['status'] == 'success'])}\")\n",
    "        print(f\"   ‚ùå Failed configs: {len([r for r in all_results if r['status'] == 'failed'])}\")\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def save_intermediate_results(self, results: List[Dict[str, Any]], batch_idx: int):\n",
    "        \"\"\"Save intermediate results to prevent data loss.\"\"\"\n",
    "        results_file = os.path.join(OUTPUT_DIR, f'intermediate_results_batch_{batch_idx+1}.json')\n",
    "        \n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump({\n",
    "                'metadata': {\n",
    "                    'batch_index': batch_idx + 1,\n",
    "                    'total_results': len(results),\n",
    "                    'timestamp': datetime.now().isoformat(),\n",
    "                    'successful_configs': len([r for r in results if r['status'] == 'success']),\n",
    "                    'failed_configs': len([r for r in results if r['status'] == 'failed'])\n",
    "                },\n",
    "                'results': results\n",
    "            }, f, indent=2)\n",
    "    \n",
    "    def analyze_results(self):\n",
    "        \"\"\"Analyze and rank results.\"\"\"\n",
    "        successful_results = [r for r in self.results if r['status'] == 'success']\n",
    "        \n",
    "        if not successful_results:\n",
    "            print(\"   ‚ö†Ô∏è No successful results to analyze\")\n",
    "            return\n",
    "        \n",
    "        # Sort by performance score (descending)\n",
    "        successful_results.sort(key=lambda x: x['performance_score'], reverse=True)\n",
    "        \n",
    "        # Top 10 configurations\n",
    "        top_configs = successful_results[:10]\n",
    "        \n",
    "        print(f\"\\nüìä BULK TESTING ANALYSIS\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"üèÜ Top 10 Configurations:\")\n",
    "        print(f\"{'Rank':<4} {'Config ID':<15} {'Algorithm':<8} {'Test Reward':<12} {'Val Reward':<12}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for i, result in enumerate(top_configs):\n",
    "            print(f\"{i+1:<4} {result['config_id']:<15} {result['algorithm']:<8} {result['test_reward']:<12.4f} {result['validation_reward']:<12.4f}\")\n",
    "        \n",
    "        # Algorithm performance comparison\n",
    "        a2c_results = [r for r in successful_results if r['algorithm'] == 'A2C']\n",
    "        td3_results = [r for r in successful_results if r['algorithm'] == 'TD3']\n",
    "        \n",
    "        print(f\"\\nüìà Algorithm Performance Comparison:\")\n",
    "        if a2c_results:\n",
    "            a2c_mean = np.mean([r['test_reward'] for r in a2c_results])\n",
    "            a2c_std = np.std([r['test_reward'] for r in a2c_results])\n",
    "            print(f\"   A2C: {len(a2c_results)} configs, Mean reward: {a2c_mean:.4f} ¬± {a2c_std:.4f}\")\n",
    "        \n",
    "        if td3_results:\n",
    "            td3_mean = np.mean([r['test_reward'] for r in td3_results])\n",
    "            td3_std = np.std([r['test_reward'] for r in td3_results])\n",
    "            print(f\"   TD3: {len(td3_results)} configs, Mean reward: {td3_mean:.4f} ¬± {td3_std:.4f}\")\n",
    "        \n",
    "        # Best configuration details\n",
    "        best_config = successful_results[0]\n",
    "        print(f\"\\nü•á BEST CONFIGURATION: {best_config['config_id']}\")\n",
    "        print(f\"   Algorithm: {best_config['algorithm']}\")\n",
    "        print(f\"   Test Reward: {best_config['test_reward']:.4f}\")\n",
    "        print(f\"   Validation Reward: {best_config['validation_reward']:.4f}\")\n",
    "        print(f\"   Training Time: {best_config['training_time']:.1f}s\")\n",
    "        \n",
    "        print(f\"\\n   üéØ Reward Components:\")\n",
    "        for component, value in best_config['reward_components'].items():\n",
    "            print(f\"      {component}: {value}\")\n",
    "        \n",
    "        # Save full results\n",
    "        results_file = os.path.join(OUTPUT_DIR, BULK_CONFIG['results_file'])\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump({\n",
    "                'metadata': {\n",
    "                    'total_configurations_tested': len(self.results),\n",
    "                    'successful_configurations': len(successful_results),\n",
    "                    'failed_configurations': len(self.results) - len(successful_results),\n",
    "                    'testing_timestamp': datetime.now().isoformat(),\n",
    "                    'best_config_id': best_config['config_id'],\n",
    "                    'best_test_reward': best_config['test_reward']\n",
    "                },\n",
    "                'results': self.results,\n",
    "                'top_10_configs': top_configs\n",
    "            }, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nüíæ Results saved to: {results_file}\")\n",
    "\n",
    "# Initialize bulk configuration tester\n",
    "print(\"\\nüî¨ Initializing Bulk Configuration Testing Pipeline:\")\n",
    "\n",
    "# Check if config file exists, if not generate it\n",
    "if not os.path.exists(CONFIG_FILE):\n",
    "    print(f\"   üîÑ Generating hyperparameter configurations...\")\n",
    "    # Generate the configuration file\n",
    "    current_dir = os.getcwd()\n",
    "    config_dir = os.path.dirname(CONFIG_FILE) or '.'\n",
    "    os.chdir(config_dir)\n",
    "    os.system(\"python generate_drl_config.py\")\n",
    "    os.chdir(current_dir)\n",
    "\n",
    "# Initialize tester\n",
    "bulk_tester = BulkConfigurationTester(trainer, CONFIG_FILE)\n",
    "\n",
    "print(\"‚úÖ Bulk configuration testing pipeline ready!\")\n",
    "print(\"üîç Ready to test 1000 hyperparameter combinations\")\n",
    "print(\"üìã Next: Evaluation and visualization sections\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f53bf07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Initializing Performance Analysis Pipeline:\n",
      "‚úÖ PerformanceAnalyzer initialized\n",
      "‚úÖ Performance analysis pipeline ready!\n",
      "üé® Ready for comprehensive model evaluation and visualization\n",
      "üìã Next: Final summary and methodology completion status\n"
     ]
    }
   ],
   "source": [
    "# ===================== COMPREHENSIVE EVALUATION & VISUALIZATION =====================\n",
    "\n",
    "class PerformanceAnalyzer:\n",
    "    \"\"\"\n",
    "    Comprehensive performance analysis and visualization for DRL trading models.\n",
    "    \n",
    "    Features:\n",
    "    - Portfolio performance metrics\n",
    "    - Risk analysis (Sharpe ratio, drawdown, VaR)\n",
    "    - Trading behavior analysis\n",
    "    - Reward component breakdown\n",
    "    - 15D state space analysis\n",
    "    - Model comparison dashboards\n",
    "    - Advanced visualizations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize PerformanceAnalyzer.\"\"\"\n",
    "        self.results_cache = {}\n",
    "        print(\"‚úÖ PerformanceAnalyzer initialized\")\n",
    "    \n",
    "    def calculate_comprehensive_metrics(self, \n",
    "                                      portfolio_values: np.ndarray,\n",
    "                                      positions: np.ndarray,\n",
    "                                      rewards: np.ndarray,\n",
    "                                      timestamps: np.ndarray = None,\n",
    "                                      initial_capital: float = INITIAL_CAPITAL) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calculate comprehensive trading performance metrics.\n",
    "        \n",
    "        Args:\n",
    "            portfolio_values: Array of portfolio values over time\n",
    "            positions: Array of position values over time\n",
    "            rewards: Array of reward values over time\n",
    "            timestamps: Array of timestamps (optional)\n",
    "            initial_capital: Initial portfolio value\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of performance metrics\n",
    "        \"\"\"\n",
    "        if len(portfolio_values) == 0:\n",
    "            return {}\n",
    "        \n",
    "        # Basic metrics\n",
    "        final_value = portfolio_values[-1]\n",
    "        total_return = (final_value - initial_capital) / initial_capital\n",
    "        \n",
    "        # Returns calculation\n",
    "        returns = np.diff(portfolio_values) / portfolio_values[:-1]\n",
    "        returns = returns[np.isfinite(returns)]  # Remove inf/nan\n",
    "        \n",
    "        metrics = {\n",
    "            # Portfolio Performance\n",
    "            'final_portfolio_value': final_value,\n",
    "            'total_return': total_return,\n",
    "            'initial_capital': initial_capital,\n",
    "            'net_profit': final_value - initial_capital,\n",
    "            \n",
    "            # Return Statistics\n",
    "            'mean_return': np.mean(returns) if len(returns) > 0 else 0,\n",
    "            'return_volatility': np.std(returns) if len(returns) > 0 else 0,\n",
    "            'return_skewness': stats.skew(returns) if len(returns) > 2 else 0,\n",
    "            'return_kurtosis': stats.kurtosis(returns) if len(returns) > 2 else 0,\n",
    "            \n",
    "            # Risk Metrics\n",
    "            'sharpe_ratio': 0,\n",
    "            'sortino_ratio': 0,\n",
    "            'calmar_ratio': 0,\n",
    "            'max_drawdown': 0,\n",
    "            'var_95': 0,\n",
    "            'cvar_95': 0,\n",
    "            \n",
    "            # Trading Behavior\n",
    "            'position_utilization': np.mean(np.abs(positions)) if len(positions) > 0 else 0,\n",
    "            'avg_position_size': np.mean(np.abs(positions[positions != 0])) if np.any(positions != 0) else 0,\n",
    "            'position_changes': np.sum(np.abs(np.diff(positions))) if len(positions) > 1 else 0,\n",
    "            'time_in_market': np.mean(np.abs(positions) > 0.01) if len(positions) > 0 else 0,\n",
    "            \n",
    "            # Reward Analysis\n",
    "            'total_reward': np.sum(rewards) if len(rewards) > 0 else 0,\n",
    "            'mean_reward': np.mean(rewards) if len(rewards) > 0 else 0,\n",
    "            'reward_volatility': np.std(rewards) if len(rewards) > 0 else 0,\n",
    "            'positive_reward_ratio': np.mean(rewards > 0) if len(rewards) > 0 else 0,\n",
    "        }\n",
    "        \n",
    "        # Advanced risk metrics\n",
    "        if len(returns) > 1 and np.std(returns) > 0:\n",
    "            # Sharpe ratio (annualized for minute data)\n",
    "            risk_free_rate = 0  # Assume 0 risk-free rate\n",
    "            excess_returns = returns - risk_free_rate\n",
    "            metrics['sharpe_ratio'] = np.mean(excess_returns) / np.std(returns) * np.sqrt(525600)  # 525600 minutes/year\n",
    "            \n",
    "            # Sortino ratio (downside deviation)\n",
    "            downside_returns = returns[returns < 0]\n",
    "            if len(downside_returns) > 0:\n",
    "                downside_deviation = np.std(downside_returns)\n",
    "                metrics['sortino_ratio'] = np.mean(excess_returns) / downside_deviation * np.sqrt(525600)\n",
    "            \n",
    "            # VaR and CVaR (95%)\n",
    "            metrics['var_95'] = np.percentile(returns, 5)\n",
    "            metrics['cvar_95'] = np.mean(returns[returns <= metrics['var_95']])\n",
    "        \n",
    "        # Maximum drawdown\n",
    "        if len(portfolio_values) > 0:\n",
    "            peak = np.maximum.accumulate(portfolio_values)\n",
    "            drawdown = (portfolio_values - peak) / peak\n",
    "            metrics['max_drawdown'] = abs(np.min(drawdown))\n",
    "            \n",
    "            # Calmar ratio\n",
    "            if metrics['max_drawdown'] > 0:\n",
    "                annualized_return = (final_value / initial_capital) ** (525600 / len(portfolio_values)) - 1\n",
    "                metrics['calmar_ratio'] = annualized_return / metrics['max_drawdown']\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def analyze_model_performance(self, \n",
    "                                model,\n",
    "                                env,\n",
    "                                n_episodes: int = 5,\n",
    "                                config_info: Dict[str, Any] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Analyze model performance through detailed simulation.\n",
    "        \n",
    "        Args:\n",
    "            model: Trained model\n",
    "            env: Trading environment\n",
    "            n_episodes: Number of episodes to run\n",
    "            config_info: Configuration information\n",
    "            \n",
    "        Returns:\n",
    "            Comprehensive analysis results\n",
    "        \"\"\"\n",
    "        print(f\"üìä Analyzing model performance ({n_episodes} episodes)...\")\n",
    "        \n",
    "        # Collect episode data\n",
    "        all_episodes_data = []\n",
    "        \n",
    "        for episode in range(n_episodes):\n",
    "            obs = env.reset()\n",
    "            episode_data = {\n",
    "                'portfolio_values': [],\n",
    "                'positions': [],\n",
    "                'rewards': [],\n",
    "                'actions': [],\n",
    "                'nav_values': [],\n",
    "                'prices': [],\n",
    "                'sentiment': [],\n",
    "                'component_breakdowns': {  # new structure\n",
    "                    'pnl_reward': [],\n",
    "                    'sharpe_reward': [],\n",
    "                    'transaction_penalty': [],\n",
    "                    'drawdown_penalty': [],\n",
    "                    'holding_reward': [],\n",
    "                    'activity_reward': [],\n",
    "                    'sentiment_reward': [],  # keep even if zero\n",
    "                },\n",
    "                'episode_reward': 0\n",
    "            }\n",
    "            \n",
    "            done = False\n",
    "            step_count = 0\n",
    "            \n",
    "            while not done and step_count < EPISODE_LENGTH:\n",
    "                # Get model action\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "                action = np.asarray(action).reshape(-1)\n",
    "\n",
    "                # Execute step\n",
    "                obs, reward, done, info = env.step(action)\n",
    "                reward = np.asarray(reward).reshape(-1)\n",
    "\n",
    "                # Info is a lost when using VecEnv, so iterate through it\n",
    "                if isinstance(info, (list, tuple)):\n",
    "                    infos = info\n",
    "                else:\n",
    "                    infos = [info]\n",
    "\n",
    "                for r, a, i_dict in zip(reward, action, infos):\n",
    "                    episode_data['rewards'].append(float(r))\n",
    "                    episode_data['actions'].append(float(a))\n",
    "                    episode_data['portfolio_values'].append(i_dict.get('portfolio_value', INITIAL_CAPITAL))\n",
    "                    episode_data['positions'].append(i_dict.get('position', 0.0))\n",
    "                    episode_data['nav_values'].append(i_dict.get('nav', 1.0))\n",
    "                    episode_data['prices'].append(i_dict.get('price', 0.0))\n",
    "                    episode_data['sentiment'].append(i_dict.get('sentiment', 0.0))\n",
    "                    episode_data['component_breakdowns']['pnl_reward'].append(i_dict.get('pnl_reward', 0.0))\n",
    "                    episode_data['component_breakdowns']['sharpe_reward'].append(i_dict.get('sharpe_reward', 0.0))\n",
    "                    episode_data['component_breakdowns']['transaction_penalty'].append(i_dict.get('transaction_penalty', 0.0))\n",
    "                    episode_data['component_breakdowns']['drawdown_penalty'].append(i_dict.get('drawdown_penalty', 0.0))\n",
    "                    episode_data['component_breakdowns']['holding_reward'].append(i_dict.get('holding_reward', 0.0))\n",
    "                    episode_data['component_breakdowns']['activity_reward'].append(i_dict.get('activity_reward', 0.0))\n",
    "                    episode_data['component_breakdowns']['sentiment_reward'].append(i_dict.get('sentiment_reward', 0.0))\n",
    "                step_count += 1\n",
    "            \n",
    "            all_episodes_data.append(episode_data)\n",
    "        \n",
    "        # Calculate metrics for each episode\n",
    "        episode_metrics = []\n",
    "        for i, episode_data in enumerate(all_episodes_data):\n",
    "            if len(episode_data['portfolio_values']) > 0:\n",
    "                metrics = self.calculate_comprehensive_metrics(\n",
    "                    np.array(episode_data['portfolio_values']),\n",
    "                    np.array(episode_data['positions']),\n",
    "                    np.array(episode_data['rewards'])\n",
    "                )\n",
    "                metrics['episode_index'] = i\n",
    "                episode_metrics.append(metrics)\n",
    "        \n",
    "        # Aggregate metrics across episodes\n",
    "        if episode_metrics:\n",
    "            aggregate_metrics = {}\n",
    "            for key in episode_metrics[0].keys():\n",
    "                if key != 'episode_index':\n",
    "                    values = [m[key] for m in episode_metrics if key in m]\n",
    "                    aggregate_metrics[f'mean_{key}'] = np.mean(values) if values else 0\n",
    "                    aggregate_metrics[f'std_{key}'] = np.std(values) if values else 0\n",
    "                    aggregate_metrics[f'min_{key}'] = np.min(values) if values else 0\n",
    "                    aggregate_metrics[f'max_{key}'] = np.max(values) if values else 0\n",
    "        else:\n",
    "            aggregate_metrics = {}\n",
    "        \n",
    "        analysis_results = {\n",
    "            'aggregate_metrics': aggregate_metrics,\n",
    "            'episode_metrics': episode_metrics,\n",
    "            'episode_data': all_episodes_data,\n",
    "            'config_info': config_info or {},\n",
    "            'n_episodes': n_episodes\n",
    "        }\n",
    "        \n",
    "        # Print summary\n",
    "        if episode_metrics:\n",
    "            avg_return = aggregate_metrics.get('mean_total_return', 0)\n",
    "            avg_sharpe = aggregate_metrics.get('mean_sharpe_ratio', 0)\n",
    "            avg_drawdown = aggregate_metrics.get('mean_max_drawdown', 0)\n",
    "            \n",
    "            print(f\"   ‚úÖ Analysis complete:\")\n",
    "            print(f\"      üìà Average Total Return: {avg_return:.2%}\")\n",
    "            print(f\"      üìä Average Sharpe Ratio: {avg_sharpe:.3f}\")\n",
    "            print(f\"      üìâ Average Max Drawdown: {avg_drawdown:.2%}\")\n",
    "        \n",
    "        return analysis_results\n",
    "    \n",
    "    def create_performance_plots(self, \n",
    "                               analysis_results: Dict[str, Any],\n",
    "                               save_plots: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Create comprehensive performance visualizations.\n",
    "        \n",
    "        Args:\n",
    "            analysis_results: Results from analyze_model_performance\n",
    "            save_plots: Whether to save plots to files\n",
    "        \"\"\"\n",
    "        print(\"üé® Creating performance visualizations...\")\n",
    "        \n",
    "        episode_data = analysis_results['episode_data']\n",
    "        if not episode_data:\n",
    "            print(\"   ‚ö†Ô∏è No episode data available for visualization\")\n",
    "            return\n",
    "        \n",
    "        # Use first episode for detailed plotting\n",
    "        main_episode = episode_data[0]\n",
    "        \n",
    "        # Create comprehensive dashboard\n",
    "        fig = plt.figure(figsize=(20, 15))\n",
    "        \n",
    "        # 1. Portfolio Value Evolution\n",
    "        ax1 = plt.subplot(3, 3, 1)\n",
    "        for i, episode in enumerate(episode_data[:3]):  # Show first 3 episodes\n",
    "            plt.plot(episode['portfolio_values'], label=f'Episode {i+1}', alpha=0.7)\n",
    "        plt.axhline(y=INITIAL_CAPITAL, color='red', linestyle='--', alpha=0.5, label='Initial Capital')\n",
    "        plt.title('Portfolio Value Evolution')\n",
    "        plt.xlabel('Time Steps')\n",
    "        plt.ylabel('Portfolio Value ($)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. NAV Evolution\n",
    "        ax2 = plt.subplot(3, 3, 2)\n",
    "        for i, episode in enumerate(episode_data[:3]):\n",
    "            plt.plot(episode['nav_values'], label=f'Episode {i+1}', alpha=0.7)\n",
    "        plt.axhline(y=1.0, color='red', linestyle='--', alpha=0.5, label='Break-even')\n",
    "        plt.title('NAV Evolution')\n",
    "        plt.xlabel('Time Steps')\n",
    "        plt.ylabel('NAV')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Position Evolution\n",
    "        ax3 = plt.subplot(3, 3, 3)\n",
    "        plt.plot(main_episode['positions'], label='Position', color='green', alpha=0.7)\n",
    "        plt.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "        plt.axhline(y=1, color='red', linestyle='--', alpha=0.5, label='Max Long')\n",
    "        plt.axhline(y=-1, color='red', linestyle='--', alpha=0.5, label='Max Short')\n",
    "        plt.title('Position Evolution')\n",
    "        plt.xlabel('Time Steps')\n",
    "        plt.ylabel('Position')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Reward Distribution\n",
    "        ax4 = plt.subplot(3, 3, 4)\n",
    "        all_rewards = []\n",
    "        for episode in episode_data:\n",
    "            all_rewards.extend(episode['rewards'])\n",
    "        plt.hist(all_rewards, bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "        plt.axvline(x=0, color='red', linestyle='--', alpha=0.7, label='Zero Reward')\n",
    "        plt.title('Reward Distribution')\n",
    "        plt.xlabel('Reward')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 5. Cumulative Rewards\n",
    "        ax5 = plt.subplot(3, 3, 5)\n",
    "        cumulative_rewards = np.cumsum(main_episode['rewards'])\n",
    "        plt.plot(cumulative_rewards, color='purple', alpha=0.8)\n",
    "        plt.title('Cumulative Rewards')\n",
    "        plt.xlabel('Time Steps')\n",
    "        plt.ylabel('Cumulative Reward')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 6. Action Distribution\n",
    "        ax6 = plt.subplot(3, 3, 6)\n",
    "        all_actions = []\n",
    "        for episode in episode_data:\n",
    "            all_actions.extend(episode['actions'])\n",
    "        plt.hist(all_actions, bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
    "        plt.axvline(x=0, color='red', linestyle='--', alpha=0.7, label='Neutral Action')\n",
    "        plt.title('Action Distribution')\n",
    "        plt.xlabel('Action')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 7. Performance Metrics Comparison\n",
    "        ax7 = plt.subplot(3, 3, 7)\n",
    "        episode_metrics = analysis_results['episode_metrics']\n",
    "        if episode_metrics:\n",
    "            returns = [m['total_return'] for m in episode_metrics]\n",
    "            sharpes = [m['sharpe_ratio'] for m in episode_metrics]\n",
    "            episodes = list(range(1, len(returns) + 1))\n",
    "            \n",
    "            ax7_twin = ax7.twinx()\n",
    "            bars1 = ax7.bar([x - 0.2 for x in episodes], returns, 0.4, label='Total Return', alpha=0.7, color='green')\n",
    "            bars2 = ax7_twin.bar([x + 0.2 for x in episodes], sharpes, 0.4, label='Sharpe Ratio', alpha=0.7, color='blue')\n",
    "            \n",
    "            ax7.set_xlabel('Episode')\n",
    "            ax7.set_ylabel('Total Return', color='green')\n",
    "            ax7_twin.set_ylabel('Sharpe Ratio', color='blue')\n",
    "            ax7.set_title('Episode Performance Metrics')\n",
    "            ax7.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 8. Drawdown Analysis\n",
    "        ax8 = plt.subplot(3, 3, 8)\n",
    "        portfolio_values = np.array(main_episode['portfolio_values'])\n",
    "        peak = np.maximum.accumulate(portfolio_values)\n",
    "        drawdown = (portfolio_values - peak) / peak\n",
    "        plt.fill_between(range(len(drawdown)), drawdown, 0, alpha=0.5, color='red', label='Drawdown')\n",
    "        plt.plot(drawdown, color='darkred', alpha=0.8)\n",
    "        plt.title('Drawdown Analysis')\n",
    "        plt.xlabel('Time Steps')\n",
    "        plt.ylabel('Drawdown (%)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 9. Risk-Return Scatter\n",
    "        ax9 = plt.subplot(3, 3, 9)\n",
    "        if episode_metrics:\n",
    "            returns = [m['total_return'] for m in episode_metrics]\n",
    "            volatilities = [m['return_volatility'] for m in episode_metrics]\n",
    "            plt.scatter(volatilities, returns, alpha=0.7, s=100, c=range(len(returns)), cmap='viridis')\n",
    "            plt.xlabel('Return Volatility')\n",
    "            plt.ylabel('Total Return')\n",
    "            plt.title('Risk-Return Profile')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add colorbar\n",
    "            cbar = plt.colorbar()\n",
    "            cbar.set_label('Episode')\n",
    "\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_plots:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f'performance_analysis_{timestamp}.png'\n",
    "            filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "            plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "            print(f\"   üíæ Performance analysis saved to: {filepath}\")\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "        # 10. Price vs Actions vs Portfolio\n",
    "        fig2 = plt.figure(figsize=(16, 10))\n",
    "        gs = fig2.add_gridspec(3, 1, height_ratios=[3, 1.2, 1], hspace=0.25)\n",
    "\n",
    "        steps = np.arange(len(main_episode[\"prices\"]))\n",
    "        prices = np.asarray(main_episode[\"prices\"], dtype=float)\n",
    "        portfolio_vals = np.asarray(main_episode[\"portfolio_values\"], dtype=float)\n",
    "        actions = np.asarray(main_episode[\"actions\"], dtype=float)\n",
    "        sentiment_vals = np.asarray(main_episode[\"sentiment\"], dtype=float)\n",
    "\n",
    "        # --- top subplot: price + portfolio ---\n",
    "        ax_price = fig2.add_subplot(gs[0])\n",
    "        ax_price.plot(steps, prices, color=\"black\", linewidth=1.5, label=\"ETH Price\")\n",
    "\n",
    "        action_colors = np.where(actions > 0, \"green\", np.where(actions < 0, \"red\", \"gray\"))\n",
    "        ax_price.scatter(steps, prices, c=action_colors, s=10, alpha=0.6,\n",
    "                        label=\"Agent Action (long/short/flat)\")\n",
    "\n",
    "        ax_portfolio = ax_price.twinx()\n",
    "        ax_portfolio.plot(steps, portfolio_vals, color=\"royalblue\", linewidth=1.2,\n",
    "                        label=\"Portfolio Value\")\n",
    "\n",
    "        ax_price.set_title(\"ETH Price vs Agent Actions and Portfolio Response\")\n",
    "        ax_price.set_ylabel(\"ETH Price ($)\")\n",
    "        ax_price.grid(True, alpha=0.3)\n",
    "        ax_portfolio.set_ylabel(\"Portfolio Value ($)\", color=\"royalblue\")\n",
    "        ax_portfolio.tick_params(axis=\"y\", labelcolor=\"royalblue\")\n",
    "\n",
    "        lines1, labels1 = ax_price.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax_portfolio.get_legend_handles_labels()\n",
    "        ax_price.legend(lines1 + lines2, labels1 + labels2, loc=\"upper left\", frameon=True)\n",
    "\n",
    "        # --- middle subplot: sentiment on its own scale ---\n",
    "        ax_sentiment = fig2.add_subplot(gs[1], sharex=ax_price)\n",
    "        ax_sentiment.plot(steps, sentiment_vals, color=\"darkorange\", linewidth=1.2,\n",
    "                        label=\"Sentiment Score\")\n",
    "        ax_sentiment.axhline(0, color=\"gray\", linestyle=\"--\", linewidth=0.8)\n",
    "        ax_sentiment.set_ylabel(\"Sentiment\")\n",
    "        ax_sentiment.grid(True, alpha=0.25)\n",
    "        ax_sentiment.legend(loc=\"upper left\")\n",
    "\n",
    "        # --- bottom subplot: action trace ---\n",
    "        ax_actions = fig2.add_subplot(gs[2], sharex=ax_price)\n",
    "        ax_actions.step(steps, actions, where=\"post\", color=\"purple\",\n",
    "                        linewidth=1.2, label=\"Agent Action\")\n",
    "        ax_actions.fill_between(steps, 0, actions, where=actions >= 0,\n",
    "                                color=\"green\", alpha=0.2, step=\"post\")\n",
    "        ax_actions.fill_between(steps, 0, actions, where=actions < 0,\n",
    "                                color=\"red\", alpha=0.2, step=\"post\")\n",
    "        ax_actions.set_ylim(-1.05, 1.05)\n",
    "        ax_actions.set_ylabel(\"Action [-1, 1]\")\n",
    "        ax_actions.set_xlabel(\"Episode Time Step\")\n",
    "        ax_actions.grid(True, alpha=0.25)\n",
    "        ax_actions.legend(loc=\"upper right\")\n",
    "\n",
    "        fig2.tight_layout()\n",
    "        if save_plots:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"price_action_portfolio_{timestamp}.png\"\n",
    "            filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "            fig2.savefig(filepath, dpi=300, bbox_inches=\"tight\")\n",
    "            print(f\"   üíæ Price/action/portfolio chart saved to: {filepath}\")\n",
    "        plt.show()\n",
    "                \n",
    "        # Create reward component analysis\n",
    "        self._create_reward_component_analysis(analysis_results, save_plots)\n",
    "    \n",
    "    def _create_reward_component_analysis(self, \n",
    "                                        analysis_results: Dict[str, Any],\n",
    "                                        save_plots: bool = True) -> None:\n",
    "        \"\"\"Create reward component breakdown analysis.\"\"\"\n",
    "        print(\"   üéØ Creating reward component analysis...\")\n",
    "        \n",
    "        fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "        \n",
    "        # Example reward components (in real implementation, track these from environment)\n",
    "        breakdown = analysis_results['episode_data'][0]['component_breakdowns']\n",
    "        components = []\n",
    "        values = []\n",
    "\n",
    "        for key, label in [\n",
    "            ('pnl_reward', 'PnL Reward'),\n",
    "            ('sharpe_reward', 'Sharpe Reward'),\n",
    "            ('transaction_penalty', 'Transaction Penalty'),\n",
    "            ('drawdown_penalty', 'Drawdown Penalty'),\n",
    "            ('holding_reward', 'Holding Reward'),\n",
    "            ('activity_reward', 'Activity Reward'),\n",
    "            ('sentiment_reward', 'Sentiment Reward'),\n",
    "        ]:\n",
    "            series = breakdown.get(key)\n",
    "            if series:\n",
    "                components.append(label)\n",
    "                values.append(np.mean(series))\n",
    "        \n",
    "        colors = ['green', 'blue', 'red', 'orange', 'purple', 'brown', 'pink']\n",
    "        \n",
    "        bars = ax.bar(components, values, color=colors, alpha=0.7)\n",
    "        ax.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "        ax.set_title('Multi-Component Reward Function Analysis')\n",
    "        ax.set_ylabel('Contribution')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, values):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + (0.01 if height >= 0 else -0.03),\n",
    "                   f'{value:.3f}', ha='center', va='bottom' if height >= 0 else 'top')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_plots:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f'reward_components_{timestamp}.png'\n",
    "            filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "            plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "            print(f\"   üíæ Reward component analysis saved to: {filepath}\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "    def create_model_comparison_dashboard(self, \n",
    "                                        models_results: Dict[str, Dict[str, Any]],\n",
    "                                        save_plots: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Create comprehensive model comparison dashboard.\n",
    "        \n",
    "        Args:\n",
    "            models_results: Dictionary of {model_name: analysis_results}\n",
    "            save_plots: Whether to save plots\n",
    "        \"\"\"\n",
    "        print(\"üìä Creating model comparison dashboard...\")\n",
    "        \n",
    "        fig = plt.figure(figsize=(16, 12))\n",
    "        \n",
    "        # Extract model names and metrics\n",
    "        model_names = list(models_results.keys())\n",
    "        n_models = len(model_names)\n",
    "        \n",
    "        if n_models < 2:\n",
    "            print(\"   ‚ö†Ô∏è Need at least 2 models for comparison\")\n",
    "            return\n",
    "        \n",
    "        # Performance comparison metrics\n",
    "        metrics_to_compare = ['mean_total_return', 'mean_sharpe_ratio', 'mean_max_drawdown', 'mean_return_volatility']\n",
    "        \n",
    "        for i, metric in enumerate(metrics_to_compare):\n",
    "            ax = plt.subplot(2, 2, i+1)\n",
    "            values = []\n",
    "            errors = []\n",
    "            \n",
    "            for model_name in model_names:\n",
    "                agg_metrics = models_results[model_name]['aggregate_metrics']\n",
    "                mean_val = agg_metrics.get(metric, 0)\n",
    "                std_val = agg_metrics.get(metric.replace('mean_', 'std_'), 0)\n",
    "                values.append(mean_val)\n",
    "                errors.append(std_val)\n",
    "            \n",
    "            bars = ax.bar(model_names, values, yerr=errors, capsize=5, alpha=0.7)\n",
    "            ax.set_title(metric.replace('mean_', '').replace('_', ' ').title())\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Color bars based on performance\n",
    "            for bar, value in zip(bars, values):\n",
    "                if 'return' in metric and value > 0:\n",
    "                    bar.set_color('green')\n",
    "                elif 'sharpe' in metric and value > 0:\n",
    "                    bar.set_color('blue')\n",
    "                elif 'drawdown' in metric:\n",
    "                    bar.set_color('red')\n",
    "                else:\n",
    "                    bar.set_color('orange')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_plots:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f'model_comparison_{timestamp}.png'\n",
    "            filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "            plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "            print(f\"   üíæ Model comparison saved to: {filepath}\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def create_trading_summary_table(self, models_results: Dict[str, Dict[str, Any]]) -> None:\n",
    "        \"\"\"Create comprehensive trading performance summary table.\"\"\"\n",
    "        print(\"\\nüìã COMPREHENSIVE TRADING PERFORMANCE SUMMARY\")\n",
    "        print(\"=\" * 90)\n",
    "        \n",
    "        # Header\n",
    "        header = f\"{'Metric':<25}\"\n",
    "        for model_name in models_results.keys():\n",
    "            header += f\"{model_name:<20}\"\n",
    "        print(header)\n",
    "        print(\"-\" * 90)\n",
    "        \n",
    "        # Metrics to display\n",
    "        metrics_display = [\n",
    "            ('Total Return (%)', 'mean_total_return', lambda x: f\"{x*100:.2f}%\"),\n",
    "            ('Sharpe Ratio', 'mean_sharpe_ratio', lambda x: f\"{x:.3f}\"),\n",
    "            ('Max Drawdown (%)', 'mean_max_drawdown', lambda x: f\"{x*100:.2f}%\"),\n",
    "            ('Return Volatility', 'mean_return_volatility', lambda x: f\"{x:.4f}\"),\n",
    "            ('Final Portfolio ($)', 'mean_final_portfolio_value', lambda x: f\"${x:,.0f}\"),\n",
    "            ('Position Utilization', 'mean_position_utilization', lambda x: f\"{x:.3f}\"),\n",
    "            ('Time in Market (%)', 'mean_time_in_market', lambda x: f\"{x*100:.1f}%\"),\n",
    "            ('Reward Volatility', 'mean_reward_volatility', lambda x: f\"{x:.0f}\"),\n",
    "        ]\n",
    "        \n",
    "        for display_name, metric_key, formatter in metrics_display:\n",
    "            row = f\"{display_name:<25}\"\n",
    "            for model_name in models_results.keys():\n",
    "                agg_metrics = models_results[model_name]['aggregate_metrics']\n",
    "                value = agg_metrics.get(metric_key, 0)\n",
    "                row += f\"{formatter(value):<20}\"\n",
    "            print(row)\n",
    "        \n",
    "        print(\"=\" * 90)\n",
    "        print(\"\\nüìù NOTES:\")\n",
    "        print(\"   ‚Ä¢ Total Return > 0% indicates profitable trading\")\n",
    "        print(\"   ‚Ä¢ Sharpe Ratio > 1.0 indicates good risk-adjusted returns\")\n",
    "        print(\"   ‚Ä¢ Lower Max Drawdown indicates better risk management\")\n",
    "        print(\"   ‚Ä¢ Position Utilization shows how actively the model trades\")\n",
    "        print(\"   ‚Ä¢ Results based on multi-episode evaluation\")\n",
    "\n",
    "\n",
    "\n",
    "# Initialize performance analyzer\n",
    "print(\"\\nüìä Initializing Performance Analysis Pipeline:\")\n",
    "analyzer = PerformanceAnalyzer()\n",
    "\n",
    "print(\"‚úÖ Performance analysis pipeline ready!\")\n",
    "print(\"üé® Ready for comprehensive model evaluation and visualization\")\n",
    "print(\"üìã Next: Final summary and methodology completion status\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64358a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping a2c_0001: checkpoint already at ./models/a2c_0001_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0002: checkpoint already at ./models/a2c_0002_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0003: checkpoint already at ./models/a2c_0003_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0004: checkpoint already at ./models/a2c_0004_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0005: checkpoint already at ./models/a2c_0005_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0006: checkpoint already at ./models/a2c_0006_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0007: checkpoint already at ./models/a2c_0007_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0008: checkpoint already at ./models/a2c_0008_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0009: checkpoint already at ./models/a2c_0009_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0010: checkpoint already at ./models/a2c_0010_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0011: checkpoint already at ./models/a2c_0011_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0012: checkpoint already at ./models/a2c_0012_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0013: checkpoint already at ./models/a2c_0013_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0014: checkpoint already at ./models/a2c_0014_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0015: checkpoint already at ./models/a2c_0015_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0016: checkpoint already at ./models/a2c_0016_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0017: checkpoint already at ./models/a2c_0017_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0018: checkpoint already at ./models/a2c_0018_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0019: checkpoint already at ./models/a2c_0019_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0020: checkpoint already at ./models/a2c_0020_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0021: checkpoint already at ./models/a2c_0021_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0022: checkpoint already at ./models/a2c_0022_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0023: checkpoint already at ./models/a2c_0023_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0024: checkpoint already at ./models/a2c_0024_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0025: checkpoint already at ./models/a2c_0025_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0026: checkpoint already at ./models/a2c_0026_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0027: checkpoint already at ./models/a2c_0027_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0028: checkpoint already at ./models/a2c_0028_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0029: checkpoint already at ./models/a2c_0029_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0030: checkpoint already at ./models/a2c_0030_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0031: checkpoint already at ./models/a2c_0031_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0032: checkpoint already at ./models/a2c_0032_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0033: checkpoint already at ./models/a2c_0033_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0034: checkpoint already at ./models/a2c_0034_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0035: checkpoint already at ./models/a2c_0035_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0036: checkpoint already at ./models/a2c_0036_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0037: checkpoint already at ./models/a2c_0037_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0038: checkpoint already at ./models/a2c_0038_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0039: checkpoint already at ./models/a2c_0039_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0040: checkpoint already at ./models/a2c_0040_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0041: checkpoint already at ./models/a2c_0041_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0042: checkpoint already at ./models/a2c_0042_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0043: checkpoint already at ./models/a2c_0043_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0044: checkpoint already at ./models/a2c_0044_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0045: checkpoint already at ./models/a2c_0045_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0046: checkpoint already at ./models/a2c_0046_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0047: checkpoint already at ./models/a2c_0047_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0048: checkpoint already at ./models/a2c_0048_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0049: checkpoint already at ./models/a2c_0049_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0050: checkpoint already at ./models/a2c_0050_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0051: checkpoint already at ./models/a2c_0051_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0052: checkpoint already at ./models/a2c_0052_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0053: checkpoint already at ./models/a2c_0053_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0054: checkpoint already at ./models/a2c_0054_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0055: checkpoint already at ./models/a2c_0055_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0056: checkpoint already at ./models/a2c_0056_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0057: checkpoint already at ./models/a2c_0057_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0058: checkpoint already at ./models/a2c_0058_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0059: checkpoint already at ./models/a2c_0059_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0060: checkpoint already at ./models/a2c_0060_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0061: checkpoint already at ./models/a2c_0061_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0062: checkpoint already at ./models/a2c_0062_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0063: checkpoint already at ./models/a2c_0063_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0064: checkpoint already at ./models/a2c_0064_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0065: checkpoint already at ./models/a2c_0065_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0066: checkpoint already at ./models/a2c_0066_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0067: checkpoint already at ./models/a2c_0067_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0068: checkpoint already at ./models/a2c_0068_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0069: checkpoint already at ./models/a2c_0069_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0070: checkpoint already at ./models/a2c_0070_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0071: checkpoint already at ./models/a2c_0071_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0072: checkpoint already at ./models/a2c_0072_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0073: checkpoint already at ./models/a2c_0073_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0074: checkpoint already at ./models/a2c_0074_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0075: checkpoint already at ./models/a2c_0075_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0076: checkpoint already at ./models/a2c_0076_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0077: checkpoint already at ./models/a2c_0077_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0078: checkpoint already at ./models/a2c_0078_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0079: checkpoint already at ./models/a2c_0079_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0080: checkpoint already at ./models/a2c_0080_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0081: checkpoint already at ./models/a2c_0081_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0082: checkpoint already at ./models/a2c_0082_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0083: checkpoint already at ./models/a2c_0083_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0084: checkpoint already at ./models/a2c_0084_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0085: checkpoint already at ./models/a2c_0085_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0086: checkpoint already at ./models/a2c_0086_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0087: checkpoint already at ./models/a2c_0087_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0088: checkpoint already at ./models/a2c_0088_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0089: checkpoint already at ./models/a2c_0089_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0090: checkpoint already at ./models/a2c_0090_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0091: checkpoint already at ./models/a2c_0091_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0092: checkpoint already at ./models/a2c_0092_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0093: checkpoint already at ./models/a2c_0093_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0094: checkpoint already at ./models/a2c_0094_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0095: checkpoint already at ./models/a2c_0095_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0096: checkpoint already at ./models/a2c_0096_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0097: checkpoint already at ./models/a2c_0097_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0098: checkpoint already at ./models/a2c_0098_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0099: checkpoint already at ./models/a2c_0099_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0100: checkpoint already at ./models/a2c_0100_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0101: checkpoint already at ./models/a2c_0101_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0102: checkpoint already at ./models/a2c_0102_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0103: checkpoint already at ./models/a2c_0103_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0104: checkpoint already at ./models/a2c_0104_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0105: checkpoint already at ./models/a2c_0105_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0106: checkpoint already at ./models/a2c_0106_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0107: checkpoint already at ./models/a2c_0107_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0108: checkpoint already at ./models/a2c_0108_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0109: checkpoint already at ./models/a2c_0109_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0110: checkpoint already at ./models/a2c_0110_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0111: checkpoint already at ./models/a2c_0111_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0112: checkpoint already at ./models/a2c_0112_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0113: checkpoint already at ./models/a2c_0113_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0114: checkpoint already at ./models/a2c_0114_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0115: checkpoint already at ./models/a2c_0115_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0116: checkpoint already at ./models/a2c_0116_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0117: checkpoint already at ./models/a2c_0117_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0118: checkpoint already at ./models/a2c_0118_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0119: checkpoint already at ./models/a2c_0119_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0120: checkpoint already at ./models/a2c_0120_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0121: checkpoint already at ./models/a2c_0121_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0122: checkpoint already at ./models/a2c_0122_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0123: checkpoint already at ./models/a2c_0123_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0124: checkpoint already at ./models/a2c_0124_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0125: checkpoint already at ./models/a2c_0125_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0126: checkpoint already at ./models/a2c_0126_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0127: checkpoint already at ./models/a2c_0127_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0128: checkpoint already at ./models/a2c_0128_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0129: checkpoint already at ./models/a2c_0129_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0130: checkpoint already at ./models/a2c_0130_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0131: checkpoint already at ./models/a2c_0131_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0132: checkpoint already at ./models/a2c_0132_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0133: checkpoint already at ./models/a2c_0133_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0134: checkpoint already at ./models/a2c_0134_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0135: checkpoint already at ./models/a2c_0135_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0136: checkpoint already at ./models/a2c_0136_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0137: checkpoint already at ./models/a2c_0137_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0138: checkpoint already at ./models/a2c_0138_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0139: checkpoint already at ./models/a2c_0139_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0140: checkpoint already at ./models/a2c_0140_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0141: checkpoint already at ./models/a2c_0141_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0142: checkpoint already at ./models/a2c_0142_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0143: checkpoint already at ./models/a2c_0143_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0144: checkpoint already at ./models/a2c_0144_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0145: checkpoint already at ./models/a2c_0145_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0146: checkpoint already at ./models/a2c_0146_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0147: checkpoint already at ./models/a2c_0147_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0148: checkpoint already at ./models/a2c_0148_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0149: checkpoint already at ./models/a2c_0149_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0150: checkpoint already at ./models/a2c_0150_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0151: checkpoint already at ./models/a2c_0151_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0152: checkpoint already at ./models/a2c_0152_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0153: checkpoint already at ./models/a2c_0153_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0154: checkpoint already at ./models/a2c_0154_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0155: checkpoint already at ./models/a2c_0155_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0156: checkpoint already at ./models/a2c_0156_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0157: checkpoint already at ./models/a2c_0157_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0158: checkpoint already at ./models/a2c_0158_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0159: checkpoint already at ./models/a2c_0159_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0160: checkpoint already at ./models/a2c_0160_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0161: checkpoint already at ./models/a2c_0161_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0162: checkpoint already at ./models/a2c_0162_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0163: checkpoint already at ./models/a2c_0163_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0164: checkpoint already at ./models/a2c_0164_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0165: checkpoint already at ./models/a2c_0165_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0166: checkpoint already at ./models/a2c_0166_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0167: checkpoint already at ./models/a2c_0167_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0168: checkpoint already at ./models/a2c_0168_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0169: checkpoint already at ./models/a2c_0169_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0170: checkpoint already at ./models/a2c_0170_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0171: checkpoint already at ./models/a2c_0171_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0172: checkpoint already at ./models/a2c_0172_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0173: checkpoint already at ./models/a2c_0173_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0174: checkpoint already at ./models/a2c_0174_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0175: checkpoint already at ./models/a2c_0175_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0176: checkpoint already at ./models/a2c_0176_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0177: checkpoint already at ./models/a2c_0177_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0178: checkpoint already at ./models/a2c_0178_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0179: checkpoint already at ./models/a2c_0179_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0180: checkpoint already at ./models/a2c_0180_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0181: checkpoint already at ./models/a2c_0181_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0182: checkpoint already at ./models/a2c_0182_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0183: checkpoint already at ./models/a2c_0183_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0184: checkpoint already at ./models/a2c_0184_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0185: checkpoint already at ./models/a2c_0185_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0186: checkpoint already at ./models/a2c_0186_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0187: checkpoint already at ./models/a2c_0187_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0188: checkpoint already at ./models/a2c_0188_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0189: checkpoint already at ./models/a2c_0189_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0190: checkpoint already at ./models/a2c_0190_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0191: checkpoint already at ./models/a2c_0191_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0192: checkpoint already at ./models/a2c_0192_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0193: checkpoint already at ./models/a2c_0193_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0194: checkpoint already at ./models/a2c_0194_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0195: checkpoint already at ./models/a2c_0195_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0196: checkpoint already at ./models/a2c_0196_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0197: checkpoint already at ./models/a2c_0197_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0198: checkpoint already at ./models/a2c_0198_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0199: checkpoint already at ./models/a2c_0199_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0200: checkpoint already at ./models/a2c_0200_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0201: checkpoint already at ./models/a2c_0201_final.zip\n",
      "‚ö†Ô∏è Skipping a2c_0202: checkpoint already at ./models/a2c_0202_final.zip\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No models trained successfully. Check logs above.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 76\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Bail out early if nothing succeeded\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m experiment_results:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo models trained successfully. Check logs above.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# 5) Rank configurations by test reward (highest = best)\u001b[39;00m\n\u001b[32m     79\u001b[39m ranked = \u001b[38;5;28msorted\u001b[39m(   \n\u001b[32m     80\u001b[39m     experiment_results,\n\u001b[32m     81\u001b[39m     key=\u001b[38;5;28;01mlambda\u001b[39;00m entry: entry[\u001b[33m\"\u001b[39m\u001b[33mmean_test_reward\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     82\u001b[39m     reverse=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     83\u001b[39m )\n",
      "\u001b[31mRuntimeError\u001b[39m: No models trained successfully. Check logs above."
     ]
    }
   ],
   "source": [
    "# === Train a handful of configs === #\n",
    "import json\n",
    "from copy import deepcopy\n",
    "\n",
    "def merged_reward_config(cfg: Dict[str, Any]) -> Dict[str, float]:\n",
    "    merged = REWARD_COMPONENTS.copy()\n",
    "    merged.update(cfg.get(\"reward_components\", {}))\n",
    "    return merged\n",
    "\n",
    "\n",
    "# 1) Load generated hyperparameter configurations\n",
    "with open(CONFIG_FILE, \"r\") as f:\n",
    "    config_payload = json.load(f)\n",
    "\n",
    "all_configs = config_payload[\"configurations\"]\n",
    "\n",
    "# Choose how many configs you want to try in this pass (keep small to start)\n",
    "configs_to_try = all_configs   # e.g. first 1 configs; change as needed\n",
    "\n",
    "experiment_results = []\n",
    "\n",
    "for cfg in configs_to_try:\n",
    "    cfg_id = cfg.get(\"config_id\", \"unknown\")\n",
    "    save_path = os.path.join(MODEL_DIR, f\"{cfg_id}_final.zip\")\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"‚ö†Ô∏è Skipping {cfg_id}: checkpoint already at {save_path}\")\n",
    "    else:\n",
    "        cfg = cfg.copy()\n",
    "        cfg[\"reward_components\"] = merged_reward_config(cfg)\n",
    "        reward_cfg = cfg[\"reward_components\"]\n",
    "\n",
    "        cfg_id = cfg.get(\"config_id\", \"unknown\")\n",
    "        algo = cfg.get(\"algorithm\", \"A2C\").upper()\n",
    "        print(f\"\\nüöÄ Running experiment for {cfg_id} ({algo})\")\n",
    "\n",
    "        if algo == \"A2C\":\n",
    "            model, train_metrics = trainer.train_a2c_model(cfg, reward_config=reward_cfg)\n",
    "        elif algo == \"TD3\":\n",
    "            model, train_metrics = trainer.train_td3_model(cfg, reward_config=reward_cfg)\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è Unsupported algorithm: {algo}, skipping\")\n",
    "            continue\n",
    "\n",
    "        # 3) Evaluate on the hold-out test set\n",
    "        test_env = trainer.create_environment(\n",
    "            trainer.test_data,\n",
    "            reward_config=cfg.get(\"reward_components\"),\n",
    "            random_start=False\n",
    "        )\n",
    "        mean_test_reward = trainer.evaluate_model(model, test_env, n_episodes=3)\n",
    "\n",
    "        # 4) Collect richer episode statistics for visualization\n",
    "        analysis = analyzer.analyze_model_performance(\n",
    "            model=model,\n",
    "            env=test_env,\n",
    "            n_episodes=3,\n",
    "            config_info={\"config_id\": cfg_id, \"algorithm\": algo}\n",
    "        )\n",
    "\n",
    "        experiment_results.append({\n",
    "            \"config\": deepcopy(cfg),\n",
    "            \"model\": model,\n",
    "            \"train_metrics\": train_metrics,\n",
    "            \"mean_test_reward\": mean_test_reward,\n",
    "            \"analysis\": analysis,\n",
    "        })\n",
    "        import os\n",
    "        os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "        cfg_id = deepcopy(cfg)[\"config_id\"]\n",
    "        model.save(os.path.join(MODEL_DIR, f\"{cfg_id}_final\"))\n",
    "        print(f\"üíæ Saved {cfg_id} to {save_path}.zip\")\n",
    "\n",
    "# Bail out early if nothing succeeded\n",
    "if not experiment_results:\n",
    "    raise RuntimeError(\"No models trained successfully. Check logs above.\")\n",
    "\n",
    "# 5) Rank configurations by test reward (highest = best)\n",
    "ranked = sorted(   \n",
    "    experiment_results,\n",
    "    key=lambda entry: entry[\"mean_test_reward\"],\n",
    "    reverse=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac5ee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from stable_baselines3 import A2C, TD3\n",
    "\n",
    "# loading the results from the previous experiment\n",
    "\n",
    "loaded_results = []\n",
    "\n",
    "for cfg in all_configs:\n",
    "    cfg_id = cfg[\"config_id\"]\n",
    "    algo = cfg[\"algorithm\"].upper()\n",
    "    model_path = Path(MODEL_DIR) / f\"{cfg_id}_final.zip\"\n",
    "    if not model_path.exists():\n",
    "        continue\n",
    "\n",
    "    reward_cfg = merged_reward_config(cfg)\n",
    "    test_env = trainer.create_environment(\n",
    "        trainer.test_data,\n",
    "        reward_config=reward_cfg,\n",
    "        random_start=False,\n",
    "    )\n",
    "\n",
    "    if algo == \"A2C\":\n",
    "        model = A2C.load(model_path, device=trainer.device)\n",
    "    elif algo == \"TD3\":\n",
    "        model = TD3.load(model_path, device=trainer.device)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    model.set_env(test_env)\n",
    "\n",
    "    mean_test_reward = trainer.evaluate_model(model, test_env, n_episodes=3)\n",
    "    analysis = analyzer.analyze_model_performance(\n",
    "        model=model,\n",
    "        env=test_env,\n",
    "        n_episodes=3,\n",
    "        config_info={\"config_id\": cfg_id, \"algorithm\": algo},\n",
    "    )\n",
    "\n",
    "    loaded_results.append({\n",
    "        \"config\": cfg,\n",
    "        \"model\": model,\n",
    "        \"train_metrics\": {\"mean_reward\": float(\"nan\")},\n",
    "        \"mean_test_reward\": mean_test_reward,\n",
    "        \"analysis\": analysis,\n",
    "    })\n",
    "\n",
    "ranked = sorted(loaded_results, key=lambda e: e[\"mean_test_reward\"], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143d2db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === visualize the top results === #\n",
    "\n",
    "# Keep the top 15 for visual comparison (adjust if you prefer more)\n",
    "top_k = ranked[:15]\n",
    "print(\"\\nüèÜ Top configurations:\")\n",
    "for idx, entry in enumerate(top_k, 1):\n",
    "    cfg = entry[\"config\"]\n",
    "    print(f\" {idx}. {cfg['config_id']} ({cfg['algorithm']}) \"\n",
    "          f\"| Val mean={entry['train_metrics']['mean_reward']:.4f} \"\n",
    "          f\"| Test mean={entry['mean_test_reward']:.4f}\")\n",
    "\n",
    "# 6) Build the payload that PerformanceAnalyzer expects\n",
    "comparison_payload = {\n",
    "    entry[\"config\"][\"config_id\"]: entry[\"analysis\"] for entry in top_k\n",
    "}\n",
    "\n",
    "for analysis in top_k:\n",
    "    # 7) Generate visualizations\n",
    "    best_analysis = analysis[\"analysis\"]\n",
    "    analyzer.create_performance_plots(best_analysis)                      # detailed deep-dive for best run\n",
    "\n",
    "if len(comparison_payload) >= 2:\n",
    "    analyzer.create_model_comparison_dashboard(comparison_payload)    # side-by-side metrics\n",
    "    analyzer.create_trading_summary_table(comparison_payload)         # textual summary\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Only one successful configuration: skipping comparison dashboard/table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381cff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== FINAL TESTING & COMPREHENSIVE ANALYSIS =====================\n",
    "\n",
    "print(\"üìä Starting Final Testing & Comprehensive Analysis...\")\n",
    "\n",
    "# Initialize performance analyzer\n",
    "analyzer = PerformanceAnalyzer(config)\n",
    "\n",
    "# =================== STEP 1: LOAD PREVIOUSLY TRAINED MODELS ===================\n",
    "print(\"\\nüîÑ Loading previously trained models...\")\n",
    "\n",
    "# Load configurations (you need this defined)\n",
    "all_configs = hyperparameter_optimizer.load_configurations()\n",
    "\n",
    "# Create ranked_results by loading saved models\n",
    "ranked_results = []\n",
    "\n",
    "for cfg in all_configs:\n",
    "    cfg_id = cfg[\"config_id\"]\n",
    "    algo = cfg[\"algorithm\"].upper()\n",
    "    \n",
    "    # Check for saved model (using the auto-save naming convention)\n",
    "    model_path = Path(config.data.model_dir) / f\"{cfg_id}_final.zip\" \n",
    "    \n",
    "    if not model_path.exists():\n",
    "        print(f\"‚ö†Ô∏è Model not found: {model_path}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Create test environment\n",
    "        reward_cfg = merged_reward_config(cfg)\n",
    "        test_env = trainer.create_environment(\n",
    "            trainer.test_data,\n",
    "            reward_config=reward_cfg,\n",
    "            random_start=False,\n",
    "        )\n",
    "\n",
    "        # Load model\n",
    "        if algo == \"A2C\":\n",
    "            model = A2C.load(model_path, device=trainer.device)\n",
    "        elif algo == \"TD3\":\n",
    "            model = TD3.load(model_path, device=trainer.device)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        model.set_env(test_env)\n",
    "\n",
    "        # Evaluate model on test set  \n",
    "        mean_test_reward, test_reward_std = trainer.evaluate_model(model, test_env, n_episodes=3)\n",
    "        \n",
    "        # Add to ranked_results\n",
    "        ranked_results.append({\n",
    "            \"config\": cfg,\n",
    "            \"model\": model,\n",
    "            \"train_metrics\": {\"mean_reward\": float(\"nan\")},  # We don't have training metrics from saved models\n",
    "            \"mean_test_reward\": mean_test_reward,\n",
    "            \"test_reward_std\": test_reward_std,\n",
    "        })\n",
    "        \n",
    "        print(f\"   ‚úÖ Loaded {cfg_id}: Test reward = {mean_test_reward:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error loading {cfg_id}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Sort by test performance to create ranked_results\n",
    "ranked_results = sorted(ranked_results, key=lambda e: e[\"mean_test_reward\"], reverse=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded {len(ranked_results)} trained models successfully!\")\n",
    "\n",
    "# =================== STEP 2: COMPREHENSIVE ANALYSIS ===================\n",
    "if not ranked_results:\n",
    "    print(\"‚ùå No trained models found. Please check your model directory.\")\n",
    "else:\n",
    "    print(\"\\nüîÑ Starting comprehensive analysis of loaded models...\")\n",
    "\n",
    "    loaded_results = []\n",
    "    \n",
    "    for entry in ranked_results[:5]:  # Analyze top 5 models\n",
    "        cfg = entry[\"config\"]\n",
    "        cfg_id = cfg[\"config_id\"]\n",
    "        model = entry[\"model\"]  # Use the already loaded model\n",
    "        \n",
    "        try:\n",
    "            # Create test environment for analysis\n",
    "            reward_cfg = merged_reward_config(cfg)\n",
    "            test_env = trainer.create_environment(\n",
    "                trainer.test_data,\n",
    "                reward_config=reward_cfg,\n",
    "                random_start=False,\n",
    "            )\n",
    "            \n",
    "            model.set_env(test_env)\n",
    "            \n",
    "            # Comprehensive analysis\n",
    "            analysis = analyzer.analyze_model_performance(\n",
    "                model=model,\n",
    "                env=test_env,\n",
    "                n_episodes=5,\n",
    "                config_info={\"config_id\": cfg_id, \"algorithm\": cfg[\"algorithm\"]},\n",
    "            )\n",
    "\n",
    "            loaded_results.append({\n",
    "                \"config\": cfg,\n",
    "                \"model\": model,\n",
    "                \"train_metrics\": entry[\"train_metrics\"],\n",
    "                \"mean_test_reward\": entry[\"mean_test_reward\"],\n",
    "                \"analysis\": analysis,\n",
    "            })\n",
    "            \n",
    "            print(f\"   ‚úÖ Analyzed {cfg_id}: Test reward = {entry['mean_test_reward']:.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error analyzing {cfg_id}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Re-rank based on final test performance\n",
    "    final_ranked = sorted(loaded_results, key=lambda e: e[\"mean_test_reward\"], reverse=True)\n",
    "\n",
    "    print(f\"\\nüèÜ Final Rankings (Test Set Performance):\")\n",
    "    for idx, entry in enumerate(final_ranked, 1):\n",
    "        cfg = entry[\"config\"]\n",
    "        print(f\" {idx}. {cfg['config_id']} ({cfg['algorithm']}) \"\n",
    "              f\"| Test reward: {entry['mean_test_reward']:.4f}\")\n",
    "\n",
    "    # =================== STEP 3: GENERATE VISUALIZATIONS ===================\n",
    "    if final_ranked:\n",
    "        # Get the top performers for detailed analysis\n",
    "        top_k = final_ranked[:3]  # Top 3 models\n",
    "        \n",
    "        print(f\"\\nüé® Generating comprehensive visualizations...\")\n",
    "        \n",
    "        # Build comparison payload for multi-model analysis\n",
    "        comparison_payload = {\n",
    "            entry[\"config\"][\"config_id\"]: entry[\"analysis\"] for entry in top_k\n",
    "        }\n",
    "        \n",
    "        # Generate detailed analysis for the best model\n",
    "        best_analysis = top_k[0][\"analysis\"]\n",
    "        print(f\"\\nüìà Detailed analysis for best model: {top_k[0]['config']['config_id']}\")\n",
    "        analyzer.create_performance_plots(best_analysis, save_plots=True)\n",
    "        \n",
    "        # Multi-model comparison\n",
    "        if len(comparison_payload) >= 2:\n",
    "            print(f\"\\nüìä Creating model comparison dashboard...\")\n",
    "            analyzer.create_model_comparison_dashboard(comparison_payload, save_plots=True)\n",
    "            \n",
    "            print(f\"\\nüìã Generating comprehensive performance summary...\")\n",
    "            analyzer.create_trading_summary_table(comparison_payload)\n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è Only one model available: skipping comparison dashboard.\")\n",
    "        \n",
    "        # Final summary\n",
    "        best_model_analysis = best_analysis\n",
    "        best_metrics = best_model_analysis['aggregate_metrics']\n",
    "        \n",
    "        print(f\"\\nüéâ FINAL RESULTS SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ü•á Best Model: {top_k[0]['config']['config_id']}\")\n",
    "        print(f\"üîß Algorithm: {top_k[0]['config']['algorithm']}\")\n",
    "        print(f\"üìä Final Test Reward: {top_k[0]['mean_test_reward']:.4f}\")\n",
    "        print(f\"üìà Average Total Return: {best_metrics.get('mean_total_return', 0):.2%}\")\n",
    "        print(f\"üìä Average Sharpe Ratio: {best_metrics.get('mean_sharpe_ratio', 0):.3f}\")\n",
    "        print(f\"üìâ Average Max Drawdown: {best_metrics.get('mean_max_drawdown', 0):.2%}\")\n",
    "        print(f\"üí∞ Average Final Portfolio: ${best_metrics.get('mean_final_portfolio_value', 0):,.0f}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Complete DRL Trading Analysis Finished!\")\n",
    "        print(f\"üìÅ All plots and results saved to: {config.data.output_dir}\")\n",
    "        print(f\"üíæ Trained models saved to: {config.data.model_dir}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No models available for final analysis.\")\n",
    "\n",
    "print(f\"\\nüéØ Analysis pipeline complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454b8b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== FINAL TESTING & COMPREHENSIVE ANALYSIS =====================\n",
    "\n",
    "print(\"üìä Starting Final Testing & Comprehensive Analysis...\")\n",
    "\n",
    "# Initialize performance analyzer\n",
    "analyzer = PerformanceAnalyzer(config)\n",
    "\n",
    "# =================== STEP 1: LOAD PREVIOUSLY TRAINED MODELS ===================\n",
    "print(\"\\nüîÑ Loading previously trained models...\")\n",
    "\n",
    "# Load configurations (you need this defined)\n",
    "all_configs = hyperparameter_optimizer.load_configurations()\n",
    "\n",
    "# Create ranked_results by loading saved models\n",
    "ranked_results = []\n",
    "\n",
    "for cfg in all_configs:\n",
    "    cfg_id = cfg[\"config_id\"]\n",
    "    algo = cfg[\"algorithm\"].upper()\n",
    "    \n",
    "    # Check for saved model (using the auto-save naming convention)\n",
    "    model_path = Path(config.data.model_dir) / f\"{cfg_id}_final.zip\" \n",
    "    \n",
    "    if not model_path.exists():\n",
    "        print(f\"‚ö†Ô∏è Model not found: {model_path}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Create test environment\n",
    "        reward_cfg = merged_reward_config(cfg)\n",
    "        test_env = trainer.create_environment(\n",
    "            trainer.test_data,\n",
    "            reward_config=reward_cfg,\n",
    "            random_start=False,\n",
    "        )\n",
    "\n",
    "        # Load model\n",
    "        if algo == \"A2C\":\n",
    "            model = A2C.load(model_path, device=trainer.device)\n",
    "        elif algo == \"TD3\":\n",
    "            model = TD3.load(model_path, device=trainer.device)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        model.set_env(test_env)\n",
    "\n",
    "        # Evaluate model on test set  \n",
    "        mean_test_reward, test_reward_std = trainer.evaluate_model(model, test_env, n_episodes=3)\n",
    "        \n",
    "        # Add to ranked_results\n",
    "        ranked_results.append({\n",
    "            \"config\": cfg,\n",
    "            \"model\": model,\n",
    "            \"train_metrics\": {\"mean_reward\": float(\"nan\")},  # We don't have training metrics from saved models\n",
    "            \"mean_test_reward\": mean_test_reward,\n",
    "            \"test_reward_std\": test_reward_std,\n",
    "        })\n",
    "        \n",
    "        print(f\"   ‚úÖ Loaded {cfg_id}: Test reward = {mean_test_reward:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error loading {cfg_id}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Sort by test performance to create ranked_results\n",
    "ranked_results = sorted(ranked_results, key=lambda e: e[\"mean_test_reward\"], reverse=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded {len(ranked_results)} trained models successfully!\")\n",
    "\n",
    "# =================== STEP 2: COMPREHENSIVE ANALYSIS ===================\n",
    "if not ranked_results:\n",
    "    print(\"‚ùå No trained models found. Please check your model directory.\")\n",
    "else:\n",
    "    print(\"\\nüîÑ Starting comprehensive analysis of loaded models...\")\n",
    "\n",
    "    loaded_results = []\n",
    "    \n",
    "    for entry in ranked_results[:5]:  # Analyze top 5 models\n",
    "        cfg = entry[\"config\"]\n",
    "        cfg_id = cfg[\"config_id\"]\n",
    "        model = entry[\"model\"]  # Use the already loaded model\n",
    "        \n",
    "        try:\n",
    "            # Create test environment for analysis\n",
    "            reward_cfg = merged_reward_config(cfg)\n",
    "            test_env = trainer.create_environment(\n",
    "                trainer.test_data,\n",
    "                reward_config=reward_cfg,\n",
    "                random_start=False,\n",
    "            )\n",
    "            \n",
    "            model.set_env(test_env)\n",
    "            \n",
    "            # Comprehensive analysis\n",
    "            analysis = analyzer.analyze_model_performance(\n",
    "                model=model,\n",
    "                env=test_env,\n",
    "                n_episodes=5,\n",
    "                config_info={\"config_id\": cfg_id, \"algorithm\": cfg[\"algorithm\"]},\n",
    "            )\n",
    "\n",
    "            loaded_results.append({\n",
    "                \"config\": cfg,\n",
    "                \"model\": model,\n",
    "                \"train_metrics\": entry[\"train_metrics\"],\n",
    "                \"mean_test_reward\": entry[\"mean_test_reward\"],\n",
    "                \"analysis\": analysis,\n",
    "            })\n",
    "            \n",
    "            print(f\"   ‚úÖ Analyzed {cfg_id}: Test reward = {entry['mean_test_reward']:.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error analyzing {cfg_id}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Re-rank based on final test performance\n",
    "    final_ranked = sorted(loaded_results, key=lambda e: e[\"mean_test_reward\"], reverse=True)\n",
    "\n",
    "    print(f\"\\nüèÜ Final Rankings (Test Set Performance):\")\n",
    "    for idx, entry in enumerate(final_ranked, 1):\n",
    "        cfg = entry[\"config\"]\n",
    "        print(f\" {idx}. {cfg['config_id']} ({cfg['algorithm']}) \"\n",
    "              f\"| Test reward: {entry['mean_test_reward']:.4f}\")\n",
    "\n",
    "    # =================== STEP 3: GENERATE VISUALIZATIONS ===================\n",
    "    if final_ranked:\n",
    "        # Get the top performers for detailed analysis\n",
    "        top_k = final_ranked[:3]  # Top 3 models\n",
    "        \n",
    "        print(f\"\\nüé® Generating comprehensive visualizations...\")\n",
    "        \n",
    "        # Build comparison payload for multi-model analysis\n",
    "        comparison_payload = {\n",
    "            entry[\"config\"][\"config_id\"]: entry[\"analysis\"] for entry in top_k\n",
    "        }\n",
    "        \n",
    "        # Generate detailed analysis for the best model\n",
    "        best_analysis = top_k[0][\"analysis\"]\n",
    "        print(f\"\\nüìà Detailed analysis for best model: {top_k[0]['config']['config_id']}\")\n",
    "        analyzer.create_performance_plots(best_analysis, save_plots=True)\n",
    "        \n",
    "        # Multi-model comparison\n",
    "        if len(comparison_payload) >= 2:\n",
    "            print(f\"\\nüìä Creating model comparison dashboard...\")\n",
    "            analyzer.create_model_comparison_dashboard(comparison_payload, save_plots=True)\n",
    "            \n",
    "            print(f\"\\nüìã Generating comprehensive performance summary...\")\n",
    "            analyzer.create_trading_summary_table(comparison_payload)\n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è Only one model available: skipping comparison dashboard.\")\n",
    "        \n",
    "        # Final summary\n",
    "        best_model_analysis = best_analysis\n",
    "        best_metrics = best_model_analysis['aggregate_metrics']\n",
    "        \n",
    "        print(f\"\\nüéâ FINAL RESULTS SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"ü•á Best Model: {top_k[0]['config']['config_id']}\")\n",
    "        print(f\"üîß Algorithm: {top_k[0]['config']['algorithm']}\")\n",
    "        print(f\"üìä Final Test Reward: {top_k[0]['mean_test_reward']:.4f}\")\n",
    "        print(f\"üìà Average Total Return: {best_metrics.get('mean_total_return', 0):.2%}\")\n",
    "        print(f\"üìä Average Sharpe Ratio: {best_metrics.get('mean_sharpe_ratio', 0):.3f}\")\n",
    "        print(f\"üìâ Average Max Drawdown: {best_metrics.get('mean_max_drawdown', 0):.2%}\")\n",
    "        print(f\"üí∞ Average Final Portfolio: ${best_metrics.get('mean_final_portfolio_value', 0):,.0f}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Complete DRL Trading Analysis Finished!\")\n",
    "        print(f\"üìÅ All plots and results saved to: {config.data.output_dir}\")\n",
    "        print(f\"üíæ Trained models saved to: {config.data.model_dir}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No models available for final analysis.\")\n",
    "\n",
    "print(f\"\\nüéØ Analysis pipeline complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3be4788",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "\n",
    "### Train/Val/Test workflow\n",
    "\n",
    "You slice the chronologically ordered data once into train (70‚ÄØ%), validation (15‚ÄØ%), and test (15‚ÄØ%). Train/val are available immediately; the test slice is held untouched for the eventual winner.\n",
    "\n",
    "For each of the 100 configs:\n",
    "\n",
    "Build a train environment on the 70‚ÄØ% slice (optionally per rolling window). Run model.learn there so gradients only see historical minutes.\n",
    "In parallel, hook up the validation environment. Stable‚ÄëBaselines‚Äô EvalCallback hits it every eval_freq steps; you also run a final evaluate_model over a handful of deterministic episodes. All hyperparameter comparisons use these validation rewards (plus any analyzer metrics you compute on the validation slice).\n",
    "Skip the test set entirely at this stage‚Äîno training or evaluation touches it.\n",
    "After the sweep, rank configs by the validation score(s). Pick the champion (or top few) based on the metrics relevant to you (mean reward, Sharpe, drawdown, etc.).\n",
    "\n",
    "Only now bring out the test split. Recreate an environment on the 15‚ÄØ% holdout, run the final analyzer/evaluation for the winning model, and report cumulative/annualized returns, risk metrics, plots, etc. Because the test data stayed unseen until this point, these numbers give you an unbiased generalization check.\n",
    "\n",
    "That‚Äôs the workflow: Train on the training slice, monitor and rank on validation, then reserve the test slice for the single post-selection evaluation.\n",
    "\n",
    "### Further explanation on val set\n",
    "\n",
    "At the validation stage the notebook keeps training and evaluation completely separate:\n",
    "\n",
    "When you call train_a2c_model/train_td3_model, it builds a second EthereumTradingEnvironment using the 15‚ÄØ% validation slice (no random start so episodes begin from a fixed index).\n",
    "\n",
    "That validation env is never used for learning. Instead Stable‚ÄëBaselines‚Äô EvalCallback attaches it to the training run. Every eval_freq steps the callback pauses, runs a handful of deterministic episodes on the validation data, and logs the mean reward. This gives you an out‚Äëof‚Äësample score while training progresses.\n",
    "\n",
    "After model.learn(...) finishes, the code calls evaluate_model(model, val_env, n_episodes=5). This just resets the validation env five times, plays deterministic episodes, sums the step rewards (your 6‚Äëcomponent signal), and reports the average episode reward. That printed ‚ÄúMean reward‚Äù is the validation metric you use to compare hyperparameter configs.\n",
    "\n",
    "If you later pass the validation env/episodes into PerformanceAnalyzer, you get the full suite of portfolio curves, Sharpe, drawdown, etc.‚Äîall computed strictly from validation data.\n",
    "\n",
    "In short, the validation split is the dataset the agent never trains on but sees repeatedly for out‚Äëof‚Äësample checks during hyperparameter selection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
